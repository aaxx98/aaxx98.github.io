{"data":{"allMdx":{"totalCount":25,"nodes":[{"id":"886061f9-12a1-589d-8ce8-c1c0e52086c8","excerpt":"작성중","body":"\n작성중\n","frontmatter":{"date":"2026년 2월 24일","slug":"til-20260224","title":"AWS 배포, github actions CI/CD","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","deploy","aws"],"thumbnail":null}},{"id":"96e08115-6462-5219-bf74-8486a59b4cf4","excerpt":"종합 프로젝트에서 웹 기반 백엔드 시스템의 아키텍처를 직접 설계하고 구현해보았다. 시스템 전체를 설계하는 것이 처음이었기 때문에, 당시에는 구조적 선택을 비교하기보다는 서비스가 안정적으로 동작하는지 여부, 즉 가용성을 중심으로 판단하며 작업했던 것…","body":"\n종합 프로젝트에서 웹 기반 백엔드 시스템의 아키텍처를 직접 설계하고 구현해보았다. 시스템 전체를 설계하는 것이 처음이었기 때문에, 당시에는 구조적 선택을 비교하기보다는 서비스가 안정적으로 동작하는지 여부, 즉 가용성을 중심으로 판단하며 작업했던 것 같다.\n\n앞으로 진행할 최종 융합 프로젝트에서는 잘 동작하는 시스템을 만드는 것을 넘어서, 여러 설계 대안을 비교하고 그중 적절한 선택을 의도적으로 할 수 있기를 기대하고 있다. 이를 위해서는 신뢰성, 가용성, 성능, 확장성과 같은 시스템 품질 속성이 각각 어떤 의미를 가지며, 실제 설계에서 어떤 목표를 설정해야하는지를 먼저 정리해보고자 한다.\n\n또한 이번 TIL에서는 웹 기반 백엔드 시스템을 전제로 하여, 시스템 설계에서 반복적으로 등장하는 핵심 개념들을 정리해보려고 한다.\n\n---\n\n## 1. 시스템 품질 목표\n\n백엔드 시스템 설계에서 가장 먼저 정리해야 할 것은 시스템이 어떤 품질을 목표로 하는지에 대한 기준이다. 신뢰성, 가용성, 성능, 확장성은 시스템의 특성을 설명하는 용어로 자주 사용되지만, 설계 관점에서는 각각 서로 다른 목표를 의미한다.\n\n어떤 것을 목표로 하느냐에 따라 trade off가 발생하기 때문에 시스템 목적에 적절한 목표를 설정하는 것이 중요하다.\n\n![서비스 유형에 따른 시스템 품질 목표의 차이](./goal.png)\n\n예를 들어 결제 서비스의 경우에는 데이터의 정확성과 보안이 핵심이 되므로 신뢰성이 가장 중요한 목표가 된다. 반면 메신저 서비스는 사용자 간의 실시간 상호작용이 중심이 되기 때문에 낮은 지연 시간과 높은 확장성이 중요한 설계 목표가 된다. 온라인 쇼핑 서비스처럼 트래픽 변동이 큰 서비스의 경우에는 장애 상황에서도 서비스가 지속될 수 있는 가용성과, 사용자 증가에 대응할 수 있는 확장성이 우선적으로 고려된다.\n\n이처럼 시스템 설계에서 신뢰성, 가용성, 성능, 확장성은 동시에 모두 극대화해야 할 속성이 아니라, 서비스가 달성하고자 하는 목표에 따라 우선순위가 달라지는 기준으로 이해하는 것이 적절하다.\n\n### 1.1 신뢰성 (Reliability)\n\n신뢰성은 시스템이 의도된 동작을 정확하게 수행하는 능력을 의미한다. 이는 시스템이 단순히 동작 중인지 여부와는 구분된다. 신뢰성이 논의되는 지점은 주로 결과의 정확성과 데이터 정합성이다.\n\n신뢰성은 다음과 같은 요소들과 강하게 연결된다.\n\n- 동일한 요청이 반복되더라도 결과가 일관되게 유지되는지 여부\n- 부분 실패나 재시도 상황에서 데이터가 훼손되지 않는지 여부\n- 중간 상태로 데이터가 저장되거나 불완전한 결과가 노출되지 않는지 여부\n- 트랜잭션, 멱등성, 무결성 제약 조건의 적절한 적용 여부\n\n신뢰성 지표는 다음과 같은것이 있다.\n\n- MTBF(Mean Time Between Failures)\n- MTTR(Mean Time To Repair)\n- 에러율\n- 데이터 무결성 검사\n\n### 1.2 가용성 (Availability)\n\n가용성은 시스템이 요청을 처리할 수 있는 상태로 존재하는 비율을 의미한다. 이 개념은 결과의 정확성보다는 응답 가능성에 초점을 둔다.\n\n가용성의 관점에서 중요한 판단 기준은 다음과 같다.\n\n- 일부 기능이 제한된 상태에서도 요청을 받을 수 있는지 여부\n- 특정 인스턴스에 장애가 발생해도 전체 서비스가 중단되지 않는지 여부\n- 장애 발생 시 빠르게 대체 경로로 요청을 우회할 수 있는지 여부\n\n가용성은 다중 인스턴스 구성, 장애 격리, 로드 밸런싱과 같은 구조적 선택과 밀접하게 관련된다.\n\n가용성을 높이기 위해서는 다음을 고려할 수 있다.\n\n- 이중화\n- 재시도 및 실패 대비\n- 장애 감지\n\n### 1.3 성능 (Performance)\n\n성능은 단일 요청 또는 일정 시간 동안의 요청을 얼마나 효율적으로 처리할 수 있는지를 나타낸다. 일반적으로 응답 시간과 처리량이 주요 지표로 사용된다.\n\n성능 문제는 주로 다음과 같은 원인에서 발생한다.\n\n- 비효율적인 쿼리나 데이터 접근 패턴\n- 과도한 동기 호출로 인한 대기 시간 증가\n- 불필요한 네트워크 왕복\n- 애플리케이션 내부 로직의 비효율\n\n성능이라는 목표는 단순히 자원을 추가하는 방식으로 달성되지 않으며, 내부 처리 흐름과 구현 효율에 크게 의존한다.\n\n성능을 높이기 위해서는 다음과 같은 방식을 고려할 수 있다.\n\n- 캐싱\n  - 자주 조회되는 데이터의 응답 속도를 빠르게 한다.\n  - 캐시 무효화, 오래된 데이터 표시 가능성\n- 비동기 처리\n  - 오래 걸리는 작업의 대기시간을 줄인다.\n  - 순서 보장되지 않음, 에러 처리 로직 복잡도 증가\n- DB 최적화\n  - 쿼리 최적화, 인덱스 추가, Read용 레플리카, 샤딩\n\n### 1.4 확장성 (Scalability)\n\n확장성은 부하 증가 시 자원을 추가함으로써 성능 저하를 억제할 수 있는 능력을 의미한다. 이는 현재 시스템이 빠른지 여부와는 별개의 문제다.\n\n확장성을 판단할 때 고려해야 할 요소는 다음과 같다.\n\n- 애플리케이션 서버가 상태를 내부에 보관하고 있는지 여부\n- 서버를 수평으로 추가하는 것이 가능한 구조인지 여부\n- 특정 컴포넌트가 병목으로 고정되어 있지 않은지 여부\n\n확장성이라는 목표는 구현 단계에서 보완하기보다, 설계 단계에서의 구조적 선택에 의해 대부분 결정된다.\n\n### 1.5 시스템 품질 목표 관계 정리\n\n각 품질 목표는 시스템 설계 시 서로 다른 질문에 대응한다. 설계 과정에서는 이 목표들 중 무엇을 우선할 것인지, 그리고 어떤 목표를 부분적으로 양보할 것인지를 명확히 인식할 필요가 있다.\n\n- 신뢰성은 결과가 정확한지를 묻는다.\n- 가용성은 요청을 받을 수 있는지를 묻는다.\n- 성능은 얼마나 빠르게 처리되는지를 묻는다.\n- 확장성은 규모가 커져도 유지 가능한지를 묻는다.\n\n이 네 가지는 서로 보완적일 수 있으나, 동일한 개념은 아니다.\n\n## 2. 백엔드 시스템 주요 컴포넌트의 역할\n\n백엔드 시스템은 여러 컴포넌트가 각자의 책임을 나누어 가지는 구조로 구성된다. 각 컴포넌트는 특정 품질 속성을 보완하기 위해 존재한다.\n\n### 2.1 애플리케이션 서버\n\n애플리케이션 서버는 클라이언트 요청을 처리하고 비즈니스 로직을 수행하는 역할을 담당한다. 일반적으로 애플리케이션 서버는 다음 원칙을 따른다.\n\n- 상태를 최소화하여 확장성을 확보한다.\n- 요청 수명을 짧게 유지한다.\n- 장기 상태는 외부 저장소로 위임한다.\n\n여기서 말하는 애플리케이션 서버는 정적 리소스를 제공하는 웹 서버와 구분되며, 비즈니스 로직과 데이터 접근을 담당하는 서버를 의미한다.\n\n![웹 서버와 웹 애플리케이션 서버의 역할 분리 구조](./server.png)\n\n### 2.2 데이터베이스\n\n데이터베이스는 영속 데이터를 저장하며, 시스템의 최종적인 진실 원천으로 취급된다. 데이터베이스는 상대적으로 느릴 수 있으나, 결과의 정확성은 반드시 보장되어야 한다. 이로 인해 데이터베이스는 성능보다 신뢰성과 더 강하게 연결된다.\n\n캐시나 메시지 큐가 데이터베이스 앞단에 배치되는 이유는 데이터베이스의 부담을 줄이기 위함이지, 데이터베이스의 정확성을 대체하기 위함은 아니다.\n\n### 2.3 캐시\n\n캐시는 성능과 확장성을 개선하기 위해 사용된다. 캐시 도입은 최신성이 아닌 데이터를 일정 수준 허용하겠다는 설계적 선택을 포함한다.\n\n캐시 사용 시 고려해야 할 기준은 다음과 같다.\n\n- 데이터의 최신성이 반드시 필요한지 여부\n- 일시적인 불일치가 허용 가능한지 여부\n- 캐시 무효화 전략을 명확히 정의했는지 여부\n\n캐싱하기 적합한 데이터는 다음과 같이 정리할 수 있다.\n\n![캐싱하기 적합한 데이터](./cache.png)\n\n### 2.4 메시지 큐\n\n![동기 처리와 비동기 처리](./sync.png)\n\n동기 처리에서는 요청을 보낸 쪽이 응답을 받을 때까지 다음 작업을 진행하지 못한다.\n반면 비동기 처리에서는 요청을 보낸 이후 응답을 기다리지 않고 다음 작업을 바로 진행하여 대기시간 동안 다른 작업을 이어서 할 수 있다.\n\n순차적으로 처리해야할 필요가 없다면 비동기적으로 처리하는 것이 효율적이라고 볼 수 있다.\n\n메시지 큐는 비동기 처리 수단이자 트래픽 완충 장치다. 즉시 처리할 필요가 없는 작업을 분리함으로써 시스템 전반의 안정성을 높인다.\n\n메시지 큐는 다음 목적에 사용된다.\n\n- 트래픽 급증 시 부하 완화\n- 실패 전파 차단\n- 처리 책임의 분리\n\n## 3. 로드 밸런싱\n\n가용성과 운영 안정성을 확보하기 위해 서버를 여러 대로 구성하는 경우가 많다. 이때 각 요청은 여러 서버 중 하나에서 처리되며, 클라이언트는 실제로 어떤 서버가 요청을 처리하는지 알 필요가 없다.\n\n요청이 어떤 서버로 전달되느냐에 따라 처리 결과가 달라질 수 있다면, 시스템은 예측하기 어려운 상태가 된다. 특히 서버가 클라이언트의 상태를 내부에 보관하는 구조에서는 요청을 특정 서버로 고정해야 하는 제약이 생길 수 있다.\n\n### 3.1 로드 밸런서의 역할\n\n로드 밸런서는 다음 기능을 수행한다.\n\n- 요청 분산을 통한 부하 완화\n- 장애 인스턴스의 격리\n- 배포 중 트래픽 제어\n\n![로드 밸런서의 위치와 역할](./load-balancer.png)\n\n이러한 분산 처리 구조에서는, 요청이 어느 서버로 전달되더라도 처리 결과가 달라지지 않아야 한다.\n\n분산 환경에서는 요청이 어느 서버로 전달되든 처리 결과가 달라지지 않아야 한다. 서버 내부에 클라이언트 상태를 보관하면 이 조건을 만족시키기 어렵기 때문에, 애플리케이션 서버는 보통 무상태 구조로 설계된다.\n\n하지만 특정 사용자의 요청을 항상 동일한 서버에서 처리해야 하는 요구가 있다면, 목적에 따라 상태성 구조를 선택할 수도 있다.\n\n![상태성/무상태성](./stateful.png)\n\n### 3.2 헬스 체크의 의미\n\n헬스 체크 실패는 서버가 물리적으로 중단되었음을 의미하지 않는다. 이는 해당 인스턴스가 현재 요청을 처리하기에 적합하지 않다는 판단을 의미한다. 이 메커니즘을 통해 무중단 배포와 부분 장애 격리가 가능해진다.\n\n## 3.3 로드 밸런싱 알고리즘\n\n로드 밸런서는 단순히 요청을 여러 서버로 나누는 장치가 아니다. 어떤 기준으로 요청을 분배하느냐에 따라 성능, 가용성, 그리고 장애 전파 방식이 달라진다. 이를 결정하는 것이 로드 밸런싱 알고리즘이다. 알고리즘은 서버의 상태를 얼마나 고려하는지, 요청의 특성을 반영하는지에 따라 구분된다.\n\n다음 요청을 어떤 서버에 할당할지를 결정하는 알고리즘은 다음과 같다.\n\n![로드 밸런싱 알고리즘](./algorithm.png)\n\n### 3.3.1 Round Robin\n\nRound Robin은 가장 단순한 로드 밸런싱 알고리즘이다. 요청을 서버 목록에 순서대로 하나씩 분배한다. 서버 간 성능이나 현재 부하 상태는 고려하지 않는다.\n\n이 방식의 장점은 구현이 단순하고, 서버 간 성능 차이가 거의 없으며 요청 처리 시간이 비교적 균일한 환경에서는 공정한 분배가 가능하다는 점이다. 반면, 서버 성능이 서로 다르거나 특정 요청이 오래 걸리는 경우에는 일부 서버에 부하가 집중될 수 있다. 또한 서버가 느려졌더라도 다음 차례라는 이유만으로 요청이 전달될 수 있어, 상태를 반영하지 못한다는 한계가 있다.\n\nRound Robin은 테스트 환경이나 요청 특성이 매우 단순한 시스템에서 주로 사용된다.\n\n### 3.3.2 Weighted Round Robin\n\nWeighted Round Robin은 Round Robin에 가중치를 추가한 방식이다. 서버마다 처리 능력에 따라 서로 다른 비율로 요청을 분배한다. 예를 들어 성능이 좋은 서버에는 더 많은 요청을, 상대적으로 약한 서버에는 적은 요청을 전달한다.\n\n이 방식은 서버 스펙이 서로 다른 환경에서 비교적 합리적인 분배를 가능하게 한다. 그러나 가중치는 정적으로 설정되는 경우가 많기 때문에, 실행 중 서버 상태 변화나 일시적인 부하 증가는 반영하지 못한다. 따라서 서버의 성능 특성이 고정되어 있고 부하 패턴이 비교적 안정적인 경우에 적합하다.\n\n### 3.3.3 Least Connections\n\nLeast Connections 방식은 현재 연결 수가 가장 적은 서버로 요청을 전달한다. 이 알고리즘은 서버의 실시간 부하를 간접적으로 반영한다는 점에서 Round Robin 계열보다 발전된 방식이다.\n\n이 방식의 장점은 요청 처리 시간이 균일하지 않은 환경에서도 상대적으로 균형 잡힌 분배가 가능하다는 점이다. 장시간 유지되는 연결이 많은 경우에도 효과적이다. 그러나 연결 수가 실제 처리 부하를 정확히 반영하지 못하는 경우에는 문제가 발생할 수 있다. 예를 들어 연결 수는 적지만 매우 무거운 작업을 처리 중인 서버가 선택될 가능성도 존재한다.\n\nLeast Connections는 웹 애플리케이션이나 API 서버처럼 요청 처리 시간이 가변적인 환경에서 자주 사용된다.\n\n### 3.3.4 Least Response Time\n\nLeast Response Time 방식은 현재 응답 시간이 가장 짧은 서버를 선택한다. 이는 연결 수뿐 아니라 서버의 실제 처리 성능을 직접적으로 반영하려는 접근이다.\n\n이 알고리즘은 성능 관점에서 매우 합리적인 선택처럼 보이지만, 구현과 운영 비용이 높다. 응답 시간 측정을 위한 추가 오버헤드가 발생하며, 일시적인 네트워크 지연이나 측정 오차에 민감하게 반응할 수 있다. 또한 응답 시간이 빠른 서버로 트래픽이 몰리면서 역으로 부하가 집중되는 현상이 발생할 수 있다.\n\n따라서 이 방식은 고급 로드 밸런서나 특정 환경에서 제한적으로 사용된다.\n\n### 3.3.5 IP Hash\n\nIP Hash 방식은 클라이언트 IP 주소를 해시하여 특정 서버로 요청을 고정적으로 전달한다. 동일한 클라이언트는 항상 같은 서버로 라우팅된다.\n\n이 방식의 가장 큰 장점은 세션 고정이 필요할 때 별도의 세션 스토리지를 사용하지 않아도 된다는 점이다. 그러나 서버 장애나 스케일 인/아웃이 발생하면 해시 결과가 크게 변하면서 트래픽 분포가 깨질 수 있다. 또한 특정 IP 대역에서 트래픽이 몰리는 경우 서버 간 부하 불균형이 심해질 수 있다.\n\nIP Hash는 세션 상태를 서버 내부에 유지해야 하는 레거시 시스템에서 주로 사용되며, 확장성과는 상충하는 선택이 되는 경우가 많다.\n\n### 3.3.6 알고리즘 선택 시 고려 사항\n\n로드 밸런싱 알고리즘 선택은 단순한 기술 문제가 아니라 설계 판단의 문제다. 다음과 같은 기준을 함께 고려해야 한다.\n\n- 서버 간 성능이 균일한지 여부\n- 요청 처리 시간이 일정한지 여부\n- 세션 고정이 필요한지 여부\n- 실시간 상태 정보를 반영할 필요가 있는지 여부\n- 스케일 인·아웃과 장애 발생 빈도\n\n어떤 알고리즘도 모든 상황에서 최선의 선택이 되지는 않는다. 각 알고리즘은 특정 품질 속성을 우선하는 대신 다른 속성을 부분적으로 희생한다는 점을 인식하는 것이 중요하다.\n\n## 4. 정리\n\n백엔드 시스템 설계를 정리하면서, 기술 자체보다 중요한 것은 무엇을 목표로 삼고 어떤 선택을 했는지를 설명할 수 있는 기준이라는 점을 느꼈다. 이전에는 가용성처럼 눈에 보이는 결과에만 집중해 구조를 선택했다면, 이제는 신뢰성, 성능, 확장성과 같은 품질 목표를 기준으로 설계를 바라보게 되었다.\n\n각 컴포넌트와 로드 밸런싱 역시 독립적인 기술 요소가 아니라, 특정 품질 목표를 만족시키기 위한 선택이라는 점이 분명해졌다. 이 기준을 가지고 설계를 바라보면, 구조가 왜 그렇게 구성되었는지, 어떤 제약을 감수한 선택인지가 함께 보이기 시작한다.\n\n아직 모든 설계 판단을 정확히 할 수 있는 단계는 아니지만, 적어도 다음 프로젝트에서는 “왜 이 구조를 선택했는가”를 스스로 설명할 수 있는 상태로 설계를 시작할 수 있을 것 같다.\n","frontmatter":{"date":"2026년 2월 9일","slug":"til-20260209","title":"백엔드 시스템 설계 기초 정리","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","system-design","architecture"],"thumbnail":null}},{"id":"7d247557-b8ea-588b-a357-558c91f53c16","excerpt":"1. 성취 (Achievement) 검색/조회 중심 서비스 설계를 위한 Polyglot Persistence 구조 설계 RDBMS(MySQL), MongoDB, Elasticsearch 역할 분리로 성능과 정합성 확보 사용자 사전, 동의어 사전을…","body":"\n## 1. 성취 (Achievement)\n\n- 검색/조회 중심 서비스 설계를 위한 Polyglot Persistence 구조 설계\n- RDBMS(MySQL), MongoDB, Elasticsearch 역할 분리로 성능과 정합성 확보\n- 사용자 사전, 동의어 사전을 기반으로 검색 인덱스 설계 전략 수립\n- MySQL 기반 검색 최적화(B-Tree, Composite Index, Covering Index, Full-Text)와 NoSQL/검색 엔진 최적화 전략 학습\n\n---\n\n## 2. 학습 (Learning)\n\n### 2.1 MySQL 검색 최적화 요소 및 고민\n\n1. **B-Tree Index**\n\n   B-Tree는 MySQL에서 가장 기본적인 인덱스 구조로, 특정 값(`=`) 조회나 범위 조회(`>`, `<`)에서 뛰어난 성능을 보여준다.\n\n   부분 일치 검색(`LIKE '%키워드%'`)은 인덱스를 타지 못하고 Full Table Scan이 발생하므로 성능 저하가 크다.\n\n   **깨달음**: 단순 조회와 범위 조건은 효율적이지만, 복잡한 텍스트 검색에는 한계가 있다.\n\n2. **Composite Index (복합 인덱스)**\n\n여러 컬럼을 하나의 인덱스로 묶어 복합 조건 조회에 활용한다.\n\n- **전통적 이해**: 왼쪽 컬럼 조건이 없으면 인덱스를 제대로 활용할 수 없었음.\n  예: `(상담일자, 상담사ID)` → 상담일자만 조건으로 조회 가능, 상담사ID만 조건이면 인덱스 활용 어려움.\n  ![과거 복합 인덱스](traditional-index.gif)\n- **최신 구조**: 각 컬럼 값과 포인터를 연결한 다중 컬럼 B-Tree 구조로 관리. 옵티마이저가 통계 기반 탐색 경로를 결정하므로, 왼쪽 컬럼 조건 없이도 일부 인덱스 활용 가능.\n  ![최신 복합 인덱스](multicolumn-index.gif)\n\n- **설계 고려**: 조회 패턴, 범위 검색, 자주 사용되는 조건을 분석해 인덱스 구성. 부분 일치나 범위 조건은 여전히 성능에 영향.\n\n3. **Cardinality (카디널리티)**\n\n   컬럼 값의 다양성에 따라 인덱스 효율이 달라진다.\n\n   값이 적은 상태 컬럼(대기/처리중/완료)은 단독 인덱스 효율이 낮아, 다른 조건과 결합한 복합 인덱스를 설계해야 한다.\n\n4. **Covering Index**\n\n   인덱스에 필요한 모든 컬럼이 포함되면 테이블 접근 없이 쿼리를 처리할 수 있다.\n\n   조회 속도가 크게 개선되며, 자주 조회되는 컬럼을 중심으로 설계하는 것이 중요하다.\n\n5. **EXPLAIN / Execution Plan**\n\n   실제 쿼리가 어떻게 실행되는지 확인하고 인덱스 사용 여부를 판단한다.\n\n   단순 인덱스 존재 여부보다 **쿼리 구조와 조건 순서**가 성능에 결정적인 영향을 미친다.\n\n6. **LIKE 검색 패턴**\n\n   접두어 검색(`'키워드%'`)과 부분 일치(`'%키워드%'`)의 성능 차이를 확인했다.\n\n   복잡한 키워드 검색에는 B-Tree 한계가 명확하므로 Elasticsearch 도입 근거가 된다.\n\n7. **Full-Text Search**\n\n   문자열을 토큰화 후 역색인 방식으로 검색할 수 있지만, 한국어 형태소 처리 한계, 동의어 처리 한계, 관련도 점수 계산 한계가 있다.\n\n8. **Tokenizer / Stop Word**\n\n   검색 품질을 높이기 위해 단어 분리 기준과 의미 없는 단어 제거 규칙을 고민했다.\n\n   MySQL Full-Text에서는 한국어 형태소 처리 최적화가 어려워 Elasticsearch와 결합 필요성을 느꼈다.\n\n9. **Search Normalization**\n\n   검색어 표기 통일, 동의어/유의어 처리 등.\n\n   커스텀 사전 관리 필요성을 확인했다.\n\n10. **Denormalization (반정규화)**\n    ![정규화 테이블](./normalization.png)\n    ![반정규화 테이블](./denormalization.png)\n\n    JOIN 제거, 검색 대상 단순화 위해 일부 컬럼을 한 테이블에 집약하여 중복 저장.\n\n    조회 성능과 정합성 간 트레이드오프를 설계 단계에서 고려.\n\n11. **Full Scan vs Index Scan**\n\n    전체 테이블 스캔보다 인덱스 스캔 활용이 필수적임을 확인.\n\n12. **Pagination + Index**\n\n    OFFSET 기반 페이징은 성능 저하가 크므로, search_after 등 인덱스 기반 페이지 전략 설계를 고려했다.\n\n13. **RDB Full-Text 한계**\n\n    대규모 데이터, 다중 필드 검색에는 MySQL Full-Text로 한계가 있었고, 검색 전용 엔진 도입 근거가 됨.\n\n---\n\n### 2.2 MongoDB 검색/조회 설계 고려\n\n#### 읽기 중심 설계\n\n- 상담 요약 문서에 고객 정보, 상담사 정보, 키워드, 카테고리 등 조회 대상 전체 내용포함 → JOIN 없이 단일 조회 가능\n- 역정규화 구조 설계로 조회 성능 극대화\n\n#### 검색/조회 최적화 전략\n\n1. **복합 인덱스 설계**\n\n   단일 필드 인덱스로는 다중 조건 조회를 최적화하기 어렵다.\n\n   상담 요약 조회가 `category + date + keyword` 조합으로 수행된다면, 해당 조회 패턴을 기준으로 복합 인덱스를 설계해야 한다.\n   - 자주 사용되는 조건을 앞쪽에 배치 (Leftmost Prefix Rule 고려)\n   - 정렬 조건이 있다면 인덱스에 포함\n   - 선택도가 높은 필드를 앞에 배치하여 필터링 효율 극대화\n\n     ```jsx\n     db.summary.createIndex({ category: 1, date: -1 });\n     ```\n\n   - 트레이드오프\n   - 인덱스 수 증가 → 쓰기 성능 저하\n   - 메모리 사용량 증가\n\n   읽기 중심 시스템이라면 인덱스 비용을 감수하고 조회 성능을 우선한다.\n\n2. **Projection 활용**\n\n   MongoDB는 기본적으로 문서 전체를 반환한다.\n\n   요약 문서가 크다면 네트워크 비용과 메모리 사용량이 증가한다.\n   - 필요한 필드만 조회하여 I/O 감소\n   - 인덱스 커버링 조회 가능성 확보\n   - 대용량 필드(ex: 상담 원문)는 목록 조회에서 제외\n\n   ```jsx\n   db.summary.find({ category: 'billing' }, { title: 1, date: 1, keywords: 1, _id: 0 });\n   ```\n\n   목록 조회와 상세 조회를 분리 설계하는 것이 일반적이다.\n\n3. **Aggregation Pipeline 최적화**\n\n   ![Aggregation Pipeline](./pipeline.png)\n   Aggregation은 단계 순서에 따라 성능 차이가 크다.\n   - 원칙\n     - `$match`를 가능한 한 앞에 배치\n       - 조건별 데이터 선별 시 인덱스 활용 가능\n     - `$project`로 불필요 필드 제거 후 다음 단계 수행\n     - `$group`은 데이터 양을 충분히 줄인 뒤 실행\n\n   - 인덱스 적용 불가로 비효율적인 순서\n\n     ```jsx\n     $project → $match → $group\n     ```\n\n   - 권장 구조\n\n     ```jsx\n     $match → $project → $group\n     ```\n\n   Aggregation은 내부적으로 메모리를 사용하므로, 초기 단계에서 데이터 양을 줄이는 것이 핵심이다.\n\n4. **요약 문서 설계**\n\n   읽기 중심 구조에서는 조인을 제거하기 위해 역정규화한다.\n   - 상담 요약 문서에 포함할 필드\n     - 고객 식별 정보\n     - 상담사 정보\n     - 키워드 배열\n     - 카테고리\n     - 작성 일시\n     - 상태 값\n     - 목표\n     - 단일 쿼리로 목록/상세 조회 가능\n     - 조회 경로 단순화\n     - ES 연동 시 색인 데이터로 재사용 가능\n\n   - 단점\n     - 데이터 중복\n     - 수정 시 동기화 필요\n\n5. **데이터 일관성 전략**\n\n   역정규화 구조에서는 데이터 불일치 위험이 존재한다.\n   - 대응 전략\n     - Bulk Write로 다중 문서 동시 갱신\n     - Atomic Update 연산자 활용 ($set, $inc 등)\n     - 트랜잭션 사용 (복수 컬렉션 갱신 시)\n     - 이벤트 기반 비동기 동기화 설계\n\n   쓰기 비용은 증가하지만 읽기 성능과 확장성을 확보한다.\n\n#### 자동완성/텍스트 검색 보완 고려\n\n- 접두어 검색은 가능하지만, 오타 허용/유사도 기반 검색은 별도 구현 필요\n- 검색 관련 부하 최소화를 위해 ES와 결합 설계\n\n---\n\n### 2.3 Elasticsearch 검색/조회 설계 고려\n\n- **선택 이유**\n  - 부분 일치, 다중 필드, 형태소 분석, 키워드 자동완성, 유사도 기반 랭킹 필요\n- **검색/조회 최적화 전략**\n  - **분석기(Nori) 적용**\n    ![형태소 분석 결과](./es-analyzer.png)\n    - 한국어는 조사·어미 변화가 많아 공백 기준 토큰화로는 의미 단위가 정확히 분리되지 않는다.\n    - Nori 분석기를 적용해 형태소 단위로 색인하면 검색 정확도가 개선된다.\n    - 사용자 사전을 추가해 도메인 특화 용어(서비스명, 고유명사 등)를 보완할 수 있다.\n    - 색인 시점 분석 전략이 검색 품질을 결정하므로, 기본 analyzer 선택은 설계 단계에서 확정해야 한다.\n  - **Query vs Filter 분리**\n    - relevance score가 필요한 조건만 Query Context로 처리한다.\n    - 상태값, 카테고리, 날짜 범위처럼 점수 계산이 불필요한 조건은 Filter Context로 분리한다.\n    - Filter는 scoring을 수행하지 않고 캐시 활용이 가능하므로 CPU 사용량과 응답 지연을 줄일 수 있다.\n    - 검색 정확도와 성능을 동시에 확보하기 위한 기본 설계 원칙이다.\n  - **Multi-field 및 Nested 설계**\n    - 하나의 필드를 text + keyword 형태로 다중 색인해 전문 검색과 정렬·집계를 동시에 지원한다.\n    - 자동완성용 필드는 edge_ngram 전용 필드로 분리해 목적에 맞게 색인한다.\n    - 배열 내부 객체가 독립적인 의미 단위를 갖는 경우 nested 매핑을 사용한다.\n    - 단순 object 배열은 필드 간 교차 매칭 문제를 유발할 수 있으므로 주의가 필요하다.\n  - **인덱스 버전 관리 및 Alias 전략**\n    - 매핑 변경이나 대규모 재색인이 필요한 경우 새로운 버전의 인덱스를 생성한다.\n    - 서비스는 alias를 통해 인덱스를 참조하도록 구성한다.\n    - 재색인 완료 후 alias를 전환하면 무중단 배포가 가능하다.\n    - 장애 발생 시 이전 인덱스로 즉시 롤백할 수 있는 구조를 유지한다.\n  - **Batch vs Incremental Update 전략**\n    - 대량 데이터 초기 색인은 Batch 방식으로 처리해 색인 효율을 극대화한다.\n    - 실시간 변경 데이터는 Incremental Update로 처리해 색인 지연을 최소화한다.\n    - Bulk API를 활용해 네트워크 오버헤드를 줄이고 처리량을 확보한다.\n    - 쓰기 부하와 검색 지연 사이의 균형을 고려해 전략을 병행한다.\n  - **JVM 및 클러스터 운영 고려**\n    - 힙 메모리는 전체 메모리의 약 50% 수준에서 설정하고, 과도한 힙 증설은 GC 지연을 유발할 수 있다.\n    - shard 수는 인덱스 크기와 노드 수를 기준으로 산정하며, 과도한 shard 분할은 오히려 성능을 저하시킨다.\n    - replica는 가용성과 읽기 확장성을 동시에 고려해 설정한다.\n    - GC 모니터링, heap usage 추적, segment 병합 상태 확인은 운영 필수 항목이다.\n  - **자동완성 설계 (edge_ngram 기반)**\n    - edge_ngram 분석기를 활용해 접두어 기반 검색을 지원한다.\n    - 검색 전용 필드로 분리해 일반 검색 필드와 혼합하지 않는다.\n    - 최소/최대 gram 길이는 사용자 입력 패턴을 기준으로 설정한다.\n    - 도메인 사전을 반영해 의미 없는 토큰 생성을 방지하고 정확도를 유지한다.\n- **운영/성능 고민**\n  - 대규모 색인/검색 시 메모리와 GC 영향\n  - 사전 변경 시 재인덱싱 전략, Alias 전환, 롤백 고려\n\n---\n\n### 2.4 키워드 및 자동완성\n\n![키워드 및 자동완성](./auto.gif)\n\n- 상담/사용자 텍스트 의미 있는 단어 → 키워드 추출\n- 자동완성 → edge_ngram, prefix 토크나이저\n- 커스텀 사전 활용 → 신조어나 도메인 특화 용어 처리\n\n### 2.5 사용자/동의어 사전\n\n- 사용자 사전: 공백 포함 단어도 하나로 인식\n- 동의어 사전: 검색 일관성 확보\n\n---\n\n## 3. 개선 (Improvement)\n\n- 재인덱싱 부담 최소화 → MongoDB 요약 데이터 활용\n- Filter Context, 점수 계산 제외 조건 적용\n- 키워드 집계 위치 결정 → ES vs MongoDB\n\n---\n\n## 4. 피드백 (Feedback)\n\n- **Good**: 사전 기반 검색 품질 향상, 역할 분리 설계로 성능·안정성 확보\n- **Bad**: Elasticsearch Nori 세부 설정, JVM/GC 관리, 재인덱싱 전략 고민 필요\n\n---\n\n## 5. TIP (Tip)\n\n- Filter Context → CPU 절감, 캐싱 가능\n- 커스텀 사전 관리 → 검색/요약 시스템에서\n- MongoDB 기반 사전 업데이트 → ES 재인덱싱 최소화\n- 자동완성 → edge_ngram + 커스텀 단어 반영\n- JVM 기반 ES → 리소스/GC 설계 필수\n\n---\n\n### 6. RDBMS / NoSQL / Elasticsearch 검색·조회 장점 요약\n\n<div className=\"mdx-table-wrapper\">\n  <table className=\"mdx-table\">\n    <thead className=\"mdx-thead\">\n      <tr className=\"mdx-tr\">\n        <th className=\"mdx-th\">저장소</th>\n        <th className=\"mdx-th\">장점(검색/조회 관점)</th>\n        <th className=\"mdx-th\">설계 단계에서 고려한 최적화 전략</th>\n      </tr>\n    </thead>\n    <tbody className=\"mdx-tbody\">\n      <tr className=\"mdx-tr\">\n        <td className=\"mdx-td\">MySQL</td>\n        <td className=\"mdx-td\">정확 조건, 범위 조회, 정합성</td>\n        <td className=\"mdx-td\">\n          인덱스 설계, Composite Index, Covering Index, Full-Text, Pagination 전략\n        </td>\n      </tr>\n      <tr className=\"mdx-tr\">\n        <td className=\"mdx-td\">MongoDB</td>\n        <td className=\"mdx-td\">문서 단위 조회, JOIN 제거, 요약 데이터 활용</td>\n        <td className=\"mdx-td\">\n          복합 인덱스, Projection, Aggregation Pipeline 최적화, 샤딩, 요약 문서 설계, Bulk Write\n        </td>\n      </tr>\n      <tr className=\"mdx-tr\">\n        <td className=\"mdx-td\">Elasticsearch</td>\n        <td className=\"mdx-td\">텍스트 검색, 유사도 기반 정렬, 자동완성</td>\n        <td className=\"mdx-td\">\n          Nori 분석기, query/filter 분리, edge_ngram, 인덱스 버전+Alias, Batch/Incremental Update,\n          JVM 운영 고려\n        </td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n","frontmatter":{"date":"2026년 2월 3일","slug":"til-20260203","title":"RDBMS(MySQL), NoSQL(MongoDB) 비교 및 검색/조회 최적화","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","database","architecture"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAACxUlEQVR42o1T70tTURi+/0Uf+iMio6KIPogZCBFWGkNNgyDQrP4AhQqKPgRRFkQRaVpo+tkK11Ist2g/UnJsQ+factvd3d3v3bvd7d6n8x45Y4s+9MLL+3LuOc99nuc9R9qZiSE8/Rs7LH8t7KJaMEBhmRb+Faa1t74a+Y5n7rd47p7l9YVnDi+985Bc3T/wrWcDXzt9sB90IuaNw4LJEPcALKsVuG7WeR19fwdtk+fQ+e4K2meHcHzGho65ywywzwvXgA/OXi+cNg/kYArpbBpqWkW1Wm0ACeC6ZfJ6d/kpOt4M4sL8KM7PX0P3wgjOzF6FREz43iYidFjTNKRSDDydhq7rDUBRC8UCyoYOvVZFtphDNB5DhfVScPEWgou3QTX08T70oipQ+eFKpcJBCbxUKsE09xiWWS981jUdclLmveR6dAyuh0fgfHAAjrF9CG+sQjdMGNVKi3eGYSCTyUCWZeRyOaiqikKhwH+WzWZRLpd5L/mmhuCbGoT3lQ2eyUvIyhHkCyUoSoqzokPEsnk4tE4Zi8Xgdrt5RiIRrK+vMw8bJrYGAdRqtQYLRVE4Q+EnsaSe9tAPyQ6q0vDET4w88WP48SZGJjYRVzRhYet1qde5LJJKzEh6KBSCw+GAx+Pha36/H9Lhm2tou76GQzec2N+/jFVPBMW8CjmlcABiQRMXw+CXm/XEMJ/PI5FINNhTSuPTYYy/DmOM1XtzO5BVxtCq8yEI82mjkE1JQ6AUPkajUf6dD8VKJ2GlE0AmBTBmaGLyt6eNO8h8JWY0iGAwiK2tLd5TSisDNiz3XYS96xSWuk6zlxKAwUA1JpekiiTpQr5gTwOh1yTkF4tFBthzFl/6e2FvP4EPJ48i6vMhz8AyTKqQ1px0OJlM8qSgNRpKIBDA9vY2pF2HHbuflhBf+Yykaw01XcP/BA2MrgpVui6C/R/fKjsFQw4TbQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/84042c94d721c77a4a48799302b7086c/ce24c/denormalization.png","srcSet":"/static/84042c94d721c77a4a48799302b7086c/aacf3/denormalization.png 200w,\n/static/84042c94d721c77a4a48799302b7086c/0e3a3/denormalization.png 400w,\n/static/84042c94d721c77a4a48799302b7086c/ce24c/denormalization.png 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/84042c94d721c77a4a48799302b7086c/abf68/denormalization.webp 200w,\n/static/84042c94d721c77a4a48799302b7086c/ce081/denormalization.webp 400w,\n/static/84042c94d721c77a4a48799302b7086c/c11e0/denormalization.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":600}}}}},{"id":"872ce078-57d0-593d-9234-bf08c2ff9f37","excerpt":"이번 프로젝트에서는 전체 시스템 설계와 예약 발송 메시지 처리 구조 설계와 구현을 중심으로 맡아 작업했다. 관리자 화면에서 예약 발송이 등록된 이후 실제 메시지 상태 생성, 큐 발행, 발송 완료 처리까지 이어지는 전체 흐름을 배치 기반으로 설계했고,…","body":"\n> 이번 프로젝트에서는 전체 시스템 설계와 예약 발송 메시지 처리 구조 설계와 구현을 중심으로 맡아 작업했다.\n>\n> 관리자 화면에서 예약 발송이 등록된 이후 실제 메시지 상태 생성, 큐 발행, 발송 완료 처리까지 이어지는 전체 흐름을 배치 기반으로 설계했고, 대량 메시지 처리 과정에서 발생할 수 있는 예외와 복구 시나리오를 함께 고려했다.\n\n# 1. 시스템 아키텍쳐\n\n![시스템 아키텍쳐](./architecture.png)\n\n- 시스템은 역할에 따라 세 개의 Spring 애플리케이션과 하나의 메시지 브로커로 분리되어 있다.\n  - **Admin Web**\n    - 관리자 화면을 담당한다.\n    - 전송 실패 또는 배치 실패 발생 시 웹 UI를 통해 수동 재처리가 가능하다.\n    - 메시지 전송 상태와 배치 실행 상태를 조회할 수 있다.\n  - **Billing Batch**\n    - 스케줄러 및 대량 처리 전담 시스템이다.\n    - Scheduler\n      - 정산서 데이터 생성: 월 1회\n      - 예약 발송 실행: 5분 주기\n      - 메시지 자동 재전송 및 복구 처리: 1~5분 주기\n    - Spring Batch\n      - 정산 배치와 예약 발송 배치를 Chunk / Partition 기반으로 수행한다.\n    - Producer 역할\n      - 예약 발송 실행 시 Redis Stream에 메시지를 발행하는 Producer 역할을 수행한다.\n  - **Message Sender**\n    - 실제 메시지 전송을 담당하는 Consumer 애플리케이션이다.\n    - 다중 인스턴스 및 다중 스레드 환경에서 메시지를 병렬로 처리한다.\n    - 메시지 채널에 따라 Email Sender, SMS Sender(Mock)로 분기 처리한다.\n\n## Redis Stream 메시지 브로커\n\n![Redis Stream Producer / Consumer 구조](./redis-stream.png)\n\nRedis Stream은 시스템 간 비동기 연결과 메시지 상태 복구를 담당한다.\n\n- Billing Batch에서 메시지 발송 요청을 produce\n- Message Sender에서 Consumer Group 기반으로 메시지를 consume\n- Admin Web에서 실패 메시지 재시도 요청 시 재발행\n\nRedis Stream을 사용함으로써 다음을 보장한다.\n\n- 메시지 유실 방지\n- 다중 Consumer에 의한 병렬 처리\n- Pending 메시지 기반 장애 복구 가능\n\n# 2. 예약 발송 처리 흐름\n\n![예약발송 시퀀스 다이어그램](./diagram.png)\n_예약 발송 선점부터 메시지 발행, 소비까지의 전체 흐름_\n\n예약 발송은 5분 주기로 실행되는 스케줄러에 의해 처리된다. 스케줄러가 기동되면 현재 시각을 기준으로 발송 대상 예약을 조회하며, 동일한 Job이 이미 실행 중인 경우 중복 실행을 방지하도록 설계했다. 각 실행은 실행 시각을 JobParameter로 전달받아 독립적인 실행 단위로 구분된다.\n\n처리의 시작 단계에서는 `scheduled_at <= now` 조건을 만족하고 `WAITING` 상태인 예약 발송만을 대상으로 선점 처리를 수행한다. 이 과정에서 예약 발송의 상태를 `WAITING`에서 `PROCESSING`으로 변경함으로써 동일한 예약 발송이 동시에 처리되는 상황을 구조적으로 차단했다.\n\n선점이 완료된 예약 발송 ID 목록은 JobExecutionContext에 저장된다. 이후 파티셔닝 및 하위 Step에서는 해당 Context를 공통으로 참조하여, 동일한 예약 발송 집합을 기준으로 메시지 생성과 발송 요청 처리를 수행한다. 이를 통해 예약 발송 단위의 처리 흐름을 명확히 분리하고, Job 내부에서 일관된 데이터 범위를 유지할 수 있도록 했다.\n\n## 예약 발송 배치 구조와 Producer 책임\n\n예약 발송은 billing-batch의 ReservedMessageScheduler가 5분 주기로 실행하며, 예약 발송 처리 전반을 Spring Batch Job으로 수행한다. Job은 예약 발송을 선점하고(`WAITING` → `PROCESSING`), 예약 발송 ID 단위로 파티셔닝한 뒤 사용자별 메시지 상태를 생성하고 Redis Stream에 발송 요청을 발행한다. 이때 billing-batch는 메시지 발송 요청을 Redis Stream에 적재하는 Producer 역할을 맡으며, 메시지 발행은 DB 트랜잭션 커밋 이후에만 수행되도록 구성해 Consumer가 항상 커밋된 데이터만 조회하도록 보장했다.\n\n## 병렬 처리 전략\n\n예약 발송은 예약 발송 ID 단위로 파티셔닝을 적용했다. 하나의 Job 안에서 여러 예약 발송을 병렬로 처리할 수 있도록 구성했고, 각 파티션에는 reservationId, channelCode, purposeCode를 ExecutionContext로 전달해 예약 발송 단위의 처리 흐름이 서로 간섭되지 않도록 했다. 이 구조로 예약 발송 단위의 격리를 보장하면서도 처리량을 확보할 수 있었다.\n\n## 메시지 상태 생성과 Stream 발행\n\n메시지 상태 생성은 Chunk 기반 Step으로 구성했다. Reader에서 예약 발송 대상 사용자 ID 목록을 조회하고, Processor에서 각 사용자에 대해 MessageSendResult를 생성한 뒤, Writer에서 대량 Insert 부하를 줄이기 위해 Chunk 10,000건을 500건 단위로 분할 Insert하도록 했다. 메시지 발행은 트랜잭션 커밋 이후에 수행되도록 구성했으며, 이 규칙이 깨지면 Consumer가 DB 조회에 실패하거나 누락이 발생할 수 있기 때문에 “커밋 이후 발행”을 일관된 원칙으로 고정했다.\n\n또한 예약 발송 완료 처리는 별도 Tasklet에서 수행해 메시지 상태 생성/발행 단계와 분리했고, 예약 발송 상태는 최종적으로 SENT로 변경되도록 했다. 대량 메시지 상태를 한 번에 생성하는 과정에서 CPU 사용량이 급증하는 문제가 있어 admin-web에서 즉시 생성하는 방식은 배제했고, 대량 작업을 billing-batch에서 순차적(batch)으로 수행하도록 역할을 분리했다.\n\n# 3. 예외 상황 및 복구 설계\n\n![메시지 상태 기본 분기](./status1.png)\n_메시지 상태 기본 분기_\n\n비동기 메시지 처리에서 가장 문제가 되는 상황은 Consumer 장애 또는 예외로 인해 Redis Stream에 Pending 메시지가 누적되는 경우다. 이 상태에서 ACK가 되지 않으면 메시지는 재처리되지 않고 `PROCESSING`으로만 남기 때문에, Pending을 주기적으로 점검하고 회수하는 복구 루틴이 필요했다.\n\n## timeout 메시지 처리\n\n![XPENDING 타임아웃 처리](./timeout.png)\n_Redis Stream timeout 메시지 상태별 처리_\n\n복구 흐름은 다음 원칙으로 설계했다. 먼저 XPENDING으로 Consumer Group 기준 Pending 메시지를 조회하고, 마지막 전달 이후 idle 시간이 특정 timeout을 초과한 메시지에 한해서만 복구 대상으로 선정했다. 이후 XCLAIM으로 Recovery 전용 Consumer가 메시지를 회수해 “이 메시지는 내가 다시 처리하겠다”는 소유권을 가져오고, 회수한 메시지는 무조건 DB 상태를 기준으로 재처리 여부를 판단하도록 했다. Redis만 보고 재처리하면 중복 처리 위험이 있기 때문에, 최종 판단 기준은 항상 DB의 message_send_results 상태 코드로 고정했다.\n\n상태별 처리 정책은 다음과 같이 정리했다. 첫째, `SUCCESS`/`FAILED`/`EXCEEDED`처럼 이미 최종 상태인 경우에는 재처리를 하지 않고 ACK만 수행해 Pending을 정리한다. 둘째, `WAITING` 상태라면 실제 전송이 아직 시작되지 않은 상태이므로 동일 payload로 Redis Stream에 다시 발행(XADD)하여 재큐잉하고, 기존 Pending 엔트리는 ACK로 정리한다. 셋째, `PROCESSING` 상태로 오래 남아있는 경우는 전송 도중 Consumer가 죽었거나 예외로 종료된 케이스로 간주하고, DB 상태를 `PROCESSING`에서 FAILED로 확정 업데이트한 뒤 ACK 처리해 “재시도 스캔 대상”으로 넘긴다. 넷째, DB에 레코드가 없거나 payload가 불완전한 경우는 유령 메시지로 보고 ACK 후 정리한다.\n\n이 구조를 통해 Consumer가 중단되더라도 메시지가 유실되지 않고, 재처리 여부가 DB 상태에 의해 결정되므로 중복 발송 가능성을 낮출 수 있었다. 또한 예외 발생 시 즉시 ACK하지 않고 Pending에 남기도록 해서, 일시적인 장애는 복구 스케줄러가 회수해 처리할 수 있게 했다.\n\n## 정산서 전송 실패 시 메시지 자동처리\n\n정산서 전송일 경우 메시지는 EMAIL 채널로 전송되며 1% 확률로 실패한다. 100만 건을 처리하는데 총 3회의 전송이면 실패율이 1건이 되므로 최대 재전송 시도는 2회로 설정하였다.\n\n최대 재전송 횟수만큼 재전송을 실행하고, 실패하면 SMS 채널로 전환하여 전송한다. 실패 여부는 message_send_result 데이터에 상태코드로 저장되며, 스케줄러에 의해 1분 마다 자동 재전송이 시도된다.\n\n![자동 재전송 분기](./status2.png)\n_정산서 자동 재전송 상태 분기_\n\n이메일 전송이 실패하면 상태는 `FAILED`로 변경되고 retry_count가 증가하며, retry_count가 최대 재시도 횟수(2회) 미만인 경우 다시 `PROCESSING` 상태로 진입해 재전송을 시도한다. 재시도 횟수가 2회를 초과하면 더 이상 이메일 재시도를 수행하지 않고 `EXCEEDED` 상태로 전환한다.\n\n정산서 메시지가 `EXCEEDED` 상태가 되면 이메일 대신 SMS 채널로 fallback 발송을 수행한다. SMS 발송은 실패하지 않는다는 가정 하에 처리되며, SMS 전송이 완료되면 최종적으로 `SUCCESS` 상태로 확정된다. 이 흐름을 통해 정산서 메시지는 외부 시스템 장애나 일시적인 오류가 발생하더라도 시스템이 개입 없이 자동으로 복구를 시도하고, 최종적으로 사용자에게 전달되도록 설계했다.\n\n## 정산서 이외의 전송 실패 메시지는 수동 재처리\n\n![수동 재전송 분기](./status3.png)\n_정산서 이외 메시지 수동 재전송 상태 분기_\n\n정산서 목적이 아닌 일반 메시지는 자동 재시도 대상에서 제외했다. 일반 메시지는 템플릿 오류, 발송 대상 설정 오류 등 관리자의 판단이 필요한 실패 원인이 많다고 판단했기 때문이다. 이러한 메시지가 실패하면 `FAILED` 상태로 종료되며, 관리자 화면에서 수동 재시도 요청이 들어온 경우에만 재처리 흐름으로 다시 진입한다.\n\n수동 재시도가 요청되면 메시지 상태는 `FAILED`에서 `WAITING으로` 변경되고 retry_count가 증가한다. 이후 해당 메시지는 새로운 발송 요청과 동일하게 Redis Stream에 다시 발행되며, Consumer에서 `PROCESSING` 상태로 선점된 뒤 재전송을 시도한다. 이 과정에서도 자동 재시도와 동일하게 DB 상태를 기준으로 처리 여부를 판단해 중복 발송을 방지한다.\n\n이처럼 자동 재시도와 수동 재시도를 목적과 책임 주체에 따라 분리함으로써, 시스템이 처리해야 할 실패와 사람이 판단해야 할 실패를 명확히 구분할 수 있었다. 또한 모든 재처리 흐름은 DB 상태를 기준으로 수렴되도록 설계해, Consumer 장애나 중복 발행 상황에서도 메시지 처리 기준이 흔들리지 않도록 했다.\n\n결과적으로 메시지 처리 흐름은 정상 처리, 자동 복구, 수동 개입, 또는 종료(`EXCEEDED`) 중 하나로 귀결되며, 중간 상태로 방치되는 메시지는 남지 않도록 설계했다.\n\n# 회고\n\n## 잘한 점\n\n- 예약 발송 메시지 발송 구조를 설계하면서 성공 흐름보다 예외와 중단 상황을 먼저 고려했다. 예약 발송 선점 처리, 트랜잭션 커밋 이후 메시지 발행, Redis Stream Pending 복구 흐름을 통해 장애가 발생하더라도 메시지가 유실되지 않고 자동으로 복구될 수 있는 구조를 구현했다. 특히 Consumer 장애 상황을 가정하고 DB 상태를 기준으로 재처리 여부를 판단하도록 한 설계는 실제 운영 환경을 고려한 선택이었다고 생각한다.\n\n- Producer와 Consumer의 역할을 명확히 분리하고 Redis Stream과 Consumer Group을 활용해 메시지 발송을 병렬 처리하는 구조를 직접 설계하고 구현했다. 단순히 큐를 사용하는 수준이 아니라, 메시지 상태 정합성과 처리 순서를 함께 고려한 구조를 경험할 수 있었고 비동기 메시지 처리 흐름에 대한 이해도가 크게 높아졌다.\n\n- 조장으로서 팀 프로젝트를 이끌며 예약 발송 및 메시지 발송 구조처럼 복잡한 흐름을 맡아 정리하고 방향을 제시했다. 팀원들이 각자 맡은 역할을 안정적으로 수행할 수 있도록 작업 범위를 나누고, 문제 발생 시 원인과 해결 방향을 공유하는 데 집중했다. 그 결과 팀 프로젝트 전반이 큰 혼선 없이 진행되었고, 팀 리딩에 대해 긍정적인 평가를 받을 수 있었다.\n\n## 아쉬운 점\n\n- 메시지 발송과 정산 배치의 핵심 로직에 대해 테스트 코드를 충분히 작성하지 못했다. 대량 처리 특성상 수동 테스트와 로그 분석에 의존한 부분이 많았고, 예약 발송 Step이나 메시지 상태 전이 로직을 자동으로 검증할 수 있는 테스트 환경을 구축하지 못한 점이 아쉬움으로 남는다.\n\n- 메시지 상태 저장과 발송 과정에서 발생한 CPU 병목을 구조적으로 완화하는 방향까지는 도달했지만, DB 분리나 샤딩과 같은 근본적인 확장 구조까지는 적용하지 못했다. 병목을 인지하고 개선 방향을 도출했으나, 프로젝트 범위와 시간 제약으로 완전한 해결까지 이어지지 못한 한계가 있었다.\n\n- 기획 단계에서 예약 발송 정책과 메시지 처리 조건에 대한 고민이 충분하지 못했다. 실제 구현 과정에서 대량 메시지 생성 시 부하 문제나 상태 관리 책임에 대한 재설계가 필요했고, 초기 기획 단계에서 이러한 부분을 더 깊이 고려했으면 구조 변경 비용을 줄일 수 있었을 것이라고 느꼈다.\n\n- 정산 및 메시지 발송 로직에서 실제 서비스 수준의 복잡한 계산 조건이나 예외 케이스를 모두 반영하지는 못했다. 이번 프로젝트에서는 대용량 처리 구조와 안정성에 초점을 맞췄기 때문에 계산 로직 자체는 단순화된 정책 위주로 구현되었고, 이 부분은 한계로 남았다.\n","frontmatter":{"date":"2026년 1월 27일","slug":"til-20260127","title":"대용량 정산·메시지 발송 시스템 종합 프로젝트 회고","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","spring","java","redis"],"thumbnail":null}},{"id":"da82776d-320a-5e32-869b-b76c2ea404c5","excerpt":"배경 메시지 발송 시스템에서 대량의 메시지를 효율적으로 처리하기 위해 Redis Stream을 메시지큐로 활용하고, Docker 컨테이너를 통해 4개의 Consumer를 병렬로 운영하는 구조를 구축했다. 이 과정에서 발생한 여러 문제들과 해결 과정을…","body":"\n## 배경\n\n메시지 발송 시스템에서 대량의 메시지를 효율적으로 처리하기 위해 Redis Stream을 메시지큐로 활용하고, Docker 컨테이너를 통해 4개의 Consumer를 병렬로 운영하는 구조를 구축했다. 이 과정에서 발생한 여러 문제들과 해결 과정을 정리한다.\n\n## 아키텍처 개요\n\n- **Producer**: Spring 애플리케이션에서 메시지 발송 요청을 Redis Stream에 발행(admin-web)\n- **Consumer**: 4개의 Docker 컨테이너에서 각각 실행되며 메시지를 병렬 처리(message-sender)\n- **메시지큐**: Redis Stream (Consumer Group 방식)\n\n### Docker을 이용하여 환경 구성\n\n다음과 같이 실행하면 Consumer를 여러 개 실행시킬 수 있다.\n\n```\ndocker compose up -d --build --scale message-sender=4\n```\n\n![message-sender=4](./4message-sender.png)\n\n## 발생한 문제와 해결 과정\n\n### 1. 트랜잭션 타이밍 이슈: Race Condition\n\n#### 문제 흐름\n\n1. Producer가 DB에 메시지 상태 저장 (커밋 이전)\n2. Redis Stream에 메시지 발행\n3. Consumer가 메시지를 읽고 DB 조회 시도\n4. **커밋 전이라 데이터가 없어서 예외 발생** → 컨테이너 종료 및 재시작\n5. 4개 컨테이너가 각각 한 번씩 재시작되며 초기 4건의 미처리 발생\n\n- 메시지 처리 중, userId 별 메시지 상태 데이터를 Insert하며 `streamProducer.publish` 발행\n  - requestMessage가 종료되면 트랜잭션이 종료되어 메세지 상태 결과가 DB에 반영됨\n\n  ```java\n    @Transactional\n  public void requestMessage(Long messageReservationId) {\n    markProcessing(messageReservationId);\n    MessageReservation reservation =\n        messageReservationRepository.findByIdWithRelations(messageReservationId)\n            .orElseThrow();\n\n    ...\n\n    for (Long userId : userIds) {\n      MessageSendResult result = messageSendResultRepository.save(\n          MessageSendResult.createFrom(reservation, userId, waitingStatus)\n      );\n      streamProducer.publish(result.getId(), channelTypeCode, purposeTypeCode);\n    }\n\n    markSent(reservation.getId());\n  }\n  ```\n\n- Consumer에서는 위 트랜잭션이 끝나기 전, Stream에 전송된 메세지들을 읽으려고 시도하지만, Commit 되지 않았기 때문에 메세지 상태 정보를 찾을 수 없음 → 예외 발생 및 컨테이너가 종료되고 재실행됨\n  - 예외 발생시 애플리케이션 종료되는 이유: Redis Listener에서 예외를 처리하여 종료되고있음\n\n  ```java\n  public void process(MessageRequestDto dto) {\n\n  \tMessageSendResult result =\n          messageSendResultRepository.findById(dto.getMessageSendResultId())\n              .orElseThrow(() -> new IllegalArgumentException(\"메시지 없음\"));\n    ...\n  }\n  ```\n\n- 그 결과, 각 message-sender 컨테이너가 한번씩 재실행되며 초기 4건의 미처리가 발생하는것으로 추측됨\n  - code_id = 7: WAITING(발송 대기)\n  - code_id = 8: SUCCESS(발송 성공)\n    ![초기 4건의 미처리](./status7.png)\n\n#### 해결방법: `TransactionSynchronizationManager`를 활용해 트랜잭션 커밋 후에 메시지 발행\n\n- afterCommit 콜백에서 publish 처리\n  - afterCommit: 커밋 정상처리 후 실행되는 메소드\n\n```java\n@Transactional\npublic void requestMessage(Long messageReservationId) {\n  markProcessing(messageReservationId);\n\n  MessageReservation reservation =\n      messageReservationRepository.findByIdWithRelations(messageReservationId)\n          .orElseThrow();\n  ...\n\n  List<Long> resultIds = new ArrayList<>(); // 이후 메시지 발송을 위해 발송상태ID 저장\n\n  for (Long userId : userIds) {\n    MessageSendResult result = messageSendResultRepository.save(\n        MessageSendResult.createFrom(reservation, userId, waitingStatus)\n    );\n    resultIds.add(result.getId());\n  }\n\n  TransactionSynchronizationManager.registerSynchronization(\n      new TransactionSynchronization() {\n        @Override\n        public void afterCommit() {\n          for (Long resultId : resultIds) {\n            streamProducer.publish(resultId, channelTypeCode, purposeTypeCode);\n          }\n        }\n      }\n  );\n\n  markSent(reservation.getId());\n}\n```\n\n### 2. ACK 처리 실패: Serializer 불일치\n\n#### 문제 원인\n\n모든 메시지가 Pending 상태에서 ACK 처리가 되지 않았다. Redis Stream에서 ACK가 성공하려면 stream key, group, messageId가 정확히 일치해야 하는데, `RedisTemplate<String, Object>` 사용 시 serializer가 명시적으로 설정되지 않아 바이트 레벨에서 불일치가 발생했다.\n\n![Pending 메시지 전 건](./pending-all.png)\n\n#### 해결 방법: `StringRedisTemplate`로 변경하여 올바른 serializer가 기본 설정되도록 함\n\n- 변경 전: `RedisTemplate<String, Object>`\n- 변경 후: `StringRedisTemplate` 사용\n\n```java\nredisTemplate.opsForStream().acknowledge(\n    message.getStream(),\n    group,\n    message.getId()\n);\n```\n\n![전 건 ACK 처리됨](./ack.png)\n\n## 최적화 과정\n\n### 각 Consumer 별 메시지 분배를 균등하게 개선\n\n#### 초기 문제\n\n각 메시지 처리에 소요되는 시간은 1초인데, pollTimeout을 5초로 설정했더니 메시지가 균등하게 분배되지 않았다.\n\n- message-sender-1: 1건\n- message-sender-2: 1건\n- message-sender-3: N-3건\n- message-sender-4: 1건\n\n한 번 대기 상태에 빠진 Consumer는 계속 대기하는 경향이 있었다.\n\n### 개선\n\n- 1. **pollTimeout 조정**: 5초 → 500ms로 단축\n  - 여전히 각 consumer 별 pending 메시지는 균일하게 분배되지 않음\n  - onMessage에서 각 메세지 발송을 처리하기 전 우선 처리할 메시지를 consumer가 선점하고있음\n    ![pollTimeout 조정 후](./poll-timeout.png)\n\n- 2. **batchSize 설정**: 10으로 설정하여 10건씩 균일하게 할당\n  - batch로 묶여있으면 1건 처리 실패 시 batch 단위가 재처리 대상이 되므로 실패율을 고려해 10건으로 설정\n  - 각 consumer가 10건씩 할당받고, 순차적으로 처리 후 균일하게 할당됨\n    ![batchSize 10으로 설정 후](./batch10.png)\n\n## 추가 고려사항 및 향후 개선 방향\n\n### 메시지 상태 초기 저장 시 CPU 사용량 이슈\n\n![CPU 사용량](./cpu.png)\n\n메시지 발송 요청 시, 대략 2만건 이상의 대량의 메시지 상태를 한 번에 저장하는 과정에서 CPU 사용량이 급격히 증가하는 문제가 확인되었다.\n\n이에 따라 메시지 상태 초기 저장 및 Stream 발행 책임을 어느 컴포넌트에서 처리할지에 대한 구조적 검토가 필요하다.\n\n- **admin-web에서 즉시 처리**\n  - API 응답 지연 및 CPU 부하 집중 가능성\n- **별도 batch 성격의 컴포넌트(billing-batch)에서 처리**\n  - 예약 발송, 재시도 처리와 함께 순차적(batch) 처리 가능\n  - 시스템 부하 분산 측면에서 유리\n\n### 처리 속도 및 처리량 한계\n\n메시지 전송 지연을 1초로 가정할 경우, 단일 Consumer는 약 10초에 9건 수준의 처리량을 보였으며, 100만 건 처리 시 약 12.9일이 소요되는 것으로 계산되었다.\n\nConsumer를 4개로 확장하더라도 약 3.23일이 소요되어, 대량 처리 환경에서는 추가적인 처리량 개선이 필요하다.\n\n이를 위해 다음과 같은 개선 방향을 검토 중이다.\n\n- Batch Insert 및 Redis Pipeline 적용을 통한 RTT 감소\n- Consumer 내부 스레드 병렬 처리\n- Consumer 컨테이너 수 조절\n- pollTimeout, batchSize 등 Stream 설정값 튜닝\n\n### message-sender 컨테이너 실행 시점 관리 필요성\n\n현재 구조에서는 message-sender 컨테이너가 항상 Redis Stream을 polling하는 형태로 동작한다.\n\n그러나 실제 메시지 발송이 발생하는 시간대가 제한적인 경우, 불필요한 polling으로 인한 리소스 낭비가 발생할 수 있다.\n\n- 메시지 발송이 없는 시간대에도 지속적인 poll 수행\n- CPU 및 Redis 부하 증가 가능성\n\n이에 따라 다음과 같은 방식의 개선을 고려하고 있다.\n\n- **예약 발송 또는 재전송 요청이 발생하는 시점에만 Consumer 활성화**\n- Spring의 `StreamMessageListenerContainer`를 활용하여\n  Listener의 생명주기를 명시적으로 관리\n\n```java\ncontainer.start();\ncontainer.stop();\n\n```\n\n메시지 발송이 필요한 구간에서만 Consumer를 활성화하고, 불필요한 리소스 사용을 줄이는 방향으로 개선할 계획이다.\n\n## 배운 점\n\n1. **트랜잭션 경계를 명확히 해야 한다**: 특히 메시징 시스템과 DB를 함께 사용할 때 트랜잭션 커밋 시점이 중요하다.\n2. **Redis 직렬화는 명시적으로**: Template 타입에 따라 serializer가 달라질 수 있으므로 주의해야 한다.\n3. **분산 시스템의 예외 처리**: 예외로 인한 서비스 중단보다는 재처리 가능한 구조가 더 안정적이다.\n4. **메시지큐 설정의 중요성**: pollTimeout, batchSize 같은 설정값이 처리량과 분배에 큰 영향을 미친다.\n","frontmatter":{"date":"2026년 1월 19일","slug":"til-20260119","title":"Redis Stream 기반 메시지큐의 다중 Consumer 환경 구축 및 트러블슈팅","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","spring","java","redis"],"thumbnail":null}},{"id":"380d630f-ca56-5484-9997-ce9026b39c86","excerpt":"과거에 JPA를 사용해 여러 건의 데이터를 insert할 때, bulk insert를 적용하려면 설정이 복잡하다는 이유로 결국 하나씩 insert하는 방식을 사용했던 기억이 있습니다. 하지만 이번 종합 프로젝트에서 100만~500만 건 규모의…","body":"\n> 과거에 JPA를 사용해 여러 건의 데이터를 insert할 때, bulk insert를 적용하려면 설정이 복잡하다는 이유로 결국 하나씩 insert하는 방식을 사용했던 기억이 있습니다.\n>\n> 하지만 이번 종합 프로젝트에서 100만~500만 건 규모의 데이터를 다뤄야 하는 상황이 생겼고, 기존 방식으로는 성능과 안정성 모두 한계가 명확하다고 느꼈습니다.\n>\n> 이 문제를 해결하기 위해 insert 쿼리 수를 줄이는 Batch Insert 방식과, 대량 데이터를 구조적으로 처리할 수 있는 Spring Batch에 대해 본격적으로 알아보게 되었습니다.\n\n## 1. 단건 Insert 방식의 구조적 한계\n\n가장 단순한 데이터 저장 방식은 반복문을 통해 데이터를 하나씩 insert하는 방식입니다.\n\n```java\nfor (inti=0; i <1_000_000; i++) {\nDummyDatadata=newDummyData(\"dummy-\" + i, i);\n    repository.save(data);\n}\n\n```\n\n이 방식은 구현은 직관적이지만, 데이터 양이 많아질수록 구조적인 한계를 드러냅니다. 100만 건의 데이터를 저장하는 경우, insert 쿼리는 그대로 100만 번 실행되고, 그만큼 DB와의 통신과 트랜잭션 처리도 반복됩니다.\n\n이로 인해 다음과 같은 문제가 발생합니다.\n\n- **insert 쿼리 수만큼 DB와의 네트워크 왕복이 발생**해 성능이 급격히 저하됩니다.\n- **트랜잭션 단위 설정이 애매**해집니다. 크게 잡으면 실패 시 롤백 비용이 크고, 작게 잡으면 커밋이 과도하게 발생합니다.\n- **처리 흐름 관리가 어렵습니다.** 중간 실패 시 어디까지 처리됐는지 추적하거나 재시작 로직을 직접 구현해야 합니다.\n\n즉, 이 방식의 문제는 **쿼리 실행과 DB 통신이 과도하게 많이 발생하는 구조 자체에 있습니다.**\n\n### JPA 환경에서 Bulk Insert가 어려웠던 이유\n\n그렇다면 JPA에서 Bulk Insert를 사용하면 되지 않을까 생각할 수 있습니다. 하지만 실제로 JPA에서는 Bulk Insert를 자연스럽게 적용하기가 쉽지 않습니다.\n\nJPA는 엔티티 단위의 영속성 관리에 초점이 맞춰져 있어, 기본 동작만으로는 insert 시 개별 SQL이 생성됩니다. 특히 IDENTITY 전략을 사용하는 경우, DB에서 생성된 PK 값을 즉시 받아와야 하기 때문에 batch insert 자체가 사실상 불가능해집니다.\n\nbatch insert를 적용하려면 추가 설정, 전략 변경, flush/clear 타이밍 제어 등이 필요하고, 이 과정에서 설정 복잡도가 급격히 올라갑니다. 과거에는 이러한 이유로 bulk insert 적용을 포기하고 단건 insert 방식을 선택했던 경험이 있었습니다. 하지만 이번처럼 수백만 건 단위의 데이터를 다뤄야 하는 상황에서는 이 방식이 더 이상 현실적인 선택지가 아니라고 판단하게 되었습니다.\n\n---\n\n## 2. Batch Insert(Bulk Insert) 적용과 성능 비교 포인트\n\n이러한 한계를 해결하기 위해 선택한 방식이 **Batch Insert**입니다. Batch Insert는 여러 건의 데이터를 묶어 한 번에 DB로 전달함으로써, 애플리케이션과 DB 사이의 통신 횟수를 크게 줄이는 방식입니다.\n\n예를 들어 단건 insert 방식과 batch insert 방식은 다음과 같은 차이가 있습니다.\n\n```sql\n-- 단건 Insert\nINSERT INTO dummy_data (name, seq)VALUES ('dummy-1',1);\nINSERT INTO dummy_data (name, seq)VALUES ('dummy-2',2);\nINSERT INTO dummy_data (name, seq)VALUES ('dummy-3',3);\n\n-- Batch Insert\nINSERT INTO dummy_data (name, seq)VALUES\n('dummy-1',1),\n('dummy-2',2),\n('dummy-3',3);\n\n```\n\n이를 100만 건 기준으로 보면 차이는 더 명확해집니다.\n\n- **단건 insert**: 100만 번 execute → 100만 번 DB 통신\n- **batch insert (1000건 단위)**: 1000번 execute → 1000번 DB 통신\n\n데이터 양은 동일하지만, **DB와 통신하는 횟수에서 수백 배 차이**가 발생합니다.\n\n이번 실습에서는 다음 포인트를 중심으로 성능을 비교했습니다.\n\n- 전체 실행 시간\n- SQL execute 호출 횟수\n- 트랜잭션 커밋 횟수\n- 실패 시 재시작 가능 여부\n\n> 단건 insert 방식은 실행 시간이 지나치게 길어졌고, 데이터 양이 늘어날수록 성능 저하가 누적되었습니다. 반면 batch insert를 적용한 이후에는 실행 시간이 체감될 정도로 줄어들었고, 대량 데이터 처리에 훨씬 적합하다는 것을 확인할 수 있었습니다.\n\n- 단건으로 100만건을 insert했을 때는 1,927,480ms가 소요되었습니다.\n  ![단건 insert 처리 결과](./single-run-result.png)\n- 배치로 100만건을 insert했을 때는 311,859ms가 소요되었습니다.\n  ![배치 insert 처리 결과](./batch-run-result.png)\n\n현재 테스트용 데모에서는 컬럼 수가 3개인 단순한 테이블을 기준으로 측정했지만, 컬럼 수가 증가할수록 insert에 필요한 데이터 크기와 SQL 처리 비용이 함께 증가하게 됩니다.\n즉, 동일한 row 수를 처리하더라도 컬럼 수가 많아질수록 전체 실행 시간은 더 늘어날 수밖에 없습니다.\n\n아래 그래프는 단일 insert 및 batch insert 모두에서 컬럼 수 증가가 실행 시간에 직접적인 영향을 미친다는 점을 보여줍니다.\n![컬럼 수에 따른 실행 시간](./insert-row.png)\n\n# 3. Spring Batch를 적용한 이유\n\nBatch Insert 적용을 통해 insert 성능은 개선되었으나, 100만 건 이상 규모의 데이터 처리 환경에서는 다음과 같은 요구사항이 발생합니다.\n\n- 대량 데이터 처리 시 메모리 사용량을 통제하기 위한 Chunk 단위 처리\n- 처리 중 장애 발생 시 진행 지점 추적 및 재시작 가능성\n- 트랜잭션 범위를 제한하여 실패 영향 최소화\n\nSpring Batch는 이러한 요구사항을 충족하기 위한 대량 처리 전용 프레임워크로,\nJob / Step 기반 실행 구조를 통해 처리 이력을 데이터베이스에 기록하며,\nChunk 단위 트랜잭션 관리 방식을 통해 대량 데이터 처리 시 안정성을 제공합니다.\n\n이를 통해 대량 데이터 처리 과정에서 발생할 수 있는 오류에 대해\n부분 롤백 및 재시작이 가능한 구조를 구성할 수 있습니다.\n\n### Spring Batch 메타데이터 테이블 구조\n\n![Spring Batch 메타데이터 테이블](./default-table.png)\n\nSpring Batch는 Job 및 Step 실행 이력을 관리하기 위해\n\n**전용 메타데이터 테이블을 데이터베이스에 생성하여 사용합니다.**\n\n아래는 본 실습 환경에서 생성된 Spring Batch 기본 메타데이터 테이블 목록입니다.\n\n각 테이블의 역할은 다음과 같습니다.\n\n- `BATCH_JOB_INSTANCE`\n  → 동일한 Job 이름과 파라미터 조합을 기준으로 Job 인스턴스를 식별\n- `BATCH_JOB_EXECUTION`\n  → Job 실행 단위의 시작/종료 시간, 상태(SUCCESS, FAILED 등) 관리\n- `BATCH_STEP_EXECUTION`\n  → 각 Step의 실행 정보 및 처리 건수(Read / Write / Skip) 기록\n- `_CONTEXT`\n  → Job 또는 Step 실행 중 유지해야 할 상태 정보 저장\n- `_SEQ`\n  → 메타데이터 테이블에서 사용할 시퀀스 값 관리\n\n이러한 메타데이터 구조를 통해 Spring Batch는\n\n**실행 이력 추적, 실패 지점 식별, 재시작 처리**를 프레임워크 차원에서 지원합니다.\n\n즉, 대량 데이터 처리 시 발생할 수 있는 장애 상황에서도\n\n**처리 흐름을 직접 구현하지 않고 안정적인 재시작이 가능한 구조**를 제공합니다.\n\n### Job 실행 이력 및 재시작 관리 방식\n\nSpring Batch는 Job 실행 단위를 `BATCH_JOB_EXECUTION` 테이블에 기록하여\n\n각 실행의 상태와 실행 시간을 관리합니다.\n\n아래는 본 실습에서 실제로 생성된 Job 실행 이력입니다.\n\n![Job 실행 이력](./batch-job-excution.png)\n\n각 실행은 다음 정보를 기준으로 관리됩니다.\n\n- `START_TIME` / `END_TIME`\n  → Job 실행 시작 및 종료 시점\n- `STATUS`\n  → 실행 결과 상태 (COMPLETED, FAILED, STOPPED 등)\n- `EXIT_CODE` / `EXIT_MESSAGE`\n  → 실행 종료 사유 및 상세 메시지\n\n위 예시에서 확인할 수 있듯이, 2번 배치는 애플리케이션 종료로 인해 종료되었습니다.\n\n중간에 **STOPPED 상태로 종료된 실행 이력 또한 DB에 그대로 저장**되며,\n\n이후 Job을 다시 실행하면 **기존 실행을 덮어쓰지 않고 새로운 Execution으로 관리**됩니다.\n\n이러한 구조를 통해 Spring Batch는\n\n대량 데이터 처리 도중 장애가 발생하더라도\n\n**처리 이력을 기반으로 안전한 재시작을 지원**합니다.\n\n# 4. JPA vs JDBC 중 Batch 데이터 처리에 JDBC를 선택한 이유\n\n본 실습의 목적은 복잡한 도메인 로직 처리가 아닌,\n단일 테이블을 대상으로 한 100만 건 규모의 더미 데이터 생성이었습니다.\n\n아래와 같은 이유로 JPA보다는 JDBC가 적합하다고 판단했습니다.\n\n- JPA는 엔티티 관리, 영속성 컨텍스트 유지, flush/clear 처리 등으로 인해 대량 insert 환경에서 불필요한 오버헤드가 발생\n\n- 객체 매핑 및 상태 관리 비용이 증가함에 따라 순수 insert 성능 측면에서 비효율 발생 가능\n\n반면 JDBC는 SQL을 직접 실행하는 방식으로,\n엔티티 관리 및 매핑 과정이 없어 Batch Insert 성능에 집중하기에 적합합니다.\n\n이에 따라 본 실습에서는 Spring Batch의 처리 구조를 유지하면서,\nWriter 영역에 JdbcBatchItemWriter를 적용하여\n대량 데이터 insert 성능을 우선적으로 확보하는 방식으로 구성하였습니다.\n\n# 5. 실습 코드 소개 (Spring Batch + JdbcBatchItemWriter)\n\n## 처리 흐름\n\n- **Reader**: 더미 데이터를 카운터 기반으로 생성\n- **Writer**: `JdbcBatchItemWriter`로 batch insert\n\n아래는 본 실습에서 사용한 Spring Batch 처리 흐름을 간단히 나타낸 구조도입니다.\n\n![처리 흐름](./item-writer.png)\n\n본 프로젝트는 테스트용 데모를 목적으로 하여,\n현재는 별도의 연산이나 추가 액션이 필요하지 않아 Processor를 사용하지 않았습니다.\n다만 비즈니스 로직이 포함되는 경우, 데이터 가공을 위한 Processor 단계가 추가될 수 있습니다.\n\n- **Processor**: 필요 시 데이터 변환, 필터링, 검증 등의 비즈니스 로직을 수행\n  ![처리 흐름 - Processor)](./item-processor.png)\n\n## MySQL Bulk Insert 활성화 (필수 설정)\n\nMySQL에서 Bulk Insert를 제대로 사용하려면 **DB URL에 `rewriteBatchedStatements=true` 파라미터를 필수로 추가**해야 합니다.\n\n```yaml\nspring:\n  datasource:\n    url: jdbc:mysql://localhost:3306/batch_test?rewriteBatchedStatements=true\n    username: root\n    password: 1234\n    driver-class-name: com.mysql.cj.jdbc.Driver\n```\n\n**이 옵션이 없으면** 여러 INSERT 쿼리를 하나로 합쳐주지 않아 **성능 향상 효과가 없습니다.**\n\n### Batch Insert 동작 확인용 로깅 설정 (선택사항)\n\n```yaml\nspring:\n  datasource:\n    url: jdbc:mysql://localhost:3306/batch_test?rewriteBatchedStatements=true&profileSQL=true&logger=Slf4JLogger&maxQuerySizeToLog=999999\n```\n\n- `profileSQL=true`: Driver에 전송하는 쿼리를 출력\n- `logger=Slf4JLogger`: MySQL 드라이버는 기본값이 System.err이므로 필수\n- `maxQuerySizeToLog=999999`: 출력할 쿼리 길이 제한 (기본값 0이라 설정 필수)\n\n## DummyData\n\n```java\npublic record DummyData(String name, int seq) {}\n```\n\n## ItemReader (100만 건 생성)\n\n```java\n@Bean\npublic ItemReader<DummyData> dummyItemReader() {\n    return new ItemReader<>() {\n        private int count = 0;\n        private static final int MAX = 1_000_000;\n\n        @Override\n        public DummyData read() {\n            if (count >= MAX) return null;\n            count++;\n            return new DummyData(\"dummy-\" + count, count);\n        }\n    };\n}\n```\n\n`read()`가 null을 반환하면 Step이 종료되므로, 100만 건 생성 후 자연스럽게 종료됩니다.\n\n## JdbcBatchItemWriter (Batch Insert)\n\n```java\n@Bean\npublic JdbcBatchItemWriter<DummyData> dummyWriter(DataSource dataSource) {\n    return new JdbcBatchItemWriterBuilder<DummyData>()\n        .dataSource(dataSource)\n        .sql(\"INSERT INTO dummy_data (name, seq) VALUES (:name, :seq)\")\n        .beanMapped()\n        .build();\n}\n```\n\n`:name`, `:seq`는 DummyData 필드명과 자동으로 매칭됩니다.\n\n## Step (Chunk = 1000)\n\n```java\n@Bean\npublic Step dummyStep(\n    JobRepository jobRepository,\n    PlatformTransactionManager transactionManager,\n    JdbcBatchItemWriter<DummyData> dummyWriter\n) {\n    return new StepBuilder(\"dummyStep\", jobRepository)\n        .<DummyData, DummyData>chunk(1000, transactionManager)\n        .reader(dummyItemReader())\n        .processor(item -> item)\n        .writer(dummyWriter)\n        .build();\n}\n```\n\n**Chunk Size 1000**이면 \"1000건 생성 → 1000건 insert → commit\"이 반복됩니다.\n\nChunk Size를 바꾸면 커밋 횟수/배치 크기가 달라져 성능이 달라질 수 있습니다.\n\n## Job 구성\n\n```java\n@Bean\npublic Job dummyDataJob(JobRepository jobRepository, Step dummyStep) {\n    return new JobBuilder(\"dummyDataJob\", jobRepository)\n        .start(dummyStep)\n        .build();\n}\n```\n\n# 6. Identity 전략과 Batch Insert의 제약사항\n\nJPA에서 `@GeneratedValue(strategy = GenerationType.IDENTITY)`를 사용하면 **Hibernate는 JDBC 수준에서 Batch Insert를 비활성화**합니다.\n\n**이유:**\n\n- IDENTITY 전략은 DB가 자동으로 ID를 생성하므로, INSERT 전에는 ID 값을 알 수 없습니다\n- Hibernate가 채택한 'Transactional Write Behind' 방식과 충돌이 발생합니다\n- OneToMany 관계에서 부모 Entity를 대량 등록할 때, 어느 자식이 어느 부모에 매핑되는지 알 수 없는 문제가 생깁니다\n\n**해결책:**\n\n1. Auto Increment가 아닌 다른 전략(SEQUENCE, TABLE 등) 사용\n2. **JdbcTemplate의 batchUpdate를 직접 사용** (이번 실습에서 선택)\n\n### JdbcTemplate 직접 사용 예제\n\n```java\n@Repository\n@RequiredArgsConstructor\npublic class DummyDataBulkRepository {\n\n    private final JdbcTemplate jdbcTemplate;\n\n    @Transactional\n    public void saveAll(List<DummyData> dataList) {\n        String sql = \"INSERT INTO dummy_data (name, seq) VALUES (?, ?)\";\n\n        jdbcTemplate.batchUpdate(sql,\n            dataList,\n            dataList.size(),\n            (PreparedStatement ps, DummyData data) -> {\n                ps.setString(1, data.name());\n                ps.setInt(2, data.seq());\n            });\n    }\n}\n```\n\n## 마무리\n\n### 핵심 정리\n\n본 실습을 통해 대량 데이터 처리에서 가장 중요한 요소는\n\n**코드 구조보다 DB로 전송되는 쿼리의 횟수와 방식**이라는 점을 확인할 수 있었습니다.\n\n- 단건 insert 방식은 구조적으로 성능 한계가 명확하며,\n  Batch Insert 적용만으로도 실행 시간이 크게 개선되었습니다.\n- JPA의 IDENTITY 전략은 대량 insert 환경에서 제약이 존재하므로,\n  JDBC 기반 접근 방식이 더 적합한 경우가 있습니다.\n- Spring Batch를 통해 Chunk 단위 처리와 재시작 기능을 확보하면서도,\n  Batch Insert를 적용해 성능과 안정성을 동시에 확보할 수 있었습니다.\n- `rewriteBatchedStatements=true`와 같은 JDBC 설정은\n  대량 insert 성능에 직접적인 영향을 미칩니다.\n\n### Chunk 사용 시 고려할 사항\n\n- Chunk Size는 환경에 따라 성능 차이가 크므로 사전 테스트가 필요합니다.\n- Chunk 크기 증가에 따른 메모리 사용량을 지속적으로 모니터링해야 합니다.\n- Chunk 단위 트랜잭션 특성상, 실패 시 해당 범위만 롤백됩니다.\n- 멀티스레드 Step 적용을 통해 추가적인 성능 개선이 가능합니다.\n\n### 향후 개선 방향\n\n- Chunk Size별 성능 비교\n- 멀티스레드 Step 적용 및 성능 측정\n- MySQL 파라미터 튜닝\n- 파티셔닝 기반 병렬 처리 검토\n","frontmatter":{"date":"2026년 1월 13일","slug":"til-20260113","title":"Spring Batch를 이용한 대용량 데이터 처리","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","spring","java","spring-batch","jdbc","batch-insert"],"thumbnail":null}},{"id":"9b5fa884-33d4-5865-b52e-a940275f48ec","excerpt":"선착순 쿠폰 발급 기능을 구현하면서 동시성 문제를 직접 마주하게 되었습니다. 처음에는 단순한 조회-수정-저장 로직으로 충분할 줄 알았는데, 동시 요청 환경에서 예상치 못한 문제들이 발생했고,\n\n이를 해결하기 위해 여러 방법을 시도하며 각각의 장단점을…","body":"\n> 선착순 쿠폰 발급 기능을 구현하면서 동시성 문제를 직접 마주하게 되었습니다.\n>\n> 처음에는 단순한 조회-수정-저장 로직으로 충분할 줄 알았는데, 동시 요청 환경에서 예상치 못한 문제들이 발생했고,\n>\n> 이를 해결하기 위해 여러 방법을 시도하며 각각의 장단점을 정리해보았습니다.\n\n# 문제 발견\n\n선착순 쿠폰 이벤트 기능을 개발하고 있었습니다. 기본 로직은 다음과 같았습니다.\n\n1. 쿠폰 정보 조회\n2. 남은 수량 확인\n3. 수량 감소 후 저장\n4. 사용자에게 발급\n\n개발 환경에서 단건 테스트를 할 때는 문제가 없었습니다. 그런데 JMeter로 1000개의 동시 요청을 보내자 이상한 현상이 발생했습니다.\n\n100개만 발급되어야 하는 쿠폰이 실제로는 130여 개가 발급되었고, DB를 확인해보니 남은 수량이 음수로 표시되어 있었습니다.\n\n## 원인 파악\n\n로그를 추적해보니 여러 스레드가 동시에 같은 쿠폰 정보를 조회하고 있었습니다.\n\n전형적인 **Race Condition** 상황이었습니다. 여러 스레드가 동시에 같은 데이터를 읽고 수정하면서 데이터 정합성이 깨지는 것입니다.\n![Database의 Race Condition](./data-race.png)\n\n### 문제 상황\n\n- 스레드 1: 남은 수량 3 조회 → 쿠폰 발급 및 수량 1 감소\n- 스레드 2: 남은 수량 3 조회 (1의 저장 이전) → 쿠폰 발급 및 수량 1 감소\n- ...\n- 스레드 N: 남은 수량 3 조회 (1...N-1의 저장 이전) → 쿠폰 발급 및 수량 1 감소\n- 결과: 유효 수량이 3인 상태에서 3보다 큰 N번의 발급이 일어났으며, 남은 수량이 음수가 됨\n\n![멀티 스레드에서 Race Condition 발생](./race-condition.png)\n\n# 해결 과정\n\n## 비관적 락 (Pessimistic Lock)\n\n![비관적 락 동작 방식](./pessimistic.png)\n\n- 트랜잭션의 충돌이 발생한다고 가정하고, 데이터 접근 시점에 우선 락을 선점하여 다른 트랜잭션의 접근을 차단\n- 데이터베이스가 제공하는 락 기능을 사용\n- 주로 SQL 쿼리에 `SELECT ... FOR UPDATE` 구문을 사용하면서 시작하며, 버전 정보는 사용하지 않음\n- 데이터를 수정하는 즉시 트랜잭션 충돌을 감지할 수 있다.\n\n비관적 락은 DB 레벨에서 락을 거는 방식입니다. JPA에서는 `@Lock` 어노테이션을 사용하여 구현할 수 있습니다.\n\n```java\n@Lock(LockModeType.PESSIMISTIC_WRITE)\n@Query(\"select c from Coupon c where c.id = :id\")\nOptional<Coupon> findByIdWithLock(Long id);\n```\n\nSelect가 일어날 때 다음과 같이 동작합니다.\n\n```sql\nSELECT * FROM coupon WHERE id = 1 FOR UPDATE;\n```\n\n이제 쿠폰 수량 조회시, 해당 메서드를 사용하면 Update에 대한 트랜잭션에서 자동으로 락이 걸리게 됩니다.\n\n### 장점\n\n- 데이터 정합성이 매우 높음\n- 동시 수정으로 인한 충돌을 원천 차단\n- 구현 및 이해가 비교적 단순\n\n### 단점\n\n- 락 대기로 인한 **성능 저하**\n- 데드락 발생 가능\n- 트래픽이 많을수록 확장성에 불리\n\n### 적합한 상황\n\n- 충돌 가능성이 높은 경우\n- 데이터 정합성이 최우선인 경우\n- 트랜잭션 수가 많지 않은 시스템\n\n**느낀 점**\n\n데이터 정합성은 완벽하게 보장되었지만, 부하 테스트를 해보니 처리 속도가 많이 느려졌습니다. 대기하는 요청들이 쌓이면서 타임아웃이 발생하는 경우도 있었습니다. 트래픽이 많은 상황에서는 부담스러운 방식이라는 생각이 들었습니다.\n\n## 낙관적 락 (Optimistic Lock)\n\n![낙관적 락](./optimistic.png)\n\n- 트랜잭션 대부분은 충돌이 발생하지 않는다고 낙관적으로 가정\n- 락을 걸지 않고 처리한 뒤, 커밋 시점에 충돌 여부를 확인\n- 트랜잭션을 커밋하기 전까지는 트랜잭션의 충돌을 알 수 없음\n- 데이터베이스가 제공하는 락 기능이 아닌 JPA가 제공하는 버전 관리 기능을 사용\n\n낙관적 락은 버전 컬럼을 추가하고 수정 시점에 버전을 체크하여 충돌을 방지하는 방법입니다.\n\nJPA의 버전 관리 기능에 의해 작동합니다. `@Version` 어노테이션을 엔티티에 추가하면 버전 컬럼을 추가할 수 있습니다.\n\n```java\n@Version\nprivate Long version;\n```\n\n```java\npublic void issueOptimistic(Long couponId) throws InterruptedException {\n    int retryCount = 0;\n    int maxRetry = 10;\n\n    while (retryCount < maxRetry) {\n        try {\n            couponService.issueOptimistic(couponId);\n            return;\n        } catch (ObjectOptimisticLockingFailureException e) {\n            retryCount++;\n            Thread.sleep(50);\n        }\n    }\n\n    throw new IllegalStateException(\"쿠폰 발급 재시도 한도 초과\");\n}\n```\n\n이제 `Coupon` 엔티티에 버전 컬럼이 추가되고, JPA는 업데이트 시점에 업데이트 전에 읽었던 버전과 현재 버전을 체크합니다.\n\n두 버전이 달라 충돌이 발생하면 `ObjectOptimisticLockingFailureException`이 발생하고, Facade 계층에서 재시도 로직을 구현했습니다.\n\n### 장점\n\n- 락을 사용하지 않아 **성능과 확장성 우수**\n- 데드락 발생 없음\n- 읽기 위주의 시스템에 적합\n\n### 단점\n\n- 충돌 발생 시 재시도 로직 필요\n- 충돌이 잦으면 오히려 성능 저하\n- 구현 복잡도가 상대적으로 높음\n\n### 적합한 상황\n\n- 읽기 비중이 높은 경우\n- 동시 수정 가능성이 낮은 경우\n- 대규모 트래픽 환경\n\n**느낀 점**\n\n비관적 락보다는 성능이 좋았지만, 동시 요청이 많을 때 재시도가 빈번하게 발생하면서 오히려 비효율적이었습니다. 충돌이 적은 환경에서는 좋은 선택일 것 같다는 생각이 들었습니다.\n\n## Redis 분산 락 (Distributed Lock)\n\n![Redis 분산 락](./redis-lock.png)\n\n- 여러 서버 인스턴스에서 동시에 접근하더라도 하나의 요청만 임계 구역에 진입하도록 제어\n- 데이터베이스 락이 아닌 외부 저장소(Redis) 를 이용해 락 상태를 관리\n- 서버 간 메모리를 공유하지 않는 환경에서도 동일한 락을 사용할 수 있음\n- 락 획득 여부를 명시적으로 제어할 수 있음\n\nRedis 분산 락은 DB 락의 성능 문제와 단일 서버 환경의 한계를 보완하기 위해 사용했습니다.\n\n![Redis](./redis.png)\n\n이번 구현에서는 Redisson 라이브러리를 사용하여 분산 락을 적용했습니다.\n\n```java\npublic void issueCouponWithDistributedLock(Long couponId) {\n    RLock lock = redissonClient.getLock(\"coupon_lock:\" + couponId);\n    boolean isLocked = false;\n\n    try {\n        // waitTime: 락 획득을 최대 5초까지 대기\n        // leaseTime: 10초 후 자동으로 락 해제\n        isLocked = lock.tryLock(5, 10, TimeUnit.SECONDS);\n\n        if (!isLocked) {\n            throw new IllegalStateException(\"락 획득 실패\");\n        }\n\n        // === 임계 구역 ===\n        Coupon coupon = couponRepository.findById(couponId)\n            .orElseThrow();\n\n        if (coupon.getQuantity() <= 0) {\n            throw new SoldoutException();\n        }\n\n        coupon.decrease();\n        couponRepository.save(coupon);\n\n    } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        throw new RuntimeException(e);\n    } finally {\n        if (isLocked && lock.isHeldByCurrentThread()) {\n            lock.unlock();\n        }\n    }\n}\n```\n\n### 장점\n\n- 분산 환경에서 데이터 정합성 확보 가능\n- 애플리케이션 레벨에서 제어 가능\n\n### 단점\n\n- 구현 및 운영 난이도 높음\n- 네트워크 및 외부 시스템 장애에 민감\n- 잘못 설계 시 심각한 장애 유발 가능\n\n### 적합한 상황\n\n- 다중 서버에서 동일 자원을 제어해야 하는 경우\n- 배치 작업, 쿠폰 발급, 재고 차감, 스케줄러 중복 실행 방지 등\n\n**느낀 점**\n\nDB 락보다 가볍고, 멀티 서버 환경에서도 동작해서 적절한 균형점을 찾은 느낌이었습니다. 다만 Redis가 장애나면 전체 시스템에 영향을 줄 수 있다는 점은 고려해야 했습니다.\n\n## Redis Queue 방식\n\n![Redis Queue 방식](./redis-queue.png)\n\n- 동시 요청을 즉시 처리하지 않고 **큐에 적재한 뒤 순차적으로 처리**\n- 락을 사용하지 않고, 요청 흐름 자체를 직렬화(serialization)하여 동시성 문제를 제거\n- 요청 시점과 실제 처리 시점을 분리함으로써 DB 동시 접근을 최소화\n\nRedis Queue 방식은 락을 통한 동시성 제어 대신,\n\n**요청을 큐에 쌓고 하나씩 처리하는 구조**로 문제를 해결하는 접근입니다.\n\n### Producer: 요청 적재\n\n```java\npushToQueue(Long userId) {\n    redisTemplate.opsForList().rightPush(\"coupon_queue\", String.valueOf(userId));\n}\n```\n\n### Consumer: 순차 처리\n\n```java\n@Scheduled(fixedDelay = 100)\npublic void couponEventScheduler() {\n    while (true) {\n        Long userId = couponRedisRepository.popFromQueue();\n        if (userId == null) {\n            break;\n        }\n\n        if (isSoldOut) {\n            continue;\n        }\n\n        try {\n            couponService.publish(userId);\n        } catch (SoldoutException e) {\n            isSoldOut = true;\n            System.out.println(\"선착순 쿠폰이 마감되었습니다.\");\n        }\n    }\n}\n```\n\nScheduler가 주기적으로 큐에서 데이터를 하나씩 꺼내어\n쿠폰 발급 로직을 순차적으로 수행합니다.\n\n### 장점\n\n- 락 경쟁이 없어 **Race Condition 자체가 발생하지 않음**\n- DB에 동시에 접근하는 요청 수를 크게 줄일 수 있음\n- 선착순 처리 요구사항을 자연스럽게 만족\n- 구조가 비교적 단순하고 직관적임\n\n### 단점\n\n- 즉시 처리(동기 응답)가 어려움\n- 비동기 처리에 대한 사용자 경험(대기, 알림 등) 고려 필요\n- Scheduler가 단일 Consumer로 동작한다는 전제 필요\n- 대량 트래픽 시 큐 적체 관리 필요\n\n### 적합한 상황\n\n- 선착순, 예약, 응모와 같이 **순서가 중요한 기능**\n- 즉시 응답이 필수가 아닌 경우\n- 대량의 동시 요청을 안정적으로 제어해야 하는 경우\n- 처리량보다 **정합성과 안정성**이 중요한 시스템\n\n**느낀 점**\n\n락을 사용하지 않고도 동시성 문제를 구조적으로 해결할 수 있다는 점이 인상적이었습니다.\n\n요청을 순차적으로 처리하기 때문에 안정성이 높았고,\nDB에 동시에 접근하는 상황을 거의 만들지 않는다는 점에서 가장 깔끔한 해결 방식이라는 느낌을 받았습니다.\n\n다만 즉시 응답을 주기 어렵기 때문에, 비동기 처리 결과를 사용자에게 어떻게 안내할지에 대한 고민은 필요하다고 느꼈습니다.\n\n## 확장 가능성 및 추가로 고민해볼 내용\n\n이번에는 Redis Queue를 활용해 동시성 문제를 구조적으로 해결했지만,\n\n이 방식 역시 **모든 상황에서 완벽한 해답은 아니라는 점**도 함께 느낄 수 있었습니다.\n\nRedis Queue 방식은 안정적이고 구현이 단순하다는 장점이 있지만,\n트래픽이 더 커지거나 처리량을 더 끌어올려야 하는 상황에서는\n\n다음과 같은 한계가 존재합니다.\n\n- 단일 Consumer(Scheduler) 기반 처리\n- 처리 속도가 큐 소비 속도에 의존\n- 장애 발생 시 메시지 유실 가능성\n\n이러한 한계를 보완하기 위한 다음 단계로는\n\n**이벤트 기반 아키텍처**를 고려해볼 수 있을 것 같습니다.\n\n### 고려해볼 수 있는 확장 방향\n\n- **Kafka 기반 이벤트 처리**\n  - 대량 트래픽을 안정적으로 처리\n  - 메시지 영속성 보장\n  - 장애 발생 시에도 데이터 유실 최소화\n- **Consumer Group을 통한 수평 확장**\n  - 여러 Consumer가 동시에 메시지를 분산 처리\n  - 처리량 증가에 유연하게 대응 가능\n- **메시지 재처리(Replay) 전략**\n  - 장애나 오류 발생 시 특정 시점부터 재처리 가능\n  - 운영 안정성 확보에 유리\n\n이번 구현에서는 Redis Queue로 충분했지만,\n트래픽 규모가 커질수록 **“단순한 큐”에서 “스트리밍 플랫폼”으로의 전환**이\n자연스러운 수순이라는 점을 느꼈습니다.\n\n## 마무리\n\n이번 실습을 통해,\n동시성 문제에는 **하나의 정답이 존재하지 않는다**는 점을 다시 한 번 느꼈습니다.\n\n- **Pessimistic Lock**: 정합성은 확실하지만 성능 부담 큼\n- **Optimistic Lock**: 충돌이 적을 때 효과적\n- **Redis 분산 락**: 멀티 서버 환경에서 현실적인 선택지\n- **Redis Queue**: 구조적으로 안전하지만 비동기 처리 고려 필요\n\n결국 중요한 것은\n\n“어떤 락을 쓰느냐”가 아니라,\n\n**시스템의 특성, 트래픽 패턴, 사용자 경험을 종합적으로 고려해\n가장 적절한 방식을 선택하는 것**이라는 점이었습니다.\n\n특히 동시성 문제는\n\n단순히 락을 걸어 해결할 수 있는 문제가 아니라,\n\n**아키텍처와 흐름 설계의 문제**라는 점이 인상 깊게 남았습니다.\n","frontmatter":{"date":"2026년 1월 6일","slug":"til-20260106","title":"Spring - 동시성 문제와 선착순 쿠폰 발급 처리 방식 정리","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","spring","java","conccurency","redis"],"thumbnail":null}},{"id":"97e22271-8022-53dd-8372-f3703a461b42","excerpt":"이번주에는 Spring 프로젝트에서 서비스 계층 테스트를 어떻게 작성하는지와 @SpringBootTest 기반 통합 테스트와의 차이점을 실제로 작성한 테스트 코드를 기준으로 정리해보았습니다.\n\n테스트를 단순한 동작 확인이 아닌, 로직 변경에 대비하기…","body":"\n> 이번주에는 Spring 프로젝트에서 **서비스 계층 테스트를 어떻게 작성하는지**와 **@SpringBootTest 기반 통합 테스트와의 차이점**을  \n> 실제로 작성한 테스트 코드를 기준으로 정리해보았습니다.\n>\n> 테스트를 단순한 동작 확인이 아닌,\n> **로직 변경에 대비하기 위한 안전장치**라는 관점에서 접근했습니다.\n\n# 1. 테스트를 작성하게 된 배경\n\n기존에는 기능 구현 이후 애플리케이션을 직접 실행한 뒤,  \nAPI를 호출하여 정상 동작 여부를 확인하는 방식으로 검증을 진행했습니다.\n\n이 방식에는 다음과 같은 한계가 있었습니다.\n\n- 테스트를 위해 매번 서버를 실행해야 한다\n- 오류 발생 시 어느 계층의 로직에서 문제가 발생했는지 즉시 파악하기 어렵다\n- 리팩터링 이후 기존 기능이 정상 동작하는지 빠르게 확인하기 어렵다\n\n이러한 문제를 해결하기 위해  \n**서비스 로직을 독립적으로 검증할 수 있는 단위 테스트(Unit Test)** 를 작성하게 되었습니다.\n\n이번 테스트에서는 컨트롤러나 Repository를 포함한 전체 흐름이 아니라,  \n**서비스 계층의 로직만을 테스트 대상으로 한정**했습니다.\n\n![테스트의 종류](./test-types.png)\n\n위와 같이, 기존 검증 방식은 화면을 직접 실행하여 확인하는 UI Test와, Postman을 활용해 API 전체 흐름을 검증하는 End-to-End 테스트에 해당합니다.\n\n이 방식은 코드가 의도대로 동작하는지 확인하기 위해 직접 실행시켜서 결과를 확인하는 방식이었습니다.\n\n**Unit Test(단위 테스트)** 는 컨트롤러나 DB, 외부 환경과 분리된 상태에서 **코드 영역의 로직(주로 서비스 계층)** 이\n주어진 조건에 따라 올바르게 동작하는지를 검증하는 테스트를 의미합니다.\n\n![Unit Test가 필요한 이유](./unit-test.png)\n\n위 그림은 단위테스트가 없으면\n개발 → 수동 테스트 → 실패 → 수정 → 재실패의 악순환 루프에 빠지기 쉽다는 것을 보여주는 그림입니다.\n\n단위 테스트를 통해\nUI Test나 End-to-End Test를 수행하기 이전에\n코드 영역의 로직이 의도대로 동작하는지를 먼저 검증할 수 있으며,\n이로 인해 이후 테스트 단계에서 발생하는 오류의 원인을\n보다 쉽게 추적할 수 있습니다.\n\n서비스 계층은 요청 처리 과정에서 중복 검증, 예외 발생 여부 판단, 응답 객체 생성 등과 같이 비즈니스 규칙에 대한 핵심적인 판단 로직을 담당합니다.  \n이러한 로직은 DB 연동이나 웹 요청 여부와 관계없이 독립적으로 검증할 수 있는 영역입니다.\n\n따라서 이번 테스트에서는 외부 의존성은 Mockito로 Mock 처리하고, **서비스 로직의 판단과 분기 자체가 올바른지에 집중하기 위해** 서비스 계층 단위 테스트 방식을 선택했습니다.\n\n이 방식은 로직 변경이나 리팩터링 이후에도 비즈니스 규칙이 의도대로 유지되고 있는지를 빠르게 확인할 수 있다는 장점이 있습니다.\n\n# 2. 테스트 방식 개요\n\n![JUnit 5와 Mockito](./test.png)\n\n이번 테스트는 **JUnit 5와 Mockito를 사용한 서비스 계층 단위 테스트**입니다.\n\n### 주요 특징\n\n- Spring Context를 실행하지 않습니다\n- Repository, Encoder 등 외부 의존성은 모두 Mock 처리합니다\n- 서비스 로직만 단독으로 검증합니다\n\n즉, **의존성은 가짜로 생성된 객체(Mock), 로직은 실제 코드**를 사용하는 구조입니다.\n\nUserRepository의 경우, JpaRepository에 의해 구현체가 결정되므로 Mock 반환값을 내려주어 테스트할 수 있습니다.\n\n![JpaRepository를 상속받는 UserRepository interface](./jpa-repository.png)\n\nIntelliJ에서는 다음과 같이 Test 코드에서 생성된 테스트 버튼을 이용하여 직접 테스트를 실행해보고, 결과를 확인 할 수 있습니다.\n\n![IntelliJ에서 테스트 실행](./intellij.png)\n\nTest with Coverage 옵션으로 테스트를 실행하면,전체 코드 중 테스트가 실제로 실행된 코드의 비율을\nCoverage 리포트를 통해 확인할 수 있습니다.\n\n![테스트 커버리지 확인](./coverage.png)\n\n이번 테스트에서는 전체 Line Coverage 30% 이상을 목표로 설정했으며, 그 결과 다음과 같은 커버리지 결과를 확인할 수 있었습니다.\n\n# 3. 테스트 대상 구성\n\n테스트 대상은 `UserServiceImpl` 클래스입니다.\n\n회원 가입, 회원정보 변경과 같은 처리를 하고있습니다.\n\n다음과 같이 테스트 코드를 `test/` 경로에 작성했습니다.\n\n![테스트 코드 작성 위치](./test-file.png)\n\n```java\n@ExtendWith(MockitoExtension.class)\nclass UserServiceImplTest {\n    @InjectMocks\n    private UserServiceImpl userService;\n\n    @Mock\n    private UserRepository userRepository;\n\n    @Mock\n    private UserActivityRepository userActivityRepository;\n\n    @Mock\n    private PasswordEncoder passwordEncoder;\n\n    @Test\n    void 회원가입_실패_중복이메일() { ... }\n\n    @Test\n    void 회원가입_성공() { ... }\n...\n}\n```\n\n- 테스트 대상 객체: `UserServiceImpl`\n  - `@InjectMocks`\n- Mock 처리된 의존성\n  - `@Mock`\n  - 실제 DB 접근이나 암호화 로직은 수행하지 않습니다\n  - 테스트 범위를 서비스 로직으로 제한합니다\n\n# 4. 테스트 코드 작성 방식 (Given–When–Then)\n\n모든 테스트는 Given–When–Then 구조로 작성했습니다.\n\n## 실패 케이스 테스트 예시 (중복 이메일)\n\n```java\n  @Test\n  void 회원가입_실패_중복이메일() {\n    // given\n    RegisterRequest request = new RegisterRequest();\n    ReflectionTestUtils.setField(request, \"name\", \"홍길동\");\n    ReflectionTestUtils.setField(request, \"email\", \"test@test.com\");\n    ReflectionTestUtils.setField(request, \"password\", \"password\");\n\n    User user = User.builder()\n        .id(1L)\n        .name(\"홍길동\")\n        .email(\"test@test.com\")\n        .password(\"encoded-pass\")\n        .build();\n    given(userRepository.findByEmail(request.getEmail()))\n        .willReturn(Optional.of(user));\n\n    // when & then\n    assertThrows(BadRequestException.class,\n        () -> userService.registerUser(request));\n  }\n```\n\n### Given\n\n- `ReflectionTestUtils`\n  - `RegisterRequest`가 별도의 setter를 제공하지 않기 때문에, 테스트만을 위해서 코드를 변경하지 않고 Reflection을 이용해 테스트 환경에서만 필드 값을 설정했습니다\n\n- `given`\n  - 이미 동일한 이메일이 존재한다고 가정합니다.\n  - 실제 DB 조회 없이 `userRepository.findByEmail()` 메소드는 Mockito가 정의한 값을 반환합니다.\n\n### When & Then\n\n- `assertThrows`\n  - 서비스 메소드(`registerUser`)를 실행합니다\n  - 중복 이메일일 경우 특정 예외(`BadRequestException.clas`)가 발생하는지 검증합니다\n\nWhen & Then을 한번에 쓰지 않고, 아래와 같이 실행결과를 받아 비교하는 식으로 검증할 수도 있습니다.\n\n```java\nBadRequestException exception = assertThrows(\n    BadRequestException.class,\n    () -> userService.registerUser(request)\n);\n\nassertEquals(\"이미 존재하는 이메일입니다.\", exception.getMessage());\n```\n\n## 성공 케이스 테스트 예시\n\n```java\n@Test\nvoid 회원가입_성공() {\n    // given\n    RegisterRequest request = new RegisterRequest();\n    ReflectionTestUtils.setField(request, \"name\", \"홍길동\");\n    ReflectionTestUtils.setField(request, \"email\", \"test@test.com\");\n    ReflectionTestUtils.setField(request, \"password\", \"password\");\n\n    given(userRepository.findByEmail(request.getEmail()))\n        .willReturn(Optional.empty());\n\n    given(passwordEncoder.encode(request.getPassword()))\n        .willReturn(\"encoded-password\");\n\n    User savedUser = User.builder()\n        .id(1L)\n        .name(\"홍길동\")\n        .email(\"test@test.com\")\n        .password(\"encoded-password\")\n        .build();\n\n    given(userRepository.save(any(User.class)))\n        .willReturn(savedUser);\n\n    // when\n    RegisterResponse response = userService.registerUser(request);\n\n    // then\n    assertEquals(\"홍길동\", response.getName());\n    assertEquals(\"test@test.com\", response.getEmail());\n}\n```\n\n### Given\n\n- `given(userRepository.findByEmail(...))`\n  - 해당 이메일을 가진 사용자가 존재하지 않는 상황을 가정합니다\n  - 실제 DB 조회 없이 Mockito가 `Optional.empty()`를 반환하도록 설정합니다\n- `given(passwordEncoder.encode(...))`\n  - 비밀번호 암호화 결과를 고정합니다\n  - 암호화 로직 자체가 아닌, 서비스 흐름 검증에 집중합니다\n- `given(userRepository.save(...))`\n  - 사용자 저장이 정상적으로 완료되었다고 가정합니다\n  - 저장 후 반환되는 사용자 객체를 미리 정의합니다\n\n### When\n\n- `userService.registerUser(request)`\n  - 회원가입 서비스 메소드를 실행합니다\n  - 이메일 중복 확인, 비밀번호 암호화, 사용자 저장 로직이 순차적으로 수행됩니다\n\n### Then\n\n- `assertEquals(\"홍길동\", response.getName())`\n  - 응답 객체에 사용자 이름이 올바르게 담겼는지 검증합니다\n- `assertEquals(\"test@test.com\", response.getEmail())`\n  - 응답 객체에 이메일이 정상적으로 매핑되었는지 검증합니다\n\n# 5. 단위 테스트와 `@SpringBootTest`의 차이점\n\n## @SpringBootTest란?\n\n`@SpringBootTest`는  \nSpring Boot 애플리케이션 전체를 실제로 실행하는 테스트 방식으로, 일반적으로 통합 테스트에 해당합니다.\n\n- Spring Context를 로딩합니다\n- 실제 Bean이 주입됩니다\n- 설정에 따라 실제 DB 또는 테스트 DB와 연동됩니다\n- 여러 컴포넌트가 함께 동작하므로 테스트 범위가 넓습니다\n\n## 선택 기준\n\n### 단위 테스트가 적합한 경우\n\n- 서비스 로직을 단독으로 검증하고 싶은 경우\n- 예외 처리 및 분기 로직을 확인하고 싶은 경우\n- 빠른 피드백이 필요한 경우\n- 리팩터링 시 안정성을 확보하고 싶은 경우\n\n### @SpringBootTest가 적합한 경우\n\n- JPA 매핑이 정상 동작하는지 확인할 때\n- 실제 쿼리 동작을 검증해야 할 때\n- Bean 설정 오류를 확인할 때\n- 여러 컴포넌트 간 연동을 함께 검증해야 할 때\n\n다음과 같이 JPQL로 작성한 Repository 메소드는 통합 테스트로 테스트할 필요가 있습니다.\n\n엔티티 매핑, JOIN, 조건 필터링, DTO Projection 등\n실행 시점에 오류가 발생할 수 있는 요소를 포함하고 있기 때문에,\nMock 기반 단위 테스트보다는 실제 JPA 환경에서 검증하는 통합 테스트가 적합합니다.\n\n```java\npublic interface UserActivityRepository extends JpaRepository<Post, Long> {\n\n  @Query(\"\"\"\n          SELECT\n              c.id AS id,\n              p.id AS postId,\n              b.id AS boardId,\n              b.boardName AS boardName,\n              p.title AS postTitle,\n              CASE WHEN LENGTH(c.content) > 200\n                THEN CONCAT(SUBSTRING(c.content, 1, 200), '...')\n                ELSE c.content\n              END AS content,\n              c.createdAt AS createdAt\n          FROM PostComment c\n          JOIN c.post p\n          JOIN p.board b\n          WHERE c.author.id = :userId\n            AND c.deletedAt IS NULL\n            AND p.deletedAt IS NULL\n            AND b.deletedAt IS NULL\n          ORDER BY c.createdAt DESC, c.id DESC\n      \"\"\")\n  Page<UserCommentResponse> findAllCommentsByUserId(\n      @Param(\"userId\") Long userId,\n      Pageable pageable\n  );\n}\n```\n\n하지만 이번 프로젝트에서 테스트 범위는 Service 로직으로 한정했기 때문에, `@Query`로 작성한 코드에 대해 통합 테스트를 작성하지 못한 점이 아쉬웠습니다.\n\n# 마무리\n\n이번 테스트 작성을 통해 다음과 같은 점을 정리할 수 있었습니다.\n\n- 테스트는 단순한 기능 검증을 넘어, 로직 변경이나 리팩터링 이후에도 기능의 안정성을 보장하는 역할을 합니다.\n- 모든 테스트를 @SpringBootTest로 작성할 필요는 없으며, 테스트 대상과 목적에 따라 적절한 테스트 방식을 선택해야 합니다.\n- 테스트의 목적에 맞게 단위 테스트와 통합 테스트의 범위를 구분하는 것이 중요합니다.\n","frontmatter":{"date":"2025년 12월 20일","slug":"til-20251220","title":"Spring - JUnit과 Mockito를 이용한 서비스 단위 테스트 작성하기","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","spring","java","test"],"thumbnail":null}},{"id":"7c8f697c-9832-5c5e-b5df-cb5016101104","excerpt":"이번주에는 지금까지 배운 Spring 프레임워크에 전반에 대한 내용을 기반으로, 간단한 커뮤니티 게시판을 구현하는 미니프로젝트를 진행했습니다. 총 3명이 한 팀이 되어 하루 대부분의 시간을 함께하며 기능 구현부터 테스트, 오류 수정까지 전 과정을…","body":"\n> 이번주에는 지금까지 배운 Spring 프레임워크에 전반에 대한 내용을 기반으로, 간단한 커뮤니티 게시판을 구현하는 미니프로젝트를 진행했습니다.\n>\n> 총 3명이 한 팀이 되어 하루 대부분의 시간을 함께하며 기능 구현부터 테스트,\n> 오류 수정까지 전 과정을 협업으로 수행하였고,\n> 짧은 기간이었지만 실제 프로젝트 흐름을 경험할 수 있었던 의미 있는 시간이었습니다.\n>\n> 협업을 위해 신경썼던 부분과, 로그인 하지 않았을 때도 접근 가능한 데이터가 일부 있어 고민했던 부분에 대해 공유하고자 합니다.\n\n# 미니 프로젝트 소개\n\n![미니프로젝트 게시판 화면](./board.png)\n\n이번 프로젝트는 회원가입 및 로그인이 가능한 커뮤니티 게시판을 구현하는 것을 목표로 했습니다.\n로그인하지 않은 사용자도 일부 공개 게시판의 게시글 목록과 상세 조회는 가능하도록 구성하였고,\n글 작성 및 사용자 상호작용 기능(좋아요, 북마크, 댓글)은 로그인한 사용자만 사용할 수 있도록 제한했습니다.\n\n또한 비밀 게시판의 경우에는 로그인 여부를 기준으로,\n로그인한 사용자에게만 게시글이 노출되도록 접근 제어를 구현했습니다.\n\nERD는 다음과 같습니다.\n![erd](./erd.png)\n\n## 코드 관리 - github\n\n![github 레파지토리](./repo.png)\n\n프론트엔드와 백엔드 프로젝트를 하나의 레포지토리에서 관리하되,\n디렉토리를 분리하여 각각 독립적으로 개발할 수 있도록 구성했습니다.\n\n다만 프로젝트 초반에 작업 단위와 브랜치 전략을 명확히 정하지 않은 상태에서 개발을 진행하면서,\n각자의 작업량이 늘어날수록 충돌이 자주 발생하는 문제가 있었습니다.\n이로 인해 머지 과정에서 예상보다 많은 시간이 소요되었고,\n협업 프로젝트에서 초기 브랜치 전략 수립의 중요성을 체감하게 되었습니다.\n\n프론트엔드 프레임워크로는 Next.js를 선택하였으나,\n프레임워크에 대한 이해가 충분하지 않은 상태에서 개발을 진행하다 보니\n스타일이 깨지거나 구조가 일관되지 않는 문제가 발생하기도 했습니다.\n\n이 과정에서 Codex AI 도구의 도움을 받아 코드 이해와 수정에 많은 도움을 받을 수 있었습니다.\n\n## 데이터 마이그레이션 - flyway\n\n![미니프로젝트 테이블 정의서](./doc.png)\n\n다음과 같이 테이블 정의서를 이용하여 데이터베이스 테이블 정의 이후,\nDB 스키마 변경 이력을 버전 단위로 관리하기 위해 Flyway를 도입했습니다.\n\n![데이터 마이그레이션 SQL 파일](./flyway.png)\n\nSQL 기반 마이그레이션 파일을 통해 변경 사항을 명확히 기록할 수 있었고,\n특히 팀 단위 작업에서 DB 변경 이력을 코드로 남길 수 있다는 점이 큰 장점으로 느껴졌습니다.\n\n테이블 변경사항이 많다면 환경 간 스키마 불일치 문제를 줄일 수 있었겠지만, 초기 데이터 테이블에서 수정점이 거의 없었습니다ㅎㅎ\n\n![기본 게시판 데이터](./data.png)\n\n공통으로 사용되는 게시판 초기 데이터에 대해서는\n마이그레이션 파일을 통해 INSERT 구문을 정의하여,\n환경별로 동일한 기준 데이터를 유지할 수 있도록 구성했습니다.\n\n## 파일 저장 - aws s3\n\n이번에 구현한 커뮤니티는 게시글에 이미지를 첨부할 수 있으며, 사용자 프로필 사진을 설정할 수 있습니다.\n\n파일 업로드 및 관리를 위해 AWS S3 버킷을 사용했습니다.\n\n![S3 버킷](./s3.png)\n\n서버 로컬 스토리지가 아닌 외부 스토리지를 활용함으로써,\n서버 확장 시 파일 동기화 문제를 고려하지 않아도 되며,\n파일 접근 URL을 통해 정적 리소스를 효율적으로 제공할 수 있었습니다.\n\n또한 S3 접근 권한 설정을 위해 IAM 사용자를 생성하고 Access Key를 발급받아 사용하였으며,\n버킷 정책을 직접 구성하는 과정을 통해\n단순한 파일 업로드를 넘어 보안과 접근 제어의 중요성을 함께 학습할 수 있었습니다.\n\nS3 요금 구조를 조사하는 과정에서,\n파일 다운로드 요청 비용이 상대적으로 높다는 점을 알게 되었고\n캐시 서버나 CloudFront와 같은 CDN을 활용하면\n운영 비용을 절감할 수 있다는 점도 알게 되었습니다.\n\n해당 부분은 구현에 포함하지는 않았지만 추후 적용해 보고 싶습니다.\n\n## .env / .env.example를 이용한 키 관리\n\nS3 접근키 등의 환경 변수 관리를 위해 `.env` 파일을 사용하고, `.gitignore`을 통해 github에 업로드되지 않도록 관리했습니다.\n\n실제 키 값이 없는 `.env.example` 파일을 함께 관리하여 키 값 변경사항을 관리했습니다.\n\n이를 통해 API Key, DB 정보와 같은 민감한 정보가\n레포지토리에 직접 노출되는 것을 방지할 수 있었고,\n환경별 설정 분리가 명확해졌습니다.\n\n또한 협업 시 필요한 환경 변수 목록을 공유하기 쉬워졌다는 점에서도\n효과적인 방식이라고 느꼈습니다.\n\n## 로그인 분기 처리(Security Context 기반 접근 제어)\n\n이번 프로젝트에서는 로그인 여부에 따라 접근 가능한 기능이 달라지는 요구사항이 있었습니다.\n\n이에 따라 인증이 필요한 기능과 그렇지 않은 기능을 명확히 구분하여 설계했습니다.\n\n다음은 이 미니프로젝트에서 작성한 API 목록 문서의 일부입니다.\n\n![API 목록](./api.png)\n\npermit 분류에서 `All`인 것은 접근제한을 하지 않고,\n`비공개 게시글 제외`이면 비공개 게시판과 연관된 정보의 경우 로그인/비로그인을 체크하여 적절한 정보를 반환합니다.\n\n### 1. Security 설정 관점 (permitAll, authenticated)\n\nSpring Security 설정 단계에서\n게시글 조회와 같이 공개되어야 하는 API는 permitAll()로 설정하여\n인증 여부와 관계없이 접근할 수 있도록 구성했습니다.\n\n반면, 글 작성·수정·삭제, 좋아요·댓글과 같은 상호작용 기능에 대해서는\nauthenticated() 설정을 적용하여\n인증된 사용자만 요청을 보낼 수 있도록 기본적인 접근 제한을 적용했습니다.\n\n이를 통해 URL 단위에서 1차적인 접근 제어를 수행하고,\n인증이 필요 없는 요청만 컨트롤러까지 전달되도록 구성했습니다.\n\n`SecurityFilterChain`을 설정할 때 다음과 같이 작성할 수 있습니다.\n\n```java\n    http\n        .csrf(csrf -> csrf.disable())\n\n        .authorizeHttpRequests(auth -> auth\n            // 공개 API (비로그인 접근 허용)\n            .requestMatchers(\n                \"/api/posts\",\n                \"/api/posts/**\",\n                \"/api/search/**\"\n            ).permitAll()\n\n            // 로그인 사용자만 가능한 기능\n            .requestMatchers(\n                \"/api/posts/create\",\n                \"/api/posts/**/edit\",\n                \"/api/posts/**/delete\",\n                \"/api/posts/**/like\",\n                \"/api/comments/**\"\n            ).authenticated()\n            ...\n```\n\n### 2. 컨트롤러 vs 서비스 계층에서의 체크\n\n컨트롤러 계층에서는 `@AuthenticationPrincipal`을 통해\nSpring Security Context에 저장된 인증 정보를 주입받아\n해당 정보를 서비스 계층으로 전달하는 역할만 수행하도록 구성했습니다.\n\n서비스 계층에서는 전달받은 사용자 정보를 기준으로\n현재 요청이 로그인 사용자에 의한 요청인지 여부를 판단하고,\n인증 여부에 따라 비즈니스 로직의 처리 방식을 분기하도록 구현했습니다.\n\n- 컨트롤러 / Security 설정\n  - URL 단위 접근 가능 여부에 대한 1차 필터링\n  - 인증 정보 전달 역할\n- 서비스 계층\n  - 인증 정보 존재 여부를 기준으로 로직 분기\n  - 인증 상태에 따른 데이터 조회 범위 및 처리 방식 결정\n\n위와 같이 역할 분리를 통해 접근 제어를 구현했하여\n접근 제어 로직이 컨트롤러에 과도하게 집중되지 않도록 하면서도,\n비즈니스 규칙과 밀접한 인증 판단은 서비스 계층에서 처리할 수 있었습니다.\n\n## 느낀점\n\n프로젝트 전반에서 가장 아쉬웠던 점은\n작업 단위와 브랜치 전략을 명확히 정의하지 않은 상태에서 개발을 시작한 부분이었습니다.\n이로 인해 코드 충돌이 잦았고, 협업 효율이 다소 떨어졌다고 느꼈습니다.\n\n또한 게시글 조회수, 좋아요와 같이 빈번하게 조회·갱신되는 기능에 대해\n캐시를 활용한 성능 최적화를 적용하고 싶었지만\n시간적인 여유가 부족해 구현하지 못한 점이 아쉬움으로 남았습니다.\n\n하지만 단순한 기능 구현을 넘어,\nSpring 기반 애플리케이션에서 인증, 접근 제어, 데이터 조회 구조를\n실제 요구사항에 맞게 설계해 보는 경험을 할 수 있었습니다.\n\n짧은 기간이었지만 팀원들과 지속적으로 의견을 공유하며\n문제 상황을 함께 해결해 나가는 과정에서,\n개인 과제와는 다른 협업 프로젝트의 흐름을 경험할 수 있었습니다.\n\n다음 프로젝트에서는 초기 설계 단계에서\n협업 방식과 성능 요소까지 함께 고려하여 진행하고 싶습니다.\n","frontmatter":{"date":"2025년 12월 16일","slug":"til-20251216","title":"Spring 미니 프로젝트 회고","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","java","spring"],"thumbnail":null}},{"id":"7a8377d0-27b4-5bb7-99e1-e33ecb1938fa","excerpt":"오늘은 CSRF와 CORS에 대해 브라우저 보안 모델의 관점에서 체계적으로 정리했습니다. 이전에는 단편적인 지식만 보유하고 있었으나, 두 개념이 서로 다른 목적을 갖고 있다는 점을 명확하게 이해했습니다.\n\n또한 Spring Security 설정 코드를…","body":"\n> 오늘은 CSRF와 CORS에 대해 브라우저 보안 모델의 관점에서 체계적으로 정리했습니다.\n>\n> 이전에는 단편적인 지식만 보유하고 있었으나, 두 개념이 서로 다른 목적을 갖고 있다는 점을 명확하게 이해했습니다.\n>\n> 또한 Spring Security 설정 코드를 직접 작성하며 SPA 환경에서 보안 설정이 어떤 방식으로 적용되는지 확인했습니다.\n\n## CSRF(Cross-Site Request Forgery)\n\n### CSRF 공격 흐름\n\n![csrf](./csrf.png)\nCSRF는 사용자가 로그인한 상태에서 공격자가 의도하지 않은 요청을 대신 보낼 수 있게 만드는 공격 방식입니다. 브라우저가 쿠키를 자동으로 전송한다는 특성을 이용합니다. 사용자는 자신이 요청을 보냈다는 사실을 인지하지 못한 채 서버의 상태가 변경될 수 있습니다.\n\n### SameSite Cookie 옵션\n\n![same site option](./same-site-cookies.png)\n\nSameSite 옵션은 쿠키가 크로스 사이트 요청에 포함되는 조건을 제어합니다.\n\n브라우저 보안 강화 측면에서 매우 중요한 옵션입니다.\n\n- Strict: 외부 요청에 절대 포함되지 않습니다.\n- Lax: GET 등 안전한 요청에 한해서만 제한적으로 허용됩니다.\n- None: 모든 요청에서 전송할 수 있으나 HTTPS 환경이 필요합니다.\n\n### Spring Security CSRF Token 설정 예시\n\nCSRF Token은 서버가 임의의 난수 값을 발급하고, 클라이언트가 해당 값을 요청에 포함해 전송하는 인증 방식입니다. 쿠키와 독립된 값이므로 공격자가 예측할 수 없고, 요청이 실제 사용자로부터 발생했는지를 확인하는 데 사용됩니다.\n\nSpring Security에서는 다음과 같이 설정할수 있습니다.\n\n```java\n@Configuration\n@EnableWebSecurity\npublic class SecurityConfig {\n    @Bean\n    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {\n        http\n            .csrf(csrf -> csrf\n                .csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse())\n            )\n            .authorizeHttpRequests(auth -> auth.anyRequest().authenticated());\n        return http.build();\n    }\n}\n```\n\nSPA 환경에서 CSRF Token을 JavaScript에서 읽어야 하는 경우 `withHttpOnlyFalse()` 설정이 필요합니다.\n\n## CORS(Cross-Origin Resource Sharing)\n\n![Same-Origin Policy](./sop.png)\nCORS는 브라우저가 Same-Origin Policy(SOP)를 적용했기 때문에 생겨난 제약을 완화하는 규칙이었습니다. 서버가 명시적으로 허용한 경우에만 브라우저는 응답을 스크립트에서 접근할 수 있었습니다.\n\n### CORS 요청 및 Preflight 흐름\n\n![Options](./options.png)\n\nPreflight 요청은 브라우저가 서버에 “이 요청을 보내도 되는지” 사전에 확인하기 위해 OPTIONS 메서드로 보내는 검사 요청입니다.\n서버는 `Access-Control-Allow-*` 형태의 헤더로 허용 범위를 명시합니다.\n\n### Spring CORS 기본 설정 예시\n\n```java\n@Configuration\npublic class WebConfig implements WebMvcConfigurer {\n    @Override\n    public void addCorsMappings(CorsRegistry registry) {\n        registry.addMapping(\"/api/**\")\n            .allowedOrigins(\"https://frontend.example.com\")\n            .allowedMethods(\"GET\",\"POST\",\"PUT\",\"DELETE\")\n            .allowedHeaders(\"*\")\n            .allowCredentials(true)\n            .maxAge(3600);\n    }\n}\n```\n\n### 프록시 서버\n\n브라우저 환경에서 발생하는 CORS 문제를 구조적으로 제거하기 위해 가장 많이 사용하는 방식이 바로 프록시 서버입니다. 아래 그림은 그 흐름을 시각적으로 보여줍니다.\n\n![프록시 서버](./proxy-server.png)\n\nCORS는 “브라우저”가 외부 리소스를 막는 정책이기 때문에 서버 대 서버 요청은 아예 CORS 문제 자체가 발생하지 않습니다.\n\n이 원리를 활용해 프록시 서버를 두면 CORS 문제를 구조적으로 해결할 수 있습니다.\n\n**프록시 서버 동작 원리**\n\n1. 브라우저는 프론트엔드 서버(동일 Origin)로 요청을 보냅니다.\n\n2. 프론트엔드 서버(또는 API Gateway)가 백엔드 서버로 요청을 전달합니다.\n\n3. 서버 → 서버 요청은 CORS가 적용되지 않습니다.\n\n4. 프록시 서버가 결과를 브라우저에게 전달합니다.\n\n**왜 프록시 서버는 CORS를 해결할까?**\n\nCORS는 오직 브라우저의 스크립트 요청에만 적용됩니다.\n\n서버 간 요청은 CORS 차단 대상이 아닙니다.\n\n따라서 브라우저와 백엔드 사이의 “중간 서버”만 동일 Origin이면 문제 해결입니다.\n\n**프록시 방식의 장점**\n\n- 백엔드 서버에서 CORS 설정을 굳이 복잡하게 하지 않아도 된다.\n\n- 배포 환경에서 API Gateway로 확장하기 좋다.\n\n- 보안 설정을 서버 측에서 일관성 있게 관리할 수 있다.\n\n![vite proxy](./vite.png)\n\nvite를 이용하여 프론트엔드 코드를 작성했을 때 다음과 같이 프록시 서버 설정을 할 수 있었습니다.\n\n`localhost:5713`과 `localhost:8080`간의 통신이 원활히 진행됩니다.\n\n![프록시 서버](./proxy.png)\n\n## 정리\n\n개발을 하면서 CSRF와 CORS라는 용어는 자주 접했지만, 각각이 어떤 문제를 해결하기 위한 기술인지, 어떤 환경에서 발생하며 어떤 위험을 초래하는지에 대해서는 명확히 이해하지 못하고 있었습니다. 이번 TIL을 통해 두 개념을 구조적으로 비교하며 정리하면서 그 차이를 확실하게 이해할 수 있었습니다.\n\n---\n\n### **CSRF (Cross-Site Request Forgery)**\n\n사용자가 _의도하지 않은 요청을 자동으로 보내게 만드는 공격 기법_\n\n- **발생 배경**\n  - 브라우저가 *쿠키를 자동으로 전송*하는 특성을 악용\n  - 공격자가 별도의 인증 없이 사용자 권한으로 요청 실행 가능\n- **문제점**\n  - 로그인된 사용자의 권한으로 서버의 데이터가 수정될 수 있음\n  - 사용자는 요청이 발생했다는 사실조차 모름\n- **방어 방법**\n  - 서버가 CSRF Token을 발급 → 요청마다 검증\n  - SameSite Cookie 옵션(Lax/Strict) 적용\n  - 중요한 상태 변경 요청은 Token 또는 Origin 검사 수행\n\n### **CORS (Cross-Origin Resource Sharing)**\n\n브라우저가 _다른 Origin의 응답 접근을 제한하는 정책을 완화하기 위한 메커니즘_\n\n- **발생 배경**\n  - 브라우저는 보안 때문에 Same-Origin Policy(SOP)를 기본 적용\n  - 서로 다른 Origin 간에 API 요청은 되지만 **응답 접근은 차단**\n- **문제점**\n  - 프론트엔드와 백엔드가 서로 다른 도메인·포트에서 동작하면 브라우저가 응답 사용을 차단\n  - 개발·배포 환경에서 흔히 발생하는 이슈\n- **해결 방법**\n  - 서버에서 `Access-Control-Allow-Origin`, `Allow-Methods`, `Allow-Headers` 등 설정\n  - Preflight(OPTIONS) 요청을 허용하도록 구성\n  - 또는 프록시 서버(개발 서버, API Gateway 등)를 사용해 동일 Origin으로 요청을 우회\n","frontmatter":{"date":"2025년 12월 9일","slug":"til-20251209","title":"웹 및 브라우저 보안 CSRF / CORS","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","csrf","cors"],"thumbnail":null}},{"id":"b0c94793-bbad-5fe0-83fd-acd69b9a647b","excerpt":"기존 API 문서는 Posman을 통해 생성하고 json으로 Export하여 저장하고있었습니다. API 문서화를 위해 Swagger(springdoc-openapi) 를 도입하고,\n\n요청 값을 안정적으로 검증하기 위한 Request…","body":"\n![postman](./postman.png)\n\n> 기존 API 문서는 Posman을 통해 생성하고 json으로 Export하여 저장하고있었습니다.\n>\n> API 문서화를 위해 **Swagger(springdoc-openapi)** 를 도입하고,\n>\n> 요청 값을 안정적으로 검증하기 위한 **Request Validation(Bean Validation)** 을 적용했습니다.\n>\n> 또한 기존에 Request, Response에서 혼재하여 사용하던 DTO를 각 용도에 맞게 분리하고,\n>\n> 유효성 검증을 통해 잘못된 요청을 사전에 차단하고,\n>\n> Swagger UI 에서 검증 규칙(minLength, required 등)을 자동으로 확인할 수 있도록 구성했습니다.\n\n# 1. Swagger(OpenAPI) 설정\n\nSpring Boot 3에서는 `springdoc-openapi`가 사실상 표준입니다.  \nSwagger UI를 통해 API 요청 형식을 문서화하고 실시간 테스트가 가능하도록 구성했습니다.\n\n[springdoc-openapi 공식 문서](springdoc-openapi)\n\n## 1.1 의존성 추가\n\n```xml\n<!-- Swagger -->\n<dependency>\n  <groupId>org.springdoc</groupId>\n  <artifactId>springdoc-openapi-starter-webmvc-ui</artifactId>\n  <version>2.8.8</version>\n</dependency>\n```\n\n추가 후 서버 실행 시 다음 주소에서 UI로 문서화된 API 목록을 확인 할 수 있습니다.\n\n```bash\nhttp://localhost:8080/swagger-ui.html\n```\n\n## 1.2 jwt인증 필터에서 swagger 경로 제외\n\nSwagger UI 페이지가 `/swagger-ui.html`에서 보여지고,\n\nOpenAPI 정보(JSON 형식)는 `/v3/api-docs`에서 보여지므로 두 url을 인증 필터에서 제외해야합니다.\n\n`SecurityConfig`의 `SecurityFilterChain`에서 `permitAll`의 경로로 위 경로를 추가해줍니다.\n\n```java\n@Configuration\npublic class SecurityConfig {\n\n  private final JwtTokenProvider tokenProvider;\n  private final UserDao userDao;\n  private final CustomAuthenticationEntryPoint entryPoint;\n\n  public SecurityConfig(JwtTokenProvider tokenProvider, UserDao userDao,\n      CustomAuthenticationEntryPoint entryPoint) {\n    this.tokenProvider = tokenProvider;\n    this.userDao = userDao;\n    this.entryPoint = entryPoint;\n  }\n\n  @Bean\n  public PasswordEncoder passwordEncoder() {\n    return new BCryptPasswordEncoder();\n  }\n\n  @Bean\n  public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {\n\n    JwtAuthenticationFilter jwtFilter = new JwtAuthenticationFilter(tokenProvider, userDao);\n\n    http\n        .csrf(AbstractHttpConfigurer::disable)\n        .exceptionHandling(e -> e.authenticationEntryPoint(entryPoint))\n        .sessionManagement(s -> s.sessionCreationPolicy(SessionCreationPolicy.STATELESS))\n        .authorizeHttpRequests(auth -> auth\n            .requestMatchers(\n                \"/api/auth/login\",\n                \"/api/user/register\",\n                \"/swagger-ui/**\",\n                \"/v3/api-docs/**\"\n            )\n            .permitAll()\n            .anyRequest().authenticated()\n        )\n        .addFilterBefore(jwtFilter, UsernamePasswordAuthenticationFilter.class);\n\n    return http.build();\n  }\n}\n```\n\n## 1.3 OpenAPI 기본 설정\n\nJWT 기반 인증을 적용한 프로젝트이기 때문에 Swagger에서도 Authorization 헤더를 사용할 수 있도록 설정했습니다.\n\n아래왁 같이 `SwaggerConfig`를 작성하면 Swagger UI에서 \"Authorize\" 버튼을 통해 Bearer Token을 입력할 수 있습니다.\n\n`Info`로 title, description, api 버전 등을 설정하여 보여줄 수 있습니다.\n\n```java\n@Configuration\npublic class SwaggerConfig {\n\n  @Bean\n  public OpenAPI openAPI() {\n    return new OpenAPI()\n        .components(new Components()\n            .addSecuritySchemes(\"bearerAuth\",\n                new SecurityScheme()\n                    .type(SecurityScheme.Type.HTTP)\n                    .scheme(\"bearer\")\n                    .bearerFormat(\"JWT\")))\n        .addSecurityItem(new SecurityRequirement().addList(\"bearerAuth\"))\n        .info(new Info()\n            .title(\"Cafe Web API\")\n            .description(\"Spring Boot로 구현한 Cafe 관리 서비스 api입니다.\")\n            .version(\"1.0.0\"));\n  }\n}\n```\n\n---\n\n위 설정을 마치고 http://localhost:8080/swagger-ui/index.html로 접속하면 아래와 같이 Swagger UI가 보여집니다.\n\n![swagger UI 화면](./swagger.png)\n\n![v3 api-docs](./api.png)\n\n`swagger-ui/index.html`위치에서 http://localhost:8080/v3/api-docs(GET)을 이용하여 가져오고 있기 때문에\n\nhttp://localhost:8080/v3/api-docs에서는 json 형식의 api 파일을 확인할 수 있습니다.\n\n![json api docs](./json.png)\n\n이 json형식 데이터는 Postman에서도 호환되는 형식이기 때문에 import하면 확인해 볼 수 있습니다.\n\n![import json](./import.png)\n\n# 2. Request 데이터 유효성 검증\n\n사용자 입력값 검증은 서버 안정성과 보안에 매우 중요합니다.\n\nSpring은 Jakarta Bean Validation(JSR 380)을 기반으로 DTO 단위에서 유효성을 검증할 수 있습니다.\n\n다음과 같은 규칙을 적용했습니다:\n\n- 필수 필드 검사(`@NotBlank`, `@NotNull`)\n- 문자열 길이 검사(`@Size`)\n- 이메일 형식(`@Email`)\n- 최소값/최대값(`@Min`, `@Max`)\n- 정규식 검사(`@Pattern`)\n\n## 2.1 DTO에 Validation 적용\n\n회원가입 요청 DTO를 `RegisterRequest` 객체로 별도 선언하고, Validation 조건을 달아줄 수 있습니다.\n\n```java\npublic class RegisterRequest {\n\n  @NotBlank(message = \"이름은 필수 항목입니다.\")\n  private String name;\n\n  @Email(message = \"올바른 이메일 형식이어야 합니다.\")\n  @NotBlank(message = \"이메일은 필수 항목입니다.\")\n  private String email;\n\n  @NotBlank(message = \"비밀번호는 필수 항목입니다.\")\n  @Size(min = 8, message = \"비밀번호는 최소 8자 이상이어야 합니다.\")\n  private String password;\n}\n```\n\nSwagger UI에서도 자동으로 다음과 같이 표시됩니다:\n\n- required 여부\n- minLength, maxLength\n- format(email)\n- 설명(description)\n\nSwagger는 Validation 애노테이션을 스캔하여 자동 문서화를 지원합니다.\n\n![Validation Request](./validate.png)\n\n## 2.2 Controller에서 검증 활성화\n\nController의 Request 파라미터에 `@Valid` 또는 `@Validated` 를 선언하면 Spring이 DTO를 검증합니다.\n\n```java\n@PostMapping(\"/register\")\npublic ResponseEntity<?> register(@Valid @RequestBody RegisterRequest request) {\n    ResponseRegisterDto result = userService.registerUser(user);\n    return ResponseEntity.ok(result);\n}\n\n```\n\n유효성 검증 실패 시 `MethodArgumentNotValidException`이 발생합니다.\n\n현재 Exception 핸들러에서 별도로 처리하지 않는 것은 500 에러로 반환하고 있습니다.\n\npassword를 8글자 미만으로 설정하고 Request를 보내면 아래와 같이 반환됩니다.\n\n![validation error](./error.png)\n\n## 2.3 `MethodArgumentNotValidException` 핸들러 추가\n\n`MethodArgumentNotValidException` 형식의 예외를 처리하는 핸들러를 아래와 같이 추가합니다.\n\nRequest 형식이 잘못되었으므로 400에러를 반환합니다.\n\n```java\n  @ExceptionHandler(MethodArgumentNotValidException.class)\n  public ResponseEntity<ErrorResponse> handleValidationException(\n      MethodArgumentNotValidException ex,\n      HttpServletRequest request) {\n\n    // 검증 실패한 필드 메시지 추출\n    String message = ex.getBindingResult()\n        .getFieldErrors()\n        .stream()\n        .map(error -> error.getField() + \": \" + error.getDefaultMessage())\n        .collect(Collectors.joining(\", \"));\n\n    ErrorResponse error = new ErrorResponse(\n        HttpStatus.BAD_REQUEST.value(),\n        HttpStatus.BAD_REQUEST.getReasonPhrase(),\n        message,\n        request.getRequestURI()\n    );\n\n    return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(error);\n  }\n```\n\n이제 다시 password를 8글자 미만으로 작성하고 요청을 보내면 아래와 같이 400 에러가 반환되고, validation에 설정한 메세지가 보여집니다.\n\n![400 error](./400.png)\n\n# 3. 실제 설정과 검증 흐름\n\nSwagger + Validation + Security까지 통합된 요청 처리 흐름은 다음과 같습니다.\n\n1. Swagger UI에서 요청 생성\n2. Controller 진입 전 `@Valid`에 의해 요청 DTO 검증\n3. 유효하지 않을 경우 Request Body를 처리하기 전에 400 오류 반환\n4. 유효하다면 JWT 필터가 인증 여부 확인\n5. SecurityContext에 인증 정보 저장\n6. Controller에서 비즈니스 로직 실행\n7. Swagger UI에서 응답 확인 가능\n\n따라서 개발자는 Swagger에서 바로 요청 규칙을 확인하며 테스트할 수 있고,\n\n클라이언트는 어떤 데이터 형식을 보내야 하는지 명확하게 이해할 수 있습니다.\n\n# 마무리\n\n이번 작업을 통해 Swagger를 통한 문서화와 Request Validation 기반의 안정적인 입력 검증 환경을 구축했습니다.\n\n## 배운 점\n\n1. **Swagger는 Validation 정보를 자동 반영**하기 때문에 DTO 설계만 잘해도 문서 품질이 올라간다.\n2. **전역 예외 처리기 활용**으로 검증 실패 응답 형식을 일관되게 유지할 수 있다.\n3. JWT 기반 프로젝트에서는 Swagger Authorization 설정이 필수이다.\n4. 클라이언트-서버 간 API 계약(API Contract)을 명확히 유지하는 데 Validation이 가장 중요하다.\n","frontmatter":{"date":"2025년 12월 1일","slug":"til-20251201","title":"Spring - Swagger(OpenAPI) 설정 및 Request Validation 적용하기","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","spring","java"],"thumbnail":null}},{"id":"22a5e182-0d2a-567b-aaba-f00999f6d271","excerpt":"이번 주에는 저번에 작성했던 세션 기반 로그인 방식을 개선하여, JWT 기반 인증 방식으로 전환해보았습니다. 추가로 Spring Security를 이용하여 API 요청 단위로 사용자 인증 정보를 검증하도록 변경하였습니다.\n\n또한 비밀번호를 암호화하여…","body":"\n> 이번 주에는 저번에 작성했던 **세션 기반 로그인 방식**을 개선하여, **JWT 기반 인증 방식**으로 전환해보았습니다.\n>\n> 추가로 Spring Security를 이용하여 API 요청 단위로 사용자 인증 정보를 검증하도록 변경하였습니다.\n>\n> 또한 비밀번호를 암호화하여 안전하게 저장할 수 있도록 **Bcrypt 해싱**을 적용하고, 로그인 시 입력한 비밀번호를 복호화 없이 비교할 수 있도록 검증하는 로직도 추가했습니다.\n\n전체 코드는 [Github repo](https://github.com/aaxx98/java-study/tree/main/Spring/CafeWeb)에서 확인할 수 있습니다.\n\n# 1. 기존 세션 기반 로그인의 문제점\n\n```java\n  @PostMapping(\"/login\")\n  public ResponseEntity<?> login(@RequestBody LoginRequest loginRequest,\n      HttpServletRequest request) {\n      ...\n\n      HttpSession session = request.getSession(true); // 없으면 새로 생성\n      session.setAttribute(\"userDto\", user); // 세션에 user 정보 저장\n      return ResponseEntity.ok(Map.of(\"message\", \"로그인 성공\"));\n\n      ...\n  }\n```\n\n세션 기반 인증은 `HttpSession`을 통해 세션에 사용자 정보를 저장하여 간단하게 구현할 수 있었지만 다음과 같은 문제점이 있습니다.\n\n- **서버 메모리 의존성**\n  - 세션 정보가 서버 메모리에 저장되기 때문에 서버 재시작 시 모든 세션이 초기화됩니다.\n  - 세션 ID가 탈취될 경우 서버의 세션 저장소에 접근할 수 있어 사용자 정보 유출 가능성이 존재합니다.\n    - 같은 세션 ID를 통해 다른 정보를 관리하는 경우 로그인 정보 뿐만 아니라 다른 데이터도 유출될 수 있습니다.\n    - 예: 장바구니 정보를 세션에 저장하고 있을 때 - `session.setAttribute(\"cart\", updatedCart);`\n  - 로그인 사용자가 많아질수록 서버가 관리해야 하는 세션 수가 증가하여 메모리 사용량이 늘어나고 부하가 커집니다.\n\n- ## **세션 기반은 Stateful하기 때문에 무상태 API 설계에 부적합**\n  - 서버가 각 클라이언트의 상태(Session)를 보관해야 하므로 요청마다 동일 서버에 도달해야 합니다.\n  - 서버 확장(스케일 아웃) 시 세션을 공유하기 위해 Redis 같은 외부 저장소 도입이 필요해 구조가 복잡해집니다.\n  - MSA나 모바일/SPA 기반 환경에서 선호되는 Stateless API 방식과 잘 맞지 않습니다.\n\n추가로, 현재 비밀번호를 그대로 저장하고 평문으로 비교하고 있기 때문에 회원가입 시 비밀번호를 암호화하여 저장해야합니다.\n\n# 2. 개선 방향\n\n## JWT 기반 인증 방식\n\n- JWT(Json Web Token)는 사용자 인증 정보를 자체적으로 포함하는 토큰이며, 서버는 별도의 세션 상태를 저장하지 않는 Stateless 구조로 요청을 처리합니다.\n- 토큰에는 사용자 식별자, 권한, 만료 시간 등의 정보가 포함되며, 서버는 서명(Signature)을 검증하여 위조 여부를 판단합니다.\n- 로그인 후 발급된 JWT는 HttpOnly 쿠키에 저장되어 브라우저의 스크립트 접근을 차단하고, 이후 모든 요청마다 자동으로 쿠키를 통해 인증 검증이 수행됩니다.\n\n**개선점**\n\n- 서버가 세션 상태를 보관하지 않아 메모리 사용량이 감소합니다.\n- 서버 확장 시 세션 공유를 고려하지 않아도 되어 구조가 단순해집니다.\n- 토큰 만료 시간을 명시적으로 관리할 수 있어 인증 수명 제어가 용이합니다.\n- 서버 내부의 세션 저장소가 없기 때문에 세션 하이재킹으로 인한 2차 피해가 없습니다.\n\n---\n\n## Spring Security 기반의 인증 처리\n\n- `Security Filter Chain` 단계에서 JWT를 검증하여 컨트롤러에 진입하기 전에 요청을 차단할 수 있습니다.\n- 커스텀 필터(JwtFilter)를 통해 쿠키에서 토큰을 추출하고 검증한 뒤, 정상적인 토큰일 경우 `SecurityContext`에 인증 정보를 등록합니다.\n- API별 접근 제어를 설정 파일(`SecurityConfig`)에서 선언적으로 구성할 수 있어 인증 및 인가 관리가 체계적입니다.\n\n**개선점**\n\n- 인증 처리가 `Interceptor`보다 더 이른 단계에서 일관적으로 수행됩니다.\n- 보안 관련 기능(CORS, CSRF, 권한, 인증 실패 처리 등)을 Spring Security 표준 방식으로 구성할 수 있습니다.\n- 비즈니스 로직과 인증 로직이 분리되어 유지보수성이 향상됩니다.\n\n---\n\n## 비밀번호 암호화(Bcrypt)\n\n- 사용자 비밀번호를 Spring Security의 `BCryptPasswordEncoder`로 해시하여 저장합니다.\n- Bcrypt는 단방향 해시 방식이기 때문에 저장된 값은 복호화가 불가능하며, 비밀번호 비교는 `matches()` 메서드를 통해 안전하게 이루어집니다.\n- 평문 저장 또는 단순 문자열 비교 방식에서 발생하는 보안 취약점을 제거할 수 있습니다.\n\n**개선점**\n\n- 비밀번호 유출 시 원본 비밀번호가 보호됩니다.\n- 기본적인 회원 인증 보안 수준이 강화됩니다.\n\n# 3. 개선 작업\n\n## 의존성 추가\n\n`pom.xml`에 Spring Security와 Java에서 JWT를 다룰수 있도록 하는 라이브러리인 JJWT 관련 의존성을 추가합니다.\n\n```xml\n    <dependency>\n      <groupId>org.springframework.boot</groupId>\n      <artifactId>spring-boot-starter-security</artifactId>\n    </dependency>\n\n    <!-- JJWT -->\n    <dependency>\n      <groupId>io.jsonwebtoken</groupId>\n      <artifactId>jjwt-api</artifactId>\n      <version>0.12.5</version>\n    </dependency>\n    <dependency>\n      <groupId>io.jsonwebtoken</groupId>\n      <artifactId>jjwt-impl</artifactId>\n      <version>0.12.5</version>\n      <scope>runtime</scope>\n    </dependency>\n    <dependency>\n      <groupId>io.jsonwebtoken</groupId>\n      <artifactId>jjwt-jackson</artifactId>\n      <version>0.12.5</version>\n      <scope>runtime</scope>\n    </dependency>\n```\n\n## 회원가입 시 비밀번호 암호화(Bcrypt)\n\n### PasswordEncoder Bean 등록\n\nSpring Security와 관련된 설정은 `SecurityConfig`에서 관리합니다.\n\nSpring Security에서 제공하는 `BCryptPasswordEncoder`를 Bean으로 등록합니다.\n\n```java\n@Configuration\npublic class SecurityConfig {\n\n  @Bean\n  public PasswordEncoder passwordEncoder() {\n    return new BCryptPasswordEncoder();\n  }\n}\n```\n\n### 회원가입 시 암호화 된 비밀번호 저장\n\n회원가입에 사용되는 UserService 코드의 일부입니다. request로 받은 비밀번호를 암호화하여 저장합니다.\n\n```java\n  public ResponseRegisterDto registerUser(UserDto user) {\n    String encodedPassword = passwordEncoder.encode(user.getPassword()); // 비밀번호 암호화\n    user.setPassword(encodedPassword);\n\n    int inserted = userDao.registerUser(user);\n    if (inserted <= 0) {\n      throw new IllegalStateException(\"회원 등록에 실패했습니다.\");\n    }\n    ResponseRegisterDto response = new ResponseRegisterDto();\n    response.setName(user.getName());\n    response.setEmail(user.getEmail());\n    response.setStatus(\"Register Success\");\n    return response;\n  }\n```\n\n![api/user/register(POST) request](./register.png)\n\n> 회원가입 시 설정한 비밀번호는 평문으로 request body에 담겨 전송되지만,\n>\n> **HTTPS를 사용하면 전송 과정이 암호화**되기 때문에 중간에서 내용이 노출되지 않습니다.\n>\n> 따라서 서버는 안전하게 비밀번호를 수신한 뒤 Bcrypt로 해싱하여 저장할 수 있습니다.\n\n![test9@test.com 사용자 가입](./user9.png)\n\n회원가입 창에서 password를 1234로 설정하고, `api/user/register(POST)`를 호출하면, 아래와 같이 password가 암호화 되어 저장됩니다.\n\n![패스워드가 암호화 되어 저장됨](./user-password.png)\n\n### 로그인 시 패스워드 비교\n\n로그인 서비스에서 로그인 시 입력한 password와 DB에 저장된 암호화 된 password를 비교합니다.\n\n스프링 시큐리티의 crypto `PasswordEncoder`의 `matches`를 이용하면, 평문과 암호가 같은지 비교해볼 수 있습니다.\n\n`encoder.matches(loginInfo.getPassword(), userInfo.getPassword())`와 같이 사용합니다.\n\nuserInfo에는 암호화 된 비밀번호가, loginInfo는 request에 입력한 평문 비밀번호가 전달됩니다.\n\n```java\n  public LoginResponseDto login(LoginRequestDto loginInfo) {\n    UserDto userInfo = loginDao.findUserByEmail(loginInfo.getEmail());\n    if (userInfo == null) {\n      throw new NotFoundException(\"가입되지 않은 사용자입니다.\");\n    }\n    if (!encoder.matches(loginInfo.getPassword(), userInfo.getPassword())) {\n      throw new UnauthorizedException(\"로그인 정보가 일치하지 않습니다.\");\n    }\n    ...\n  }\n```\n\n## JWT 토큰 발행 및 인증 확인\n\n### `application.properties` 설정\n\nJWT의 서명(Signature)에 사용될 비밀키를 `jwt.secret`에 설정합니다.\n\nHMAC-SHA256 알고리즘을 사용할 경우 최소 256비트(32바이트) 이상의 키 길이가 필요합니다.\n\n```\njwt.secret=[your-secret-key-here]\n```\n\n이 Key는 외부에 노출되면 보안에 문제가 생길 수 있으므로 별도로 관리해야합니다.\n\n### JWT 유틸리티 클래스 생성\n\n```java\n@Component\npublic class JwtTokenProvider {\n\n  private final Key key;\n\n  public JwtTokenProvider(@Value(\"${jwt.secret}\") String secret) {\n    this.key = Keys.hmacShaKeyFor(secret.getBytes());\n  }\n\n  // 토큰 생성\n  public String createToken(Long id, String email, String name) {\n    Date now = new Date(); // 발급 시간\n    Date exp = new Date(now.getTime() + 1000 * 60 * 60 * 3); // 만료시간: 3시간\n\n    return Jwts.builder()\n        .setSubject(String.valueOf(email)) // 토큰의 subject = 사용자 이메일\n        .claim(\"id\", id)\n        .claim(\"name\", name) // 커스텀 claim\n        .setIssuedAt(now) // 발급 시간\n        .setExpiration(exp) // 만료시간\n        .signWith(key, SignatureAlgorithm.HS256) // 비밀키로 서명\n        .compact(); // 문자열 형태의 JWT 생성\n  }\n\n  // 토큰 검증\n  public void validate(String token) {\n    try {\n      Jwts.parser().setSigningKey(key).build().parseClaimsJws(token);\n    } catch (ExpiredJwtException e) {\n      throw new UnauthorizedException(\"토큰이 만료되었습니다.\");\n    } catch (JwtException e) {\n      throw new UnauthorizedException(\"유효하지 않은 토큰입니다.\");\n    }\n  }\n\n  // 사용자 정보 반환\n  public Claims getClaims(String token) {\n    return Jwts.parser().setSigningKey(key).build()\n        .parseClaimsJws(token)\n        .getBody();\n  }\n\n  public String getUserEmail(String token) {\n    return getClaims(token).getSubject();\n  }\n\n  public String getUserName(String token) {\n    return getClaims(token).get(\"name\", String.class);\n  }\n}\n```\n\n**JwtTokenProvider 생성자**\n\n- `application.properties`에 저장된 `jwt.secret` 값을 주입받아 HMAC-SHA256용 Key 객체를 생성합니다.\n- 이 Key는 토큰 생성 시 서명 과정에 사용되며, 토큰 검증 시 동일한 키를 사용하여 위변조 여부를 확인합니다.\n\n**createToken** 토큰 생성\n\n- JWT를 생성하여 문자열 형태로 반환합니다.\n- 이메일을 `sub`(subject)로 설정하고, 사용자 id 및 이름과 같은 커스텀 Claim을 추가하여 이후 요청에서 사용자 식별이 가능하도록 구성합니다.\n- 발급 시간(`iat`)과 만료 시간(`exp`)을 명시하여 토큰의 유효 기간을 제한합니다.\n- 서버의 비밀키로 HS256 알고리즘을 사용해 서명하여 위조 방지가 가능한 토큰을 생성합니다.\n\n**validate** 토큰 검증\n\n- 클라이언트가 전송한 JWT가 올바르게 서명되었는지, 만료되지 않았는지 검증합니다.\n- 서명이 올바르지 않거나, 토큰 구조가 손상되었거나, 만료된 경우 예외를 발생시킵니다.\n\n## 로그인 시 브라우저 쿠키에 accessToken 저장\n\n로그인 성공 후 서버는 생성된 JWT를 HttpOnly 쿠키에 담아 브라우저로 전달합니다.\n\n![로그인 postman](./login.png)\n\n아래는 쿠키에 accessToken을 담고있는 로그인 컨트롤러 코드입니다.\n\n로그인 service의 login에서 jwt 토큰을 create하여 반환해주고 있습니다.\n\n생성된 토큰을 브라우저 쿠키에 저장해두고, 다음 request 호출 시 자동으로 헤더에 토큰을 담아 전달해 줍니다.\n\n```java\n  @PostMapping(\"/login\")\n  public ResponseEntity<ResponseRegisterDto> login(@RequestBody LoginRequestDto loginRequest) {\n    LoginResponseDto loginSuccess = loginService.login(loginRequest);\n\n    ResponseCookie cookie = ResponseCookie.from(\"accessToken\", loginSuccess.getAccessToken())\n        .httpOnly(true)\n        .secure(false) // http: false, https: true\n        .path(\"/\")\n        .sameSite(\"Lax\")\n        .maxAge(30 * 60) // 30분\n        .build();\n\n    return ResponseEntity.ok()\n        .header(HttpHeaders.SET_COOKIE, cookie.toString())\n        .body(loginSuccess.getUser());\n  }\n```\n\n![session cookie](./session.png)\n\n지난번 세션ID를 쿠키에 저장했던 방식과 동일하게 동작합니다.\n\n포스트맨에서 login api 호출 후 다음과 같이 쿠키가 설정된 것을 확인할 수 있습니다.\n\n![./cookie](./cookie.png)\n\n## 기존 Interceptor를 Spring Security 설정으로 변경\n\n### 기존 Interceptor 제거\n\n이제 Spring Security가 인증을 처리하므로 기존의 `LoginInterceptor`와 `WebConfig` 설정은 제거합니다.\n\n일부 설정들은 Spring Security로 옮겨 사용하겠습니다.\n\n### JWT 인증 필터 작성\n\n요청에서 JWT 토큰을 추출하고 검증하는 필터를 작성합니다.\n\n```java\npublic class JwtAuthenticationFilter extends OncePerRequestFilter {\n\n  private final JwtTokenProvider provider;\n  private final LoginDao loginDao;\n\n  public JwtAuthenticationFilter(JwtTokenProvider provider, LoginDao loginDao) {\n    this.provider = provider;\n    this.loginDao = loginDao;\n  }\n\n  @Override\n  protected void doFilterInternal(HttpServletRequest request,\n      HttpServletResponse response,\n      FilterChain chain)\n      throws ServletException, IOException {\n\n    String token = null;\n\n    Cookie[] cookies = request.getCookies();\n    if (cookies != null) {\n      for (Cookie cookie : cookies) {\n        if (cookie.getName().equals(\"accessToken\")) {\n          token = cookie.getValue();\n        }\n      }\n    }\n\n    if (token != null) {\n      provider.validate(token);\n\n      String email = provider.getUserEmail(token);\n      UserDto user = loginDao.findUserByEmail(email);\n      user.setPassword(null);\n\n      UsernamePasswordAuthenticationToken auth =\n          new UsernamePasswordAuthenticationToken(user, null, null);\n      SecurityContextHolder.getContext().setAuthentication(auth);\n    }\n\n    chain.doFilter(request, response);\n  }\n}\n```\n\n**OncePerRequestFilter**\n\n- Spring Security가 제공하는 필터 기반 추상 클래스\n- 한 HTTP 요청에 대해 단 한번만 실행되는 보안 필터를 만들 때 사용\n- `doFilterInternal()` 메서드에 보안 필터 로직 작성\n\n**`doFilterInternal()` 내부 동작 설명**\n\n1. 쿠키에서 JWT 토큰(accessToken) 추출\n\n   ```java\n    String token = null;\n\n    Cookie[] cookies = request.getCookies();\n    if (cookies != null) {\n      for (Cookie cookie : cookies) {\n        if (cookie.getName().equals(\"accessToken\")) {\n          token = cookie.getValue();\n        }\n      }\n    }\n   ```\n\n2. JWT 유효성 검증\n   - `provider.validate(token);`\n3. JWT에서 사용자 email 추출\n   - `String email = provider.getUserEmail(token);`\n\n4. DB에서 사용자 조회 후 user 객체 생성\n   - `UserDto user = loginDao.findUserByEmail(email);`\n5. 스프링 시큐리티 인증 객체 `Authentication` 생성\n   ```java\n    UsernamePasswordAuthenticationToken auth =\n    new UsernamePasswordAuthenticationToken(user, null, null);\n   ```\n6. `SecurityContextHolder`에 인증 정보 저장\n   - `SecurityContextHolder.getContext().setAuthentication(auth);`\n\n7. 다음 필터로 요청 전달\n   - `chain.doFilter(request, response);`\n\n### 인증 실패 시 처리 정의\n\n기존 `LoginInterceptor`에서 세션이 비어있을 때에 대해 정의되어 있던 로직을 작성했습니다.\n\n`AuthenticationEntryPoint`는 인증 실패 또는 인증 정보가 없는 경우에 스프링 시큐리티가 호출하는 핸들러입니다.\n\n```java\n@Component\npublic class CustomAuthenticationEntryPoint implements AuthenticationEntryPoint {\n\n  @Override\n  public void commence(HttpServletRequest request,\n      HttpServletResponse response,\n      AuthenticationException authException) throws IOException {\n\n    response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);\n    response.setContentType(\"application/json; charset=UTF-8\");\n\n    response.getWriter().write(\n        \"{\\\"status\\\":401, \\\"message\\\":\\\"로그인이 필요합니다.\\\"}\"\n    );\n  }\n}\n```\n\n로그인하지 않고 다른 조회 api를 호출하면 다음과 같이 에러 코드와 메세지를 반환합니다.\n\n![401Unauthorized](./401.png)\n\n### `SecurityConfig` 설정\n\n`SecurityConfig`는\n스프링 시큐리티의 설정을 정의하는 클래스입니다.\n\n위에서 작성한 패스워드 암호화를 위한 PasswordEncoder / HTTP 각 요청에 대한 JWT 검증 필터 / 인증 실패 시 동작할 핸들러를 등록해줍니다.\n\n아래와 같은 설정을 하고있습니다.\n\n- PasswordEncoder Bean 등록\n- SecurityFilterChain 설정\n- CustomAuthenticationEntryPoint 설정\n- 기존 WebConfig에서 인터셉터 예외 경로 설정을 하는 부분\n\n```java\n@Configuration\npublic class SecurityConfig {\n\n  private final JwtTokenProvider tokenProvider;\n  private final LoginDao loginDao;\n  private final CustomAuthenticationEntryPoint entryPoint;\n\n  public SecurityConfig(JwtTokenProvider tokenProvider, LoginDao loginDao,\n      CustomAuthenticationEntryPoint entryPoint) {\n    this.tokenProvider = tokenProvider;\n    this.loginDao = loginDao;\n    this.entryPoint = entryPoint;\n  }\n\n  @Bean\n  public PasswordEncoder passwordEncoder() {\n    return new BCryptPasswordEncoder();\n  }\n\n  @Bean\n  public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {\n\n    JwtAuthenticationFilter jwtFilter = new JwtAuthenticationFilter(tokenProvider, loginDao);\n\n    http\n        .csrf(AbstractHttpConfigurer::disable)\n        .exceptionHandling(e -> e.authenticationEntryPoint(entryPoint))\n        .sessionManagement(s -> s.sessionCreationPolicy(SessionCreationPolicy.STATELESS))\n        .authorizeHttpRequests(auth -> auth\n            .requestMatchers(\"/api/auth/login\", \"/api/user/register\").permitAll()\n            .anyRequest().authenticated()\n        )\n        .addFilterBefore(jwtFilter, UsernamePasswordAuthenticationFilter.class);\n\n    return http.build();\n  }\n}\n```\n\n# 마무리\n\n## 인증 흐름 요약\n\n이번에 구현한 JWT 기반 인증의 전체 흐름은 다음과 같습니다.\n\n1. **로그인 요청:** 사용자가 `email`과 `password`를 서버로 전송합니다.\n2. **비밀번호 검증:** 서버는 저장된 Bcrypt 해시값과 입력된 평문 비밀번호를 `BCryptPasswordEncoder.matches()`로 비교합니다.\n3. **JWT 발급:** 검증이 완료되면 사용자 정보(email)를 담은 JWT 토큰을 생성합니다.\n4. **쿠키 전달:** 발급된 JWT를 `HttpOnly` 속성이 적용된 쿠키에 담아 클라이언트로 전송합니다.\n5. **자동 인증:** 클라이언트는 이후 모든 API 요청 시 이 쿠키를 자동으로 포함하여 전송합니다.\n6. **토큰 검증 및 인증:** `JwtAuthenticationFilter`가 쿠키에서 토큰을 추출하고 유효성을 검증한 뒤, `SecurityContext`에 인증 정보를 등록하여 컨트롤러가 실행됩니다.\n\n## 주요 개선 사항\n\n세션 기반 인증에서 JWT와 Spring Security를 도입하면서 다음과 같은 개선이 이루어졌습니다.\n\n### 1. **보안 강화**\n\n- **비밀번호 암호화:** 평문 저장 방식에서 Bcrypt 해시 방식으로 전환하여 DB 유출 시에도 원본 비밀번호를 보호합니다.\n- **HttpOnly 쿠키:** JWT를 HttpOnly 쿠키에 담아 JavaScript를 통한 토큰 탈취(XSS 공격)를 방지합니다.\n- **HTTPS 전제:** 평문 비밀번호 전송은 HTTPS를 통한 전송 구간 암호화를 전제로 안전하게 동작합니다.\n\n### 2. **확장성 및 유지보수성**\n\n- **Stateless 인증:** 서버가 세션 상태를 저장하지 않아 수평 확장(Scale-out)이 용이하고, MSA 환경에 적합합니다.\n- **체계적인 인증 관리:** Spring Security의 Filter Chain을 활용하여 인증 로직이 비즈니스 로직과 분리되었습니다.\n- **선언적 보안 설정:** `SecurityConfig`에서 API별 접근 제어를 중앙에서 관리할 수 있습니다.\n\n### 3. **개발 편의성**\n\n- **인터셉터 제거:** 기존 `HandlerInterceptor` 방식보다 더 이른 단계에서 인증을 처리하며, Spring Security의 표준 기능을 활용합니다.\n- **일관된 예외 처리:** `CustomAuthenticationEntryPoint`를 통해 인증 실패 시 일관된 JSON 응답을 제공합니다.\n\n## 배운 점\n\n이번 구현을 통해 다음을 깊이 이해할 수 있었습니다.\n\n1. **인증 vs 인가:** 인증(Authentication)은 \"누구인지\" 확인하는 것이고, 인가(Authorization)는 \"무엇을 할 수 있는지\" 결정하는 것입니다.\n2. **JWT의 장단점:** Stateless의 장점이 있지만, 토큰 만료 전 무효화가 어렵다는 한계도 함께 이해했습니다.\n3. **Spring Security의 필터 체인:** 요청 처리 과정에서 Security Filter가 어떻게 동작하는지, 그리고 커스터마이징 방법을 배웠습니다.\n4. **보안의 계층적 접근:** 암호화, HTTPS, HttpOnly 쿠키 등 여러 보안 기법을 조합하여 시스템을 보호하는 방법을 알게 되었습니다.\n\n---\n\n### 멀티캠퍼스 TIL 챌린지 11월 우수 포스트로 선정되었습니다.\n\n[[TIL 챌린지] 우수 참여자 발표-11월🍀](https://blog.naver.com/multicampus_it/224094467968)\n\n![우수 참여자 발표-11월](./multicampus.png)\n","frontmatter":{"date":"2025년 11월 24일","slug":"til-20251124","title":"Spring - Spring Security + JWT 기반 인증으로 로그인 로직 개선하기","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","spring","java"],"thumbnail":null}},{"id":"f6251d44-76e7-5d64-83a7-a64682c3eca6","excerpt":"오늘은 Spring을 이용하여 RESTful API를 설계하고, 컨트롤러에서의 HTTP 메서드 처리 방식과 ResponseEntity로 상태 코드를 반환하는 방법, 그리고 글로벌 예외 처리 구조를 정리해보았습니다.\n\nREST…","body":"\n> 오늘은 Spring을 이용하여 RESTful API를 설계하고, 컨트롤러에서의 HTTP 메서드 처리 방식과  \n> `ResponseEntity`로 상태 코드를 반환하는 방법, 그리고 글로벌 예외 처리 구조를 정리해보았습니다.\n\n---\n\n# 1. RESTful API\n\nREST(Representational State Transfer)는 **리소스를 URI로 표현하고**,  \n해당 리소스에 대한 행위는 **HTTP 메서드(GET, POST, PUT, PATCH, DELETE)** 로 구분하는 구조입니다.\n\n예를 들어 `/api/products`라는 URI는 ‘상품’이라는 리소스를 나타내며,  \n여기에 다양한 HTTP 메서드를 조합해 리소스를 조회, 생성, 수정, 삭제할 수 있습니다.\n\n## 주요 HTTP 메서드\n\n### **GET**\n\n- 리소스 조회(Read)\n- 서버의 상태를 변경하지 않는 안전한 메서드\n- 예: 상품 목록 조회, 상품 상세 조회\n\n### **POST**\n\n- 리소스 생성(Create)\n- 서버에 새로운 데이터를 추가\n- 예: 상품 등록\n- 일반적으로 **201 Created** 상태 코드와 Location 헤더 반환\n\n### **PUT**\n\n- 리소스 전체 수정(Update)\n- 해당 리소스를 새로운 내용으로 완전히 교체하는 개념\n- 예: 상품 전체 정보 수정\n- 이미 존재하는 리소스가 아니면 404 반환하는 방식이 일반적\n\n### **PATCH**\n\n- 리소스 부분 수정(Partial Update)\n- 전체가 아니라 **일부 필드만 수정**할 때 사용\n- 예: 상품의 가격만 변경, 재고만 업데이트\n- PUT보다 가볍고 변경 의도를 명확하게 표현할 수 있음\n\n### **DELETE**\n\n- 리소스 삭제(Delete)\n- 요청이 성공해도 보통 응답 본문은 비우고 **204 No Content**로 반환하는 것이 REST 관례\n\nRESTful API는 다음 원칙을 따릅니다.\n\n- **명확한 자원 식별:** `/products/3`은 id가 3인 상품을 의미합니다.\n- **HTTP 메서드에 따른 행위 분리:** GET(조회), POST(등록), PUT(수정), DELETE(삭제)\n- **표준 상태 코드 사용:** 200, 201, 400, 404, 500 등\n- **Stateless:** 서버는 클라이언트의 상태를 저장하지 않고 요청 기반으로 처리합니다.\n\n---\n\n# 2. Controller에서 REST API 작성\n\nSpring에서는 `@RestController` 또는 `@Controller + @ResponseBody` 조합을 사용하여 JSON 기반 응답을 반환합니다.\n\n상품 도메인을 기준으로 다음과 같은 컨트롤러를 구성했습니다.\n\n![상품 API](./postman.png)\n\n```java\n@Controller\n@ResponseBody\n@RequestMapping(\"/api/products\")\npublic class ProductController {\n\n  private final ProductService productService;\n\n  public ProductController(ProductService productService) {\n    this.productService = productService;\n  }\n\n  @GetMapping\n  public ResponseEntity<?> getProductList(@ModelAttribute PageRequestDto requestDto) {\n    if (requestDto.getPage() < 1 || requestDto.getPageSize() < 1) {\n        return ResponseEntity\n          .badRequest()\n          .body(\"page와 pageSize는 1 이상의 값이어야 합니다.\");\n    }\n    ProductListDto listDto = productService.getProductList(requestDto);\n\n    return ResponseEntity.ok(listDto);\n  }\n\n  @DeleteMapping(\"/{id}\")\n  public ResponseEntity<Void> deleteProduct(@PathVariable int id) {\n    boolean deleted = productService.deleteById(id);\n    if (!deleted) {\n      return ResponseEntity.notFound().build();\n    }\n    return ResponseEntity.noContent().build();\n  }\n\n  @PostMapping\n  public ResponseEntity<Void> saveProduct(@RequestBody ProductDto product) {\n    int createdId = productService.createProduct(product);\n    URI location = URI.create(\"/api/products/\" + createdId);\n    return ResponseEntity.created(location).build();\n  }\n\n  @PutMapping(\"/{id}\")\n  public ResponseEntity<?> updateProduct(@PathVariable int id, @RequestBody ProductDto product) {\n    boolean updated = productService.updateProduct(id, product);\n    if (!updated) {\n      return ResponseEntity.notFound().build();\n    }\n    return ResponseEntity.noContent().build();\n  }\n\n  @PatchMapping(\"/{id}/stock\")\n  public ResponseEntity<?> patchStockQuantity(\n      @PathVariable int id, @RequestBody StockUpdateDto stock) {\n    boolean updated = productService.updateStock(id, stock);\n    if (!updated) {\n      return ResponseEntity.notFound().build();\n    }\n    return ResponseEntity.noContent().build();\n  }\n}\n```\n\n## getProductList (GET: 상품 목록 조회)\n\n- `@GetMapping`\n  - HTTP GET 요청 처리\n  - 주로 조회(Read) 작업에서 사용하며, 요청 본문 없이 URL 파라미터만으로 처리\n\n- `@ModelAttribute`\n  - 쿼리스트링(`?page=1&pageSize=10`) 형태의 요청 파라미터를 DTO로 자동 바인딩\n  - GET 요청에서 페이징, 검색 조건 등을 DTO로 받을때 사용\n\n## saveProduct (POST: 상품 등록)\n\n- `@PostMapping`\n  - HTTP POST 요청 처리\n  - 새로운 리소스를 생성할 때 사용\n\n- `@RequestBody`\n  - HTTP 요청 Body(JSON)를 ProductDto로 변환하여 전달\n  - JSON → DTO 자동 매핑에 사용\n\n## updateProduct (PUT: 상품 수정)\n\n- `@PutMapping(\"/{id}\")`\n  - HTTP PUT 요청 처리\n  - 리소스를 수정하거나 교체할 때 사용\n  - URL 경로의 `{id}`를 통해 수정 대상 리소스 지정\n\n- `@PathVariable`\n  - URI 경로 변수 `{id}`의 값을 메서드 파라미터로 매핑\n  - `/api/products/3` → id = 3 과 같이 자동 매핑\n\n- `@RequestBody`\n  - 요청 Body(JSON)를 ProductDto로 변환하여 수정할 데이터 전달\n\n## patchStockQuantity (PATCH: 상품 재고 일부 수정)\n\n- `@PatchMapping(\"/{id}/stock\")`\n  - HTTP PATCH 요청 처리\n  - **리소스의 일부 필드만 부분적으로 수정할 때 사용**\n  - URL 경로의 `{id}`를 통해 재고를 수정할 상품 지정\n- `@PathVariable`\n  - URI 경로 변수 `{id}` 값을 메서드 파라미터로 매핑\n  - `/api/products/3/stock` 요청 시 `id = 3` 으로 전달\n- `@RequestBody`\n  - 요청 Body(JSON)를 `StockUpdateDto`로 변환하여\n    수정할 재고 필드(예: quantity 변경)를 전달\n  - 전체 ProductDto를 보내는 것이 아닌,\n    **변경이 필요한 필드만 포함된 DTO**를 사용한다는 점에서 PUT과 구분됨\n\n## deleteProduct (DELETE: 상품 삭제)\n\n- `@DeleteMapping(\"/{id}\")`\n  - HTTP DELETE 요청 처리\n  - 특정 리소스를 삭제할 때 사용\n\n- `@PathVariable`\n  - URL 경로 `/api/products/{id}`에서 id 값을 추출하여 메서드 파라미터로 매핑\n  - 삭제할 리소스의 식별자로 사용\n\n---\n\n# 3. ResponseEntity를 활용한 상태 코드 반환\n\n- `ResponseEntity.ok(data)` → 200 OK\n- `ResponseEntity.badRequest().body(\"잘못된 요청\")` → 400 Bad Request\n- `ResponseEntity.notFound().build()` → 404 Not Found\n- `ResponseEntity.status(HttpStatus.CREATED).build()` → 201 Created\n\n상태 코드를 명확히 지정함으로써 클라이언트가 응답을 정확히 해석할 수 있게 해줍니다.\n\n## HTTP 상태코드\n\n각 상태코드의 의미는 다음과 같습니다. 상황에 맞는 상태 코드를 사용하면 API의 의도를 더 명확하게 전달할 수 있습니다.\n\n**200 OK**  \n요청이 정상적으로 처리됨. GET, PUT, DELETE, POST 등 성공 시 공통적으로 사용.\n\n**201 Created**  \n리소스가 성공적으로 생성됨. 보통 POST 요청 성공 시 사용.\n\n**204 No Content**  \n요청은 성공했지만 반환할 데이터가 없음. DELETE 성공 시 자주 사용.\n\n**400 Bad Request**  \n클라이언트가 잘못된 요청을 보냄. 파라미터 누락, 타입 오류 등.\n\n**401 Unauthorized**  \n인증이 필요한데 인증되지 않은 경우. 세션/토큰 없음.\n\n**403 Forbidden**  \n인증은 되었으나 해당 리소스에 접근할 권한이 없음.\n\n**404 Not Found**  \n요청한 리소스를 찾을 수 없음.\n\n**409 Conflict**  \n요청이 서버의 현재 상태와 충돌함. 중복 데이터 등에서 발생 가능.\n\n**422 Unprocessable Entity**  \n요청은 문법적으로 맞지만 의미적으로 처리할 수 없음. (DTO 검증 실패 등)\n\n**500 Internal Server Error**  \n서버 내부에서 처리 중 예기치 못한 오류 발생.\n\n**502 Bad Gateway**  \n서버가 위임받은 백엔드 서버로부터 잘못된 응답을 받음.\n\n**503 Service Unavailable**  \n서버가 현재 요청을 처리할 준비가 되어 있지 않음. 과부하 또는 점검.\n\n**504 Gateway Timeout**  \n서버가 다른 서버로부터 응답을 제때 받지 못함.\n\n**+** 418 I'm a teapot이라는 상태코드도 있습니다. 만우절 장난으로 만들어졌다고 합니다.\n\n[google.com/teapot](https://www.google.com/teapot)에 접속하면 teapot 에러 페이지를 볼 수 있습니다.\n\n![teapot](./teapot.png)\n\n## 현재 구조의 문제점\n\n현재 구조에서는 Service에서 성공 여부를 boolean으로 반환하고, Controller에서 해당 값을 기반으로 HTTP 응답 코드를 분기하여 처리하고 있습니다.\n\nService의 내부 코드를 보면 아래와 같습니다. update가 성공 여부에 따라 boolean을 반환하고 있습니다.\n\n```java\n  @Transactional\n  public boolean updateProduct(int id, ProductDto product) {\n    product.setId(id);\n\n    if (product.isStockManage()) {\n      int productId = product.getId();\n      StockDto existStock = stockDao.findByProductId(productId);\n      if (existStock == null) {\n        StockDto stock = new StockDto();\n        stock.setProductId(productId);\n        stockDao.initStock(stock);\n      }\n    }\n    return productDao.updateProduct(product) > 0;\n  }\n```\n\n이렇게 작성했을 때 발생하는 문제는 아래와 같습니다.\n\n- **boolean 값 하나로는 실패한 이유를 구체적으로 표현할 수 없습니다.**\n  다양한 오류 상황이 동일한 false에 묶여 Controller는 정확한 상태 코드를 판단할 수 없습니다.\n- **Service에서 다른 형태의 값을 반환하더라도 Controller에서 분기 처리해야 하므로 유지보수가 복잡해집니다.**\n  로직이 확장될수록 Controller가 불필요하게 비즈니스 조건까지 해석하게 됩니다.\n- **Service 메소드가 정상적으로 값을 반환하면, 문제 상황이 발생해도 트랜잭션이 그대로 커밋됩니다.**\n  예외가 발생하지 않는 구조에서는 트랜잭션 롤백이 일어나지 않아 데이터 정합성이 깨질 수 있습니다.\n\n또한 Controller에서 다음과 같이 String으로만 에러 메세지를 보여주고있습니다.\n![Get parameter에러](./get.png)\n\nController의 분기 처리를 줄이고, Service에서 예외를 발생시키는것으로 통일하고,\n\n예외가 발생했을 때, 다음과 같은 형태로 에러 메세지를 반환할 수 있도록 개선해보도록 하겠습니다.\n\n![통일된 에러 메세지](./get-response.png)\n\n## 개선 방향\n\n1. **실패 상황은 예외로 명확하게 표현합니다.**\n\n   비즈니스 오류와 시스템 오류를 구분할 수 있도록 CustomException을 정의하고, Service에서는 실패 조건이 발생하면 boolean 대신 예외를 던지도록 변경합니다.\n\n2. **Controller는 성공 응답만 처리하고, 실패 응답은 전부 GlobalExceptionHandler에서 intercept하여 처리합니다.**\n\n   이렇게 하면 Controller는 로직 분기 없이 HTTP 상태 코드만 반환하는 단순한 구조가 되고, 실패 응답은 예외 타입별로 일관된 형식(JSON)으로 관리할 수 있습니다.\n\n3. **트랜잭션은 예외가 발생할 때 자동으로 롤백되도록 유지합니다.**\n\n   Service에서 예외를 던지면 Spring의 트랜잭션 관리에 따라 롤백되므로, “실패했는데 커밋되는 상황”을 방지할 수 있습니다.\n\n# 4. `@RestControllerAdvice`를 이용한 예외처리 핸들러 작성\n\nSpring에서는 `@RestControllerAdvice`를 사용해 애플리케이션 전역의 예외를 한 곳에서 처리할 수 있습니다.\n이를 통해 각 Controller에서 중복되는 에러 처리 코드를 작성할 필요가 없고, 모든 API가 동일한 형태의 에러 응답을 제공하도록 표준화할 수 있습니다.\n\n아래는 기본적인 전역 예외 처리기입니다.\n\n- **`Exception.class`는 모든 예외의 부모 타입**이므로, 어떤 예외가 발생하든 이 메서드가 처리합니다.\n- **전역 예외 처리의 최종 fallback**이며, 다른 핸들러에서 처리하지 못한 예외를 모두 잡습니다.\n- **발생한 예외를 500 Internal Server Error 형태의 JSON 응답으로 변환**합니다.\n- **요청 경로, 메시지를 ErrorResponse로 담아 일관된 오류 포맷을 유지합니다.**\n\n```java\n@RestControllerAdvice\npublic class GlobalExceptionHandler {\n\n  @ExceptionHandler(Exception.class)\n  public ResponseEntity<ErrorResponse> handleAllExceptions(\n      Exception ex,\n      HttpServletRequest request) {\n\n    ex.printStackTrace();\n\n    ErrorResponse response = new ErrorResponse(\n        500,\n        \"Internal Server Error\",\n        ex.getMessage(),\n        request.getRequestURI()\n    );\n\n    return ResponseEntity\n        .status(HttpStatus.INTERNAL_SERVER_ERROR)\n        .body(response);\n  }\n}\n```\n\n## Error Response 형식 정의\n\n일관된 형식으로 에러 메세지를 반환하기 위해 `ErrorResponse` 형식을 정의해두고 사용하였습니다.\n\n```java\n@Getter\n@Setter\npublic class ErrorResponse {\n\n  private int status;\n  private String error;\n  private String message;\n  private String timestamp;\n  private String path;\n\n  public ErrorResponse(int status, String error, String message, String path) {\n    this.status = status;\n    this.error = error;\n    this.message = message;\n    this.timestamp = java.time.LocalDateTime.now().toString();\n    this.path = path;\n  }\n}\n```\n\n## Custom Exception 타입 정의\n\n아래와 같이 자주 사용되는 Custom Exception을 정의하였습니다.\n\n- 데이터 없음을 표시하는 404는 `NotFoundException`으로 처리\n- 잘못된 Request 형식이 들어왔을 때 400은 `BadRequestException`으로 처리\n\n```java\npublic class BadRequestException extends RuntimeException {\n\n  public BadRequestException(String message) {\n    super(message);\n  }\n}\n\npublic class NotFoundException extends RuntimeException {\n\n  public NotFoundException(String message) {\n    super(message);\n  }\n}\n```\n\n## Service에서 예외 throw\n\n서비스에서는 문제가 발생하면 적절한 Exception 형식으로 에러를 throw하변 됩니다.\n\n```java\n  @Transactional\n  public void updateProduct(int id, ProductDto product) {\n    if (product.getName() == null || product.getCategory() == null) {\n      throw new BadRequestException(\"request에는 name, category를 포함하여야 합니다.\");\n    }\n\n    ProductDto exist = productDao.findById(id);\n    if (exist == null) {\n      throw new NotFoundException(\"상품을 찾을 수 없습니다.\");\n    }\n\n    product.setId(id);\n\n    if (product.isStockManage()) {\n      StockDto stock = stockDao.findByProductId(id);\n      if (stock == null) {\n        stock = new StockDto();\n        stock.setProductId(id);\n        stockDao.initStock(stock);\n      }\n    }\n\n    int updated = productDao.updateProduct(product);\n    if (updated == 0) {\n      throw new IllegalStateException(\"상품 수정에 실패했습니다.\");\n    }\n  }\n```\n\n## GlobalExceptionHandler에서 예외 타입에 따라 분기 처리\n\nthrow된 에러 처리를 위해 GlobalExceptionHandler에서 예외 타입에 따라 서로 다른 HTTP 상태 코드를 반환하도록 분기할 수 있습니다.\n\n```java\n@RestControllerAdvice\npublic\nclass GlobalExceptionHandler {\n\n  @ExceptionHandler(NotFoundException.class)\n  public ResponseEntity<ErrorResponse> handleNotFound(NotFoundException ex,\n      HttpServletRequest request) {\n\n    ErrorResponse error = new ErrorResponse(\n        404,\n        \"Not Found\",\n        ex.getMessage(),\n        request.getRequestURI()\n    );\n\n    return ResponseEntity.status(404).body(error);\n  }\n\n  @ExceptionHandler(BadRequestException.class)\n  public ResponseEntity<ErrorResponse> handleBadReq(BadRequestException ex,\n      HttpServletRequest request) {\n\n    ErrorResponse error = new ErrorResponse(\n        400,\n        \"Bad Request\",\n        ex.getMessage(),\n        request.getRequestURI()\n    );\n\n    return ResponseEntity.status(400).body(error);\n  }\n\n  @ExceptionHandler(Exception.class)\n  public ResponseEntity<ErrorResponse> handleGeneral(Exception ex,\n      HttpServletRequest request) {\n\n    ErrorResponse error = new ErrorResponse(\n        500,\n        \"Internal Server Error\",\n        ex.getMessage(),\n        request.getRequestURI()\n    );\n\n    return ResponseEntity.status(500).body(error);\n  }\n}\n```\n\n위와 같은 전역 예외 처리기를 작성해두고 활용하면 다음과 같은 장점이 있습니다.\n\n- Controller에서는 오직 정상 응답만 처리하고, 오류는 예외로 전달하여 단순한 구조를 유지할 수 있습니다.\n- 에러 응답 형식을 일관적으로 유지할 수 있어 클라이언트에서도 통합된 방식으로 처리할 수 있습니다.\n- 에러 로그를 한 곳에서 관리할 수 있으므로 유지보수가 용이합니다.\n- 서버 내부 오류가 클라이언트에게 HTML이나 서버 에러 페이지로 노출되지 않도록 보호합니다.\n\n## 개선 결과 확인\n\nService에서 예외를 판단하고 throw하고있으므로, Controller 로직은 아래와 같이 단순해졌습니다.\n\n```java\n  @PutMapping(\"/{id}\")\n  public ResponseEntity<?> updateProduct(@PathVariable int id, @RequestBody ProductDto product) {\n    productService.updateProduct(id, product);\n    return ResponseEntity.noContent().build();\n  }\n```\n\n이제 request에서 값이 누락되거나, 없는 데이터를 수정하려고 하는 경우 상황에 맞는 적절한 상태코드를 반환하고, 피드백을 담은 메세지를 반환할 수 있습니다.\n![400 error](./400.png)\n\n![404 error](./404.png)\n\n---\n\n# 5. POST 메소드 Location 헤더 반환\n\nPOST 요청으로 리소스를 생성할 때, RESTful한 응답 패턴은 아래와 같습니다.\n\n1. **상태코드 201 Created 사용**\n2. **Location 헤더 제공**\n\n   서버가 생성한 리소스의 고유 경로를 클라이언트에게 알려주는 역할\n\n3. **바디는 비워도 무관함**\n\n   생성 결과의 위치만 알려주면 클라이언트가 이후 필요한 데이터를 직접 조회할 수 있음\n\n현재 코드는 헤더에 생성된 product에 접근 가능한 경로를 알려주고 있지 않으므로 추가로 개선해줍니다.\n\n아래와 같이 생성된 상품 데이터의 ID를 받아서 `location`을 생성하여 반환할 수 있습니다.\n\n```java\n  @PostMapping\n  public ResponseEntity<Void> saveProduct(@RequestBody ProductDto product) {\n    int createdId = productService.createProduct(product);\n    URI location = URI.create(\"/api/products/\" + createdId);\n    return ResponseEntity.created(location).build();\n  }\n```\n\n이 내용을 적용하고 POST 메소드를 호출하면, 생성된 id가 담긴 Location을 헤더에서 확인할 수 있습니다.\n\n![POST Location header](./post.png)\n\n---\n\n# 마무리\n\nRESTful API 구조와 컨트롤러 설계 방식을 정리하며 다음 내용을 정리할 수 있었습니다.\n\n1. **RESTful API의 핵심 원칙:** URI로 자원을 표현하고 HTTP 메서드로 행위를 구분하는 구조를 다시 명확히 이해할 수 있었습니다.\n2. **ResponseEntity 활용:** 상태 코드와 응답 데이터를 일관성 있게 구성함으로써 API 응답을 더욱 명확하게 표현할 수 있었습니다.\n3. **전역 예외 처리의 중요성:** 공통 예외 처리 로직을 중앙에서 관리하여 에러 응답 형식을 통일할 수 있고, 유지보수성을 높일 수 있음을 확인했습니다.\n","frontmatter":{"date":"2025년 11월 17일","slug":"til-20251117","title":"Spring - REST API 기본 구조 및 예외 처리 구현","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","spring","java"],"thumbnail":null}},{"id":"fa851083-6f40-5aab-84f6-6aba4b9ce76f","excerpt":"이번주는 Spring에서 로그인 처리와 간단한 CRUD를 처리할 수 있는 API를 작성해보았습니다. 저번에 Swing으로 작성했던 미니프로젝트를 Spring을 이용한 웹 서비스로 작성해보려고 합니다.\n\n따로 회원 정보나 인증 부분이 없었기 때문에…","body":"\n> 이번주는 Spring에서 로그인 처리와 간단한 CRUD를 처리할 수 있는 API를 작성해보았습니다.\n>\n> 저번에 Swing으로 작성했던 미니프로젝트를 Spring을 이용한 웹 서비스로 작성해보려고 합니다.\n>\n> 따로 회원 정보나 인증 부분이 없었기 때문에 추가적으로 user 테이블을 생성하여 작업했습니다.\n\n전체 코드는 [Github repo](https://github.com/aaxx98/java-study/tree/main/Spring/CafeWeb)에서 확인할 수 있습니다.\n\n# 1. 유저 정보 테이블 생성\n\ncafe 데이터베이스에는 유저 정보를 저장하는 테이블이 따로 없기 때문에, 로그인 구현을 위해 추가해줍니다.\n\n```sql\nCREATE TABLE user (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    email VARCHAR(150) NOT NULL UNIQUE,\n    password VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n);\n```\n\nerd에 다음과 같이 user 테이블이 추가됩니다.\n![user erd](./user-table.png)\n\n기존 erd는 아래와 같습니다. user와 연관되는 부분은 없고 데이터 조회 api만 작성해두었습니다.\n![pre erd](./erd.png)\n\n# 2. 세션을 이용하여 단순한 로그인 및 인증 확인 구현\n\n## 로그인 API 작성\n\n- **/api/auth/login (POST)**\n  - 로그인 정보 `email`, `password`를 받습니다.\n  - `email`로 사용자 목록에서 일치하는 데이터를 찾아 user에 저장합니다.\n  - 검색된 user의 password와 입력받은 `password`를 비교하여 같으면 로그인 성공입니다.\n  - user가 없거나, password 비교가 틀리면 로그인 실패입니다.\n\n로그인에 성공하면 세션에 사용자 정보를 등록합니다.\n\n```java\n@Controller\n@ResponseBody\n@RequestMapping(\"/api/auth\")\npublic class LoginController {\n\n  private final LoginService loginService;\n\n  public LoginController(LoginService loginService) {\n    this.loginService = loginService;\n  }\n\n  @PostMapping(\"/login\")\n  public ResponseEntity<?> login(@RequestBody LoginRequest loginRequest,\n      HttpServletRequest request) {\n    Optional<UserDto> user = loginService.login(loginRequest);\n    if (user.isPresent()) {\n      HttpSession session = request.getSession(true); // 없으면 새로 생성\n      session.setAttribute(\"userDto\", user);\n      return ResponseEntity.ok(Map.of(\"message\", \"로그인 성공\"));\n    }\n    return ResponseEntity.status(HttpStatus.UNAUTHORIZED)\n        .body(Map.of(\"message\", \"아이디 또는 비밀번호 오류\"));\n  }\n}\n```\n\n![사용자 정보](./data.png)\n\n로그인 정보를 가진 data를 미리 user 테이블에 등록해 두어야 로그인 할 수 있습니다.\n\n![로그인 처리 성공](./login-success.png)\n\npostman으로 로그인 요청을 전송하면 위와 같이 로그인에 성공하는 것을 확인할 수 있습니다.\n\n![사용자 정보 세션](./session.png)\n\n로그인 요청 후, postman의 Cookies를 확인하면 `session.setAttribute(\"userDto\", user);`로 등록한 세션이 생성되어있는 것을 확인할 수 있습니다.\n\n### 세션의 동작 원리\n\nlogin 메소드에서, 아래 내용은 세션을 생성하고, userDto라는 이름으로 user 객체 정보를 세션에 할당하고 있습니다.\n\n```java\nHttpSession session = request.getSession(true);\nsession.setAttribute(\"userDto\", user);\n```\n\n`HttpSession`은 서버(웹 애플리케이션) 내부에서 관리되는 사용자별 상태 저장 객체입니다.\n\n클라이언트가 요청을 보내면, 이 요청 정보는 서블릿 컨테이너가 관리하는 `HttpServletRequest` 객체(`request`)를 통해 확인할 수 있습니다.\n\n`request.getSession(true)`으로 현재 요청을 보낸 클라이언트에 대한 세션을 생성하고, 세션을 식별할 세션 ID (JSESSIONID)를 발급합니다.\n\n서버는 응답 헤더에 아래와 같이 세션 ID를 담아서 클라이언트에게 쿠키를 내려보냅니다.\n\n```\nSet-Cookie: JSESSIONID=ABC123DEF456; Path=/; HttpOnly\n```\n\n쿠키를 전달 받은 클라이언트는 이 쿠키를 저장해두고,\n다음에 같은 도메인으로 요청을 보낼 때 자동으로 쿠키를 붙여서 요청을 보냅니다.\n\n```\nCookie: JSESSIONID=ABC123DEF456\n```\n\n로그인이 되어있는지 확인하려면, 클라이언트에서 보낸 요청으로부터 세션 ID를 받아서 서버 내부 세션 저장소의 세션 데이터가 존재하는지 확인해보면 됩니다.\n\n> 기본적으로 서버 재시작 시 세션도 함께 사라집니다. RAM에 저장되는 정보이기 떄문에, 변수와 같이 관리됩니다.\n>\n> 서버가 잠깐 종료되었다가 재시작해도 세션 유지가 필요하면 Redis와 같은 외부 세션 스토리지를 써야 합니다.\n\n## API 요청 시, 로직 실행 전 인증 여부 확인\n\n로그인 API에서 세션에 user 정보를 등록했습니다. 이제 모든 API 마다 세션을 확인하는 로직을 추가해야 할까요?\n\n공통 메소드로 정의해두고, API 로직 실행 전에 인증을 확인할 수 있지만,\n\n컨트롤러가 실행되기 전, 공통 로직을 정의하는 방식으로 효율적으로 구현할 수 있습니다.\n\n### Interceptor를 활용한 인증 처리\n\n`HandlerInterceptor`를 활용하면 Spring MVC에서 컨트롤러가 실행되기 전후에 공통 로직을 삽입할 수 있습니다.\n\n로그인 상태 확인, 권한 체크, 로깅 등 모든 API 요청에 공통적으로 적용할 수 있는 기능을 구현할 때 유용합니다.\n\n```java\n@Component\npublic class LoginInterceptor implements HandlerInterceptor {\n\n  @Override\n  public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)\n      throws Exception {\n    HttpSession session = request.getSession(false); // 세션 가져오기, 없으면 null\n    if (session != null && session.getAttribute(\"userDto\") != null) {\n      return true; // 로그인 되어 있음\n    }\n    // 로그인 안 되어 있으면 401 에러 반환\n    response.setContentType(\"application/json\"); // JSON 형식\n    response.setCharacterEncoding(\"UTF-8\");       // 한글 깨짐 방지\n    response.setStatus(HttpServletResponse.SC_UNAUTHORIZED); // 401\n\n    // JSON 문자열 작성\n    String json = \"{\\\"status\\\":401, \\\"message\\\":\\\"로그인이 필요합니다.\\\"}\";\n    response.getWriter().write(json);\n    response.getWriter().flush();\n    return false; // 컨트롤러 실행 중단\n  }\n}\n```\n\n**LoginInterceptor**\n\n- 모든 API 요청에 대해 **사용자의 로그인 상태**를 확인합니다.\n- 세션에 `userDto` 정보가 존재하면 요청을 정상적으로 컨트롤러로 전달합니다.\n- 세션 정보가 없으면, **401 Unauthorized 응답**과 함께 로그인 필요 메시지를 반환하고 요청 처리를 중단합니다.\n- 이를 통해 개별 컨트롤러마다 로그인 체크 로직을 반복 작성할 필요가 없습니다.\n\n```java\n@Configuration\npublic class WebConfig implements WebMvcConfigurer {\n\n  @Autowired\n  private LoginInterceptor loginInterceptor;\n\n  @Override\n  public void addInterceptors(InterceptorRegistry registry) {\n    registry.addInterceptor(loginInterceptor)\n        .addPathPatterns(\"/**\")\n        .excludePathPatterns(\n            \"/api/auth/login\",\n            \"/api/user/register\");\n  }\n}\n```\n\n**WebConfig 역할**\n\n- `LoginInterceptor`를 **Spring MVC에 등록**하고, 적용 범위를 설정합니다.\n- `/api/auth/login`, `/api/user/register` 등 로그인이나 회원가입과 같이 인증이 필요 없는 API 경로는 인터셉터 적용에서 **제외**합니다.\n- 나머지 모든 API 요청은 인터셉터에서 로그인 여부를 확인하도록 구성됩니다.\n\n---\n\n인터셉터를 설정하고, 로그인 하지 않은 채로 다른 api를 호출하면 다음과 같이 401 코드와 메세지를 반환합니다.\n\n![인증하지 않았을 때](./order-401.png)\n\n로그인한 뒤, session이 등록되었을 때 api를 호출하면 Controller에 정의된 메소드가 호출되며 다음과 같이 response를 반환합니다.\n\n![orders 조회](./orders.png)\n\n# 3. 개선할 점\n\n세션을 통해 간단히 로그인과 인증을 구현해보았습니다.\n\n하지만 몇가지 아쉬운 점이 있습니다. 차차 개선해나가며 다음 TIL에서 이어서 작성하도록 하겠습니다.\n\n1. **보안 취약점:** 비밀번호를 평문으로 저장하고 있어 보안에 취약합니다. 암호화가 필요합니다.\n2. **세션 만료 시간, 동시 로그인 제한** 등의 고려가 부족합니다.\n3. **JWT 전환:** 모바일 웹 개발이나 MSA 환경을 고려하여 JWT 기반 인증 방식으로 전환\n\n# 마무리\n\n세션 기반 로그인 내용을 정리해보았습니다. 이번 구현을 통해 다음을 배울 수 있었습니다.\n\n1. **HTTP의 Stateless 특성:** HTTP는 기본적으로 상태를 유지하지 않지만, 쿠키와 세션을 통해 상태 관리가 가능합니다.\n2. **HandlerInterceptor의 유용성:** 공통 로직을 중앙에서 관리하여 코드 중복을 제거할 수 있습니다.\n3. **인증의 기본 원리:** 토큰 기반 인증으로 발전하기 전에 세션 기반 인증의 원리를 확실하게 이해했습니다.\n\n다음 단계로는 비밀번호 암호화를 적용하고, 이후 JWT 기반 인증으로 발전시켜 나갈 예정입니다.\n","frontmatter":{"date":"2025년 11월 11일","slug":"til-20251111","title":"Spring - 세션을 이용하여 api 인증 구현하기","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","spring","java"],"thumbnail":null}},{"id":"e5aeda15-bfa5-5783-9515-8526204af562","excerpt":"Java Swing과 JDBC를 이용하여 ☕ 카페 재고 관리 프로그램을 만들었습니다. JDBC는 데이터베이스 연결과 트랜잭션, 자원 해제를 매번 직접 관리해야 하며, 이로 인해 데이터 접근 코드와 비즈니스 로직이 밀접하게 결합되는 단점이 있습니다.…","body":"\n# 미니 프로젝트 개선\n\n> Java Swing과 JDBC를 이용하여 **☕ 카페 재고 관리 프로그램**을 만들었습니다.\n>\n> JDBC는 데이터베이스 연결과 트랜잭션, 자원 해제를 매번 직접 관리해야 하며, 이로 인해 데이터 접근 코드와 비즈니스 로직이 밀접하게 결합되는 단점이 있습니다.\n>\n> JPA를 사용하면 Java 객체 중심으로 데이터베이스를 다룰 수 있고, SQL을 직접 작성하지 않아도 CRUD 작업과 연관관계 매핑을 쉽게 처리할 수 있습니다. 또한 1차 캐시와 변경 감지 기능 덕분에 성능 최적화가 가능하고, 코드 유지보수와 확장성이 훨씬 좋아집니다.\n>\n> 이번 til에서는 JDBC + DAO 구조으 프로젝트를 JPA를 사용하도록 변경하고, **객체지향 설계원칙 SOLID**에 맞추어 개선하는 과정을 공유하려고합니다.\n\n## 1. 현재 미니 프로젝트 폴더 구조\n\n미니 프로젝트는 간단한 재고 및 주문 관리 시스템으로, **JDBC를 직접 사용한 DAO 구조**로 작성되었습니다.\n\n설명과 코드는 [github](https://github.com/aaxx98/java-study/tree/main/MiniProject)에서 확인할 수 있습니다.\n\n### 구조\n\n프로젝트는 다음과 같은 폴더 구조를 가집니다.\n\n```\nmyProj/\n ┣ dao/\n ┃ ┣ StockDAO.java\n ┃ ┣ OrderDAO.java\n ┃ ┗ ProductDAO.java\n ┣ db/\n ┃ ┣ DBUtil.java\n ┃ ┗ DBConnection.java\n ┣ model/\n ┃ ┣ Product.java\n ┃ ┣ Stock.java\n ┃ ┣ OrderSummary.java\n ┃ ┗ OrderItemDetail.java\n ┣ panels/\n ┃ ┣ ProductPanel.java\n ┃ ┣ StockPanel.java\n ┃ ┣ OrderPanel.java\n ┃ ┗ dialogs/\n ┃   ┣ ProductAddDialog.java\n ┃   ┣ ProductEditDialog.java\n ┃   ┣ ProductDeleteDialog.java\n ┃   ┗ OrderDetailDialog.java\n ┣ frames/\n ┃ ┗ MainFrame.java\n ┗ Main.java\n```\n\nERD는 다음과 같습니다.\n\n![erd](./erd.png)\n\n프로젝트 폴더 루트의 `cafe_dump.sql`을 실행하면 cafe 데이터 베이스에 테이블과 데모 데이터들이 생성됩니다.\n\n![cafe db](./cafe-db.png)\n\n### DB 접근\n\n이번 프로젝트의 목적은 데이터베이스와의 직접적인 연결과 SQL 제어 과정을 명확히 이해하는 것이었기 때문에, ORM을 사용하지 않고 JDBC 레벨에서 SQL을 직접 작성했습니다.\n\nDB Connection 부분만 `db/DBConnection.java`에 작성하여 공유하고, 데이터 CRUD는 DAO 객체에 각각 SQL문으로 직접 작성되어있습니다.\n\n### DAO\n\n`OrderDAO`에서는 다음과 같이 `Connection`, `PreparedStatement`, `ResultSet`을 직접 다루며 SQL을 수행합니다.\n\n모든 CRUD 작업마다 DB 연결, 쿼리 파라미터 처리에 필요한 `try-catch`문이 사용됩니다.\n\n아래 코드는 OrderDAO의 insertOrder() 메서드로, 주문 생성부터 재고 차감까지 하나의 트랜잭션으로 처리합니다.\n\n비즈니스 로직과 데이터 접근 로직이 강하게 결합되어있습니다. DB 접근 부분만 분리하는 것이 좋을 것 같습니다.\n\n```java\n  public void insertOrder(List<OrderItemDetail> items) {\n    try (Connection conn = DBConnection.getConnection()) {\n      conn.setAutoCommit(false);\n\n      int orderId;\n\n      // 주문 생성\n      String orderSql = \"INSERT INTO Orders(order_date) VALUES(CURDATE())\";\n      try (PreparedStatement ps = conn.prepareStatement(orderSql,\n          Statement.RETURN_GENERATED_KEYS)) {\n        ps.executeUpdate();\n        try (ResultSet rs = ps.getGeneratedKeys()) {\n          if (rs.next()) {\n            orderId = rs.getInt(1);\n          } else {\n            throw new SQLException(\"주문 ID 생성 실패\");\n          }\n        }\n      }\n\n      // 주문 항목 추가\n      String itemSql = \"INSERT INTO OrderItems(order_id, product_id, quantity, price) VALUES(?, ?, ?, ?)\";\n      try (PreparedStatement ps = conn.prepareStatement(itemSql)) {\n        for (OrderItemDetail item : items) {\n          ps.setInt(1, orderId);\n          ps.setInt(2, item.productId);\n          ps.setInt(3, item.quantity);\n          ps.setInt(4, item.price);\n          ps.addBatch();\n        }\n        ps.executeBatch();\n      }\n\n      // 재고 감소 처리\n      String stockCheckSql = \"SELECT stock_manage FROM Products WHERE id = ?\";\n      String stockUpdateSql = \"UPDATE Stocks SET quantity = quantity - ? WHERE product_id = ?\";\n      try (PreparedStatement checkPs = conn.prepareStatement(stockCheckSql);\n          PreparedStatement updatePs = conn.prepareStatement(stockUpdateSql)) {\n\n        for (OrderItemDetail item : items) {\n          checkPs.setInt(1, item.productId);\n          try (ResultSet rs = checkPs.executeQuery()) {\n            if (rs.next() && rs.getInt(\"stock_manage\") == 1) {\n              updatePs.setInt(1, item.quantity);\n              updatePs.setInt(2, item.productId);\n              updatePs.executeUpdate();\n            }\n          }\n        }\n      }\n      conn.commit();\n    } catch (SQLException e) {\n      DBUtil.showSQLException(e, \"주문 저장 중 오류\");\n      try {\n        // 롤백 추가\n        DBConnection.getConnection().rollback();\n      } catch (SQLException rollbackEx) {\n        System.err.println(\"롤백 실패: \" + rollbackEx.getMessage());\n      }\n    }\n  }\n```\n\n### Model\n\n`model` 패키지의 클래스(`OrderItemDetail`, `OrderSummary` 등)는 DB 테이블 그대로가 아닌, **UI 페이지마다 필요한 정보를 담는 형태**로 구성했습니다.\n\n각 화면에서 필요한 데이터만 받고, 보여주기 위해 엔티티를 단순화한 설계로, 빠른 프로토타이핑에 초점을 두었습니다.\n\n```java\npublic class OrderItemDetail {\n\n  public int productId;\n  public String productName;\n  public int quantity;\n  public int price;\n\n  public OrderItemDetail(int productId, String productName, int quantity, int price) {\n    this.productId = productId;\n    this.productName = productName;\n    this.quantity = quantity;\n    this.price = price;\n  }\n}\n```\n\n`model/OrderItemDetail.java`는 테이블로 명시되어 있지 않습니다. `OrderItem`과 `Stock` 두 엔티티를 혼재하여 사용하고 있습니다.\n\n데이터 model이라기 보다는, dto에 가까운 형태입니다. model의 역할과 dto의 역할이 혼재되어 있으므로 분리가 필요합니다.\n\nmodel 클래스 변수를 public으로 선언한 것은, dto의 객체가 처음 생성된 후에 읽기 외에 새로 변수를 변경하거나 하는 로직이 없기 때문에, getter를 반복하여 작성하는 것을 피하려는 의도였습니다.\n\n하지만 추후에 캡슐화, 데이터 은닉을 중요하게 여기는 자바의 속성에 위배되는 코드라는 것을 깨달았고,\n\nJava 16 이후 문법에서는 `record`로 선언하여 불변성을 보장할 수 있다는 것을 알게 되었습니다.\n\n---\n\n## 2. JDBC + DAO 구조의 문제점\n\nJPA로 전환하기 전에, 현재 JDBC + DAO 구조에서 발견되는 주요 문제점을 정리해보겠습니다.\n\n### 반복적인 Boilerplate 코드\n\n모든 DAO 메서드마다 `Connection`, `PreparedStatement`, `ResultSet`을 생성하고 닫는 코드가 반복됩니다.\n\n`SQLException`을 catch하고 처리하는 부분도 매번 반복됩니다. 예외 처리 로직이 바뀌면 모든 DAO를 일일이 수정해야 합니다.\n\nDBMS 변경, SQL Dialect 변경 등 구조적 변화가 필요할 때 DAO 코드 전반을 수정해야 합니다.\n\n```java\ntry (Connection conn = DBConnection.getConnection();\n    PreparedStatement ps = conn.prepareStatement(sql)) {\n  // 실제 비즈니스 로직\n} catch (SQLException e) {\n  DBUtil.showSQLException(e, \"...\");\n}\n```\n\n### SQL 의존성과 데이터베이스 종속성\n\n```java\nString sql = \"\"\"\n    SELECT o.id, o.order_date, o.status, ...\n    FROM Orders o\n    JOIN OrderItems oi ON o.id = oi.order_id\n    WHERE DATE(o.order_date) = ?\n    ...\n    \"\"\";\n```\n\nJDBC에서는 위와 같이 SQL이 DAO 내부에 직접 작성되어 있어, 테이블명이나 컬럼명이 변경되면 관련 DAO 코드를 모두 수정해야 합니다.\n\n반면 JPA에서는 엔티티 클래스와 그 필드명을 기반으로 쿼리를 작성하므로, 데이터베이스가 아닌 객체 구조에 의존합니다.\n\n즉, SQL의 종속성이 데이터베이스에서 도메인 모델로 옮겨가며, 데이터베이스 교체나 스키마 변경에도 유연하게 대응할 수 있습니다.\n\n### DTO와 Entity의 혼재\n\n`OrderItemDetail`처럼 UI 화면에 맞춘 DTO가 데이터 계층에서도 사용되고 있습니다. 이는 도메인 모델이 없고, 데이터 계층과 표현 계층이 강하게 결합되어 있음을 의미합니다.\n\n`OrderItemDetail`은 UI를 위한 DTO인데, DAO의 `insertOrder`에서도 그대로 사용됩니다.\n\n```java\npublic void insertOrder(List<OrderItemDetail> items) {\n  // ...\n}\n```\n\n실제로는 `Order`, `OrderItem`, `Product` 같은 도메인 엔티티가 존재해야 하고, 이들 간의 연관관계가 명확히 정의되어야 합니다.\n\n### SOLID 원칙 위반\n\n1. **Single Responsibility Principle (SRP) 위반**\n\n- `OrderDAO.insertOrder()` 메서드를 보면, 하나의 메서드가 너무 많은 책임을 가지고 있습니다.\n  - 주문 생성\n  - 주문 항목 추가\n  - 재고 확인 및 차감\n  - 트랜잭션 관리\n  - 예외 처리 및 롤백 처리\n\n  ```java\n    public void insertOrder(List<OrderItemDetail> items) {\n    try (Connection conn = DBConnection.getConnection()) {\n      conn.setAutoCommit(false);\n\n      int orderId;\n\n      // 주문 생성\n      String orderSql = \"INSERT INTO Orders(order_date) VALUES(CURDATE())\";\n      try (PreparedStatement ps = conn.prepareStatement(orderSql,\n          Statement.RETURN_GENERATED_KEYS)) {\n        ps.executeUpdate();\n        try (ResultSet rs = ps.getGeneratedKeys()) {\n          if (rs.next()) {\n            orderId = rs.getInt(1);\n          } else {\n            throw new SQLException(\"주문 ID 생성 실패\");\n          }\n        }\n      }\n\n      // 주문 항목 추가\n      String itemSql = \"INSERT INTO OrderItems(order_id, product_id, quantity, price) VALUES(?, ?, ?, ?)\";\n      try (PreparedStatement ps = conn.prepareStatement(itemSql)) {\n        for (OrderItemDetail item : items) {\n          ps.setInt(1, orderId);\n          ps.setInt(2, item.productId);\n          ps.setInt(3, item.quantity);\n          ps.setInt(4, item.price);\n          ps.addBatch();\n        }\n        ps.executeBatch();\n      }\n\n      // 재고 감소 처리\n      String stockCheckSql = \"SELECT stock_manage FROM Products WHERE id = ?\";\n      String stockUpdateSql = \"UPDATE Stocks SET quantity = quantity - ? WHERE product_id = ?\";\n      try (PreparedStatement checkPs = conn.prepareStatement(stockCheckSql);\n          PreparedStatement updatePs = conn.prepareStatement(stockUpdateSql)) {\n\n        for (OrderItemDetail item : items) {\n          checkPs.setInt(1, item.productId);\n          try (ResultSet rs = checkPs.executeQuery()) {\n            if (rs.next() && rs.getInt(\"stock_manage\") == 1) {\n              updatePs.setInt(1, item.quantity);\n              updatePs.setInt(2, item.productId);\n              updatePs.executeUpdate();\n            }\n          }\n        }\n      }\n      conn.commit();\n    } catch (SQLException e) {\n      DBUtil.showSQLException(e, \"주문 저장 중 오류\");\n      try {\n        // 롤백 추가\n        DBConnection.getConnection().rollback();\n      } catch (SQLException rollbackEx) {\n        System.err.println(\"롤백 실패: \" + rollbackEx.getMessage());\n      }\n    }\n  }\n  ```\n\n2. **Open/Closed Principle (OCP) 위반**\n\n- 새로운 비즈니스 로직(예: 할인 적용, 포인트 적립)을 추가하려면 DAO 코드를 직접 수정해야 합니다. 확장에는 열려있지 않고, 수정에 닫혀있지도 않은 구조입니다.\n\n3. **Dependency Inversion Principle (DIP) 위반**\n\n- 상위 레벨 모듈(`OrderPanel`)이 하위 레벨 모듈(`OrderDAO`)에 직접 의존하고 있습니다. 추상화 계층이 없어 테스트가 어렵고, 구현체 교체가 불가능합니다.\n  ![OrderPanel](./OrderPanel.png)\n\n---\n\n## 3. JPA + SOLID 원칙 준수로 리팩토링\n\n새로운 프로젝트 구조와 주요 변경사항은 다음과 같습니다.\n\n1. **JPA Entity 작성**: `domain`에 JPA entity를 작성합니다. 기존 model은 DTO 역할에 가깝기 때문에 DTO로 변경합니다.\n2. **Repository 패턴 도입**: DAO의 DB 접근 로직을 JPA `EntityManager` 사용하는 Repository로 교체하여 `repository`에 작성합니다.\n3. **Service 계층 분리**: 비즈니스 로직을 DAO에서 분리하여 `service`에 작성합니다.\n4. **의존성 주입(DI)**: service, repository는 인터페이스를 작성하고, 인터페이스를 상속받는 구현체를 작성합니다. `JPAConfig`에서 `service`와 `repository`를 생성하고 주입합니다.\n5. **DTO 명시**: domain entity 클래스와 구별하기 위해 DTO 클래스명에 DTO를 붙여줍니다. DTO를 `record`로 선언하여 불변 객체로 만듭니다.\n\n```\nmyProj/\n ┣ domain/\n ┃ ┣ Orders.java\n ┃ ┣ OrderItems.java\n ┃ ┣ Products.java\n ┃ ┗ Stocks.java\n ┣ repository/\n ┃ ┣ OrderRepository.java\n ┃ ┣ OrderRepositoryImpl.java\n ┃ ┣ ProductRepositoryImpl.java\n ┃ ┣ ProductRepository.java\n ┃ ┗ StockRepository.java\n ┃ ┗ StockRepositoryImpl.java\n ┣ service/\n ┃ ┣ OrderService.java\n ┃ ┣ OrderServiceImpl.java\n ┃ ┣ ProductService.java\n ┃ ┣ ProductServiceImpl.java\n ┃ ┗ StockService.java\n ┃ ┣ StockServiceImpl.java\n ┣ dto/\n ┃ ┣ OrderSummaryDTO.java\n ┃ ┗ OrderItemDetailDTO.java\n ┣ panels/\n ┃ ┣ ProductPanel.java\n ┃ ┣ StockPanel.java\n ┃ ┣ OrderPanel.java\n ┃ ┗ dialogs/\n ┃   ┣ ProductAddDialog.java\n ┃   ┣ ProductEditDialog.java\n ┃   ┣ ProductDeleteDialog.java\n ┃   ┗ OrderDetailDialog.java\n ┗ config/\n   ┗ JPAConfig.java\n```\n\n---\n\n프로젝트를 maven으로 전환하려면 `pom.xml`, `recourses`을 작성하고, 기존 프로젝트를 `src/main/java` 경로에 넣어주면 됩니다.\n\n![maven으로 변경](./jpa-xml.png)\n\n자세한 내용은 코드를 참고하길 바랍니다.\n\n### 1) JPA Entity 작성\n\ndomain에 데이터 테이블을 기반으로 JAP Entity를 작성해줍니다.\n\n기존 DTO는 UI 중심이었지만, domain은 데이터 구조 중심으로 로직을 작성하게 됩니다.\n\njpql을 작성하는 경우에도, 이 Entity 구조를 기반으로 작성하게 됩니다.\n\n```java\n@Entity\npublic class Orders {\n\n  @Id\n  @GeneratedValue(strategy = GenerationType.IDENTITY) // auto increase\n  private int id;\n\n  @Temporal(TemporalType.DATE)\n  @Column(name = \"order_date\")\n  private LocalDate orderDate;\n  private boolean status;\n\n  public Orders() {\n  } // 기본생성자 - JPA에서 사용됨\n\n  public Orders(LocalDate orderDate, boolean status) {\n    this.orderDate = orderDate;\n    this.status = status;\n  }\n\n  public int getId() {\n    return id;\n  }\n\n  public void setId(int id) {\n    this.id = id;\n  }\n\n  public LocalDate getOrderDate() {\n    return orderDate;\n  }\n\n  public void setOrderDate(LocalDate orderDate) {\n    this.orderDate = orderDate;\n  }\n\n  public boolean isStatus() {\n    return status;\n  }\n\n  public void setStatus(boolean status) {\n    this.status = status;\n  }\n\n  @Override\n  public String toString() {\n    return \"Orders{\" +\n        \"id=\" + id +\n        \", orderDate=\" + orderDate +\n        \", status=\" + status +\n        '}';\n  }\n}\n```\n\n### 2) Repository 계층 작성\n\nJPA를 사용하면 기본 CRUD는 EntityManager가 제공하므로, Repository는 단순히 인터페이스로 선언하고 필요한 커스텀 쿼리만 추가합니다.\n\n구현체는 EntityManager를 사용하여 CRUD를 수행하도록 작성합니다.\n\n```java\npublic interface OrderRepository {\n\n  void save(Orders order);\n\n  Optional<Orders> findById(Integer id);\n\n  List<Orders> findByOrderDate(LocalDate date);\n\n  void updateStatus(Integer orderId, String status);\n}\n```\n\n```java\npackage myProj.repository;\n\nimport jakarta.persistence.EntityManager;\nimport jakarta.persistence.TypedQuery;\nimport java.time.LocalDate;\nimport java.util.List;\nimport java.util.Optional;\nimport myProj.domain.Orders;\n\npublic class OrderRepositoryImpl implements OrderRepository {\n\n  private final EntityManager em;\n\n  public OrderRepositoryImpl(EntityManager em) {\n    this.em = em;\n  }\n\n  @Override\n  public void save(Orders order) {\n    if (order.getId() == 0) {\n      em.persist(order);\n    } else {\n      em.merge(order);\n    }\n  }\n\n  @Override\n  public Optional<Orders> findById(Integer id) {\n    return Optional.ofNullable(em.find(Orders.class, id));\n  }\n\n  @Override\n  public List<Orders> findByOrderDate(LocalDate date) {\n    TypedQuery<Orders> query = em.createQuery(\n        \"SELECT o FROM Orders o WHERE o.orderDate = :date ORDER BY o.id DESC\",\n        Orders.class\n    );\n    query.setParameter(\"date\", date);\n    return query.getResultList();\n  }\n\n  @Override\n  public void updateStatus(Integer orderId, String status) {\n    Orders order = em.find(Orders.class, orderId);\n    if (order != null) {\n      order.setStatus(status);\n    }\n  }\n}\n```\n\n### 3) Service 계층 작성\n\nService 계층은 비즈니스 로직을 담당하며, Repository를 의존성으로 주입받아 데이터를 처리합니다.\n\nDAO에서 직접 데이터 접근과 비즈니스 로직을 함께 처리하던 구조에서 벗어나, 책임이 명확히 분리됩니다.\n\n```java\npublic interface OrderService {\n\n  List<OrderSummaryDTO> getOrdersByDate(LocalDate date);\n\n  List<ProductDTO> getAllProducts();\n\n  void createOrder(List<OrderItemDetailDTO> orderItems);\n\n  List<OrderItemDetailDTO> getOrderItems(Integer orderId);\n\n  void updateOrderStatus(Integer orderId, String status);\n}\n```\n\n`OrderServiceImpl`은 주문 생성, 조회, 상태 변경 등 비즈니스 로직의 중심이 되는 클래스입니다.\n\n`createOrder()`에서 여러 테이블에 접근해야하기 때문에 여러 Repository를 조합해서 하나의 트랜잭션을 처리하고 있습니다.\n\n```java\npublic class OrderServiceImpl implements OrderService {\n\n  private final OrderRepository orderRepository;\n  private final ProductRepository productRepository;\n  private final StockRepository stockRepository;\n  private final EntityManager em;\n\n  public OrderServiceImpl(\n      OrderRepository orderRepository,\n      ProductRepository productRepository,\n      StockRepository stockRepository,\n      EntityManager em) {\n    this.orderRepository = orderRepository;\n    this.productRepository = productRepository;\n    this.stockRepository = stockRepository;\n    this.em = em;\n  }\n\n  @Override\n  public List<OrderSummaryDTO> getOrdersByDate(LocalDate date) {\n    List<Orders> orders = orderRepository.findByOrderDate(date);\n\n    return orders.stream()\n        .map(order -> {\n          int itemCount = order.getOrderItems().size();\n          int totalPrice = order.getOrderItems().stream()\n              .mapToInt(item -> item.getPrice() * item.getQuantity())\n              .sum();\n\n          return new OrderSummaryDTO(\n              order.getId(),\n              order.getOrderDate(),\n              order.getStatus(),\n              itemCount,\n              totalPrice\n          );\n        })\n        .collect(Collectors.toList());\n  }\n\n  @Override\n  public List<ProductDTO> getAllProducts() {\n    List<Products> products = productRepository.findAll();\n    return products.stream()\n        .map(product -> {\n          return new ProductDTO(\n              product.getId(),\n              product.getName(),\n              product.getCategory(),\n              product.getPrice(),\n              product.isStockManage()\n          );\n        })\n        .collect(Collectors.toList());\n  }\n\n  @Override\n  public void createOrder(List<OrderItemDetailDTO> items) {\n    EntityTransaction tx = em.getTransaction();\n\n    try {\n      tx.begin();\n\n      // 1. 주문 생성\n      Orders order = new Orders();\n      order.setOrderDate(LocalDate.now());\n      order.setStatus(\"ORDER\");\n\n      // 2. 주문 항목 추가\n      for (OrderItemDetailDTO dto : items) {\n        Products product = productRepository.findById(dto.productId())\n            .orElseThrow(() -> new IllegalArgumentException(\"상품을 찾을 수 없습니다.\"));\n\n        OrderItems orderItem = new OrderItems();\n        orderItem.setProduct(product);\n        orderItem.setQuantity(dto.quantity());\n        orderItem.setPrice(dto.price());\n\n        order.addOrderItem(orderItem);\n\n        // 3. 재고 차감 (재고 관리 대상인 경우만)\n        if (product.isStockManage()) {\n          Stocks stock = stockRepository.findByProductId(product.getId())\n              .orElseThrow(() -> new IllegalStateException(\"재고 정보가 없습니다.\"));\n\n          stock.decrease(dto.quantity());\n        }\n      }\n\n      orderRepository.save(order);\n      tx.commit();\n\n    } catch (Exception e) {\n      if (tx.isActive()) {\n        tx.rollback();\n      }\n      throw new RuntimeException(\"주문 생성 중 오류 발생\", e);\n    }\n  }\n}\n```\n\n### 4) JPAConfig 설정\n\nJPAConfig는 JPA 환경 초기화와 객체 간 의존성 관리를 담당하는 클래스입니다.\n\nEntityManager와 각 Repository, Service의 의존성을 관리합니다.\n\n```java\npublic class JPAConfig {\n\n  private static EntityManagerFactory emf;\n  private static EntityManager em;\n\n  public static void init() {\n    emf = Persistence.createEntityManagerFactory(\"cafeDB\");\n    em = emf.createEntityManager();\n  }\n\n  // Repository 생성\n  public static OrderRepository orderRepository() {\n    return new OrderRepositoryImpl(em);\n  }\n\n  public static ProductRepository productRepository() {\n    return new ProductRepositoryImpl(em);\n  }\n\n  public static StockRepository stockRepository() {\n    return new StockRepositoryImpl(em);\n  }\n\n  // Service 생성\n  public static OrderService orderService() {\n    return new OrderServiceImpl(\n        orderRepository(),\n        productRepository(),\n        stockRepository(),\n        em\n    );\n  }\n\n...\n  public static void close() {\n    if (em != null && em.isOpen()) {\n      em.close();\n    }\n    if (emf != null && emf.isOpen()) {\n      emf.close();\n    }\n  }\n}\n```\n\n![JPAConfig.init()](./init.png)\nMain에서 JPAConfig를 init하고,\n\n패널(view) 생성 시 다음과 같이 사용합니다.\n\n```java\n    tabbedPane.addTab(\"상품관리\", new ProductPanel(JPAConfig.productService()));\n    tabbedPane.addTab(\"재고관리\", new StockPanel(JPAConfig.stockService()));\n    tabbedPane.addTab(\"주문관리\", new OrderPanel(JPAConfig.orderService()));\n```\n\n### 5) DTO 작성\n\nUI(view)단에서 데이터를 전달받거나 표시할 때는 Entity 대신 DTO를 사용합니다.\n\n기존 model이 DTO처럼 사용되고 있었기 때문에, 위치와 클래스명을 DTO에 맞게 변경하였고,\n\nDTO는 데이터 변환에 사용되어 한번 생성되면 내용이 바뀌어서는 안되므로, record로 선언하여 불변성을 유지하고, 계층 간 의존성을 최소화했습니다.\n\n```java\npublic record OrderSummaryDTO(\n    int id,\n    LocalDate orderDate,\n    String status,\n    int itemCount,\n    int totalPrice\n) {\n\n  public String getStatusString() {\n    return switch (this.status) {\n      case \"ORDER\" -> \"주문\";\n      case \"PREPARE\" -> \"준비중\";\n      case \"COMPLETE\" -> \"완료\";\n      default -> \"\";\n    };\n  }\n}\n```\n\n아래와 같이 Repository에서 조회 결과를 entity -> dto로 변환하거나,\nCreate, Update 작업 시 dto -> entity로 변환하여 사용합니다.\n\n```java\n  @Override\n  public List<OrderSummaryDTO> getOrdersByDate(LocalDate date) {\n    List<Orders> orders = orderRepository.findByOrderDate(date);\n\n    return orders.stream()\n        .map(order -> {\n          int itemCount = order.getOrderItems().size();\n          int totalPrice = order.getOrderItems().stream()\n              .mapToInt(item -> item.getPrice() * item.getQuantity())\n              .sum();\n\n          return new OrderSummaryDTO(\n              order.getId(),\n              order.getOrderDate(),\n              order.getStatus(),\n              itemCount,\n              totalPrice\n          );\n        })\n        .collect(Collectors.toList());\n  }\n\n    @Override\n  public void createOrder(List<OrderItemDetailDTO> items) {\n    EntityTransaction tx = em.getTransaction();\n\n    try {\n      tx.begin();\n\n      // 1. 주문 생성\n      Orders order = new Orders();\n      order.setOrderDate(LocalDate.now());\n      order.setStatus(\"ORDER\");\n\n      // 2. 주문 항목 추가\n      for (OrderItemDetailDTO dto : items) {\n        Products product = productRepository.findById(dto.productId())\n            .orElseThrow(() -> new IllegalArgumentException(\"상품을 찾을 수 없습니다.\"));\n\n        OrderItems orderItem = new OrderItems();\n        orderItem.setProduct(product);\n        orderItem.setQuantity(dto.quantity());\n        orderItem.setPrice(dto.price());\n\n        order.addOrderItem(orderItem);\n\n        // 3. 재고 차감 (재고 관리 대상인 경우만)\n        if (product.isStockManage()) {\n          Stocks stock = stockRepository.findByProductId(product.getId())\n              .orElseThrow(() -> new IllegalStateException(\"재고 정보가 없습니다.\"));\n\n          stock.decrease(dto.quantity());\n        }\n      }\n\n      orderRepository.save(order);\n      tx.commit();\n\n    } catch (Exception e) {\n      if (tx.isActive()) {\n        tx.rollback();\n      }\n      throw new RuntimeException(\"주문 생성 중 오류 발생\", e);\n    }\n  }\n```\n\n## 4. MVC와 계층형 아키텍처\n\n이번에 리팩토링한 코드는 **MVC 패턴**을 기반으로 하되, **계층형 아키텍처(Layered Architecture)** 를 일부 적용한 구조입니다.\n\nMVC는 화면, 로직, 데이터를 명확히 분리하는 구조이고, 계층형 아키텍처는 각 역할을 더 세분화해 의존성과 책임을 명확히 하는 방식입니다.\n\n이번 구조에서는 MVC의 Controller와 Model 사이에 Service 계층을 중간에 두어 비즈니스 로직을 분리했습니다.\n즉, UI는 화면과 입력만 담당하고, 데이터 처리나 트랜잭션은 모두 Service에서 담당합니다.\n\n![개선된 아키텍쳐 구조](./new.png)\n\n### MVC\n\nMVC는 Model, View, Controller의 앞 글자를 딴 것으로 애플리케이션을 구성하는 요소를 역할에 따라 세 가지로 분리하는 설계 방식입니다.\n\n![mvc pattern](./mvc.png)\n\n리팩토링한 구조에서는 Panel이 Controller + View 역할을 동시에 수행하고 있습니다.\n\n즉, 사용자의 버튼 클릭 → Service 호출 → 결과 반영 흐름이 MVC 구조로 동작하고 있습니다.\n\n- **Model**: `Domain` 엔티티와 `Repository`가 데이터를 관리\n- **View**: `Panel` (Swing UI 컴포넌트)이 화면을 표시\n- **Controller**: `Panel`의 이벤트 핸들러가 사용자 입력을 처리하고 `Service`를 호출\n\n### 계층형 아키텍쳐\n\n계층형 아키텍처는 시스템을 여러 레이어로 나누어,\n각 레이어가 명확한 역할과 책임을 가지도록 하는 설계 방식입니다.\n\n각 계층은 바로 아래 계층에만 의존하며, 위로는 의존하지 않습니다.\n\n![계층형 아키텍쳐](./layered.png)\n\n현재 리팩토링한 구조에서는 Panel이 Service를 사용하고, Service는 Repository를, Repository는 Entity를 사용하고 있습니다.\n\n- **Presentation Layer** (UI) - 사용자 입력 처리 및 화면 표시: `Panel`\n- **Business Layer** (비즈니스 로직) - 비즈니스 로직 처리: `Service`\n- **Persistence Layer** (데이터 접근) - 데이터베이스 접근: `Repository`\n- **Domain Layer** (도메인 모델) - 핵심 데이터 구조 및 규칙 정의: `Entity`\n\n## 정리\n\n이번 리팩토링을 통해 다음을 실현했습니다.\n\n- 유지보수성 향상: 계층별 책임이 명확해져 수정 범위를 최소화할 수 있었습니다.\n- 확장성 확보: 새로운 기능 추가 시 기존 코드 변경을 최소화했습니다.\n- 코드 품질 향상: SOLID 원칙 적용으로 객체지향적인 구조를 구현했습니다.\n- 생산성 향상: JPA 덕분에 반복적인 JDBC 코드를 제거하고 개발 속도를 높일 수 있었습니다.\n\n처음엔 다소 복잡해 보였지만, 계층을 분리하고 책임을 나누니 오히려 코드가 단순해지고 이해하기 쉬워졌습니다.\n특히 도메인 엔티티가 스스로의 책임을 가지도록 설계하면서, 의도가 명확하고 직관적인 코드를 작성할 수 있게 되었습니다.\n","frontmatter":{"date":"2025년 11월 4일","slug":"til-20251104","title":"미니 프로젝트 리팩토링","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","database","java"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAADLUlEQVR42m1UW0hUYRA+VAT1XODd0MLdVTNdrNS8paZWtqEPWmqkWelS5j1dL0Up6kOQWkKIZXZ7yLYytfK2ESSSafoQPWVBVFLuruZuez3n6//POW55GRjO8M8/3z8z38xhQGT8sw3eZXpsOKnDmTtGegQ7ywEcC7udhc0mKLXtLMv7m/pNWJ+jxbpsLR6PW8QYgKHG7AKLiPp5rM2eRXa7gXfahLgVQt+hon5nwQly9+xdAz79tDt8PGC12oiA6jlcGzRBPW4lHisG33zEy6EpvH4xBc0wUY2gw8OTGBn5ALPZ6njk7bQN3e+FLBm9kYNXqR4tBIzUyB+23x5ApjwLyVFK+J3KR0g40d1EQ/IRFnYOQUFK3OroF+E4PhH3Ij20Bg6MmtS/7bweX7UsOE4AvHDpHpSRSuSklCO8vAr7DxE9UE20CklJNYgIL0Zt7X0H4I85lsegvWRUj4yIJP3jeyA2XFXRjC2yZATK0xEsTYM86Ajk/qmQB6ZhZ/BR+EgVUKlalsTENM6j/KERDCUh/caCwJJdcBYW1cFjcxj8tidCEpgAiX88JLEHIdkRD9+ARLhtCkFxST1/1yqyl9q6gGNtC2CyCGDGMsDignp47IqDrC0HsmgFpIpkSK4chyxKAdnNk3ALjkVpYePqgJWrlFySWwc3VQqkfXnwddsHn4sZkJxNg4zY0udKuFamoCyvfkXJFbTkrjELvAnL3/Wsg+X8/MvwdImAX0UmZAkKSAigLI5kl3gYfpWZcHfag4KCOgcp33QsPEv0/GwyOjI2roU6tA79G5urTZ1wdg7FVue98PKIhpckBl7exPaM5s+cnELR3NLpALxOxoZuGl0QfrCfTlgwRtZvUUwmM4Y1o3jWp0Fv3yv09pBv7yvepmeDQ6Ok4RY8mbDxM7ijZg416j/CYIstwPQvFoUPDDjdYUDXmHnJmi0XkTvkkrt0n6MbfpPshMvMopMO+MZTZNlPaNE8YBIDhZ8BZdIq/hxoiXbxpXP3jFiTpYVLgR4TX2xLAXsmrTw5UQ3zfAm0pzNzHLj/sqT2zDxZNeKjADNkQ7pJu/qmrNAZhAz/Av6RqAXP/n0qAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/4947d900e3f0dc45c4fe2da180772938/936cb/refactoring.png","srcSet":"/static/4947d900e3f0dc45c4fe2da180772938/8f530/refactoring.png 128w,\n/static/4947d900e3f0dc45c4fe2da180772938/c7c09/refactoring.png 256w,\n/static/4947d900e3f0dc45c4fe2da180772938/936cb/refactoring.png 512w","sizes":"(min-width: 512px) 512px, 100vw"},"sources":[{"srcSet":"/static/4947d900e3f0dc45c4fe2da180772938/34e4f/refactoring.webp 128w,\n/static/4947d900e3f0dc45c4fe2da180772938/3247e/refactoring.webp 256w,\n/static/4947d900e3f0dc45c4fe2da180772938/8fb52/refactoring.webp 512w","type":"image/webp","sizes":"(min-width: 512px) 512px, 100vw"}]},"width":800,"height":600}}}}},{"id":"18c137b7-dc66-518b-bdcc-06f466f87597","excerpt":"이번 주에는 웹 프론트엔드의 기본적인 내용에 대해 공부하고, 기본적인 html, css, javacript를 이용하여 팀 소개, 개인 소개 페이지를 만들어보았습니다. netlify로 배포, firebase로 데이터 받기, 보여주기도 추가로…","body":"\n> 이번 주에는 웹 프론트엔드의 기본적인 내용에 대해 공부하고, 기본적인 html, css, javacript를 이용하여 팀 소개, 개인 소개 페이지를 만들어보았습니다.\n>\n> netlify로 배포, firebase로 데이터 받기, 보여주기도 추가로 진행했습니다.\n\n# HTML, CSS, JavaScript 작동 방식 정리\n\n![web - html, js, css](./web.png)\n\n## HTML (HyperText Markup Language)\n\n- **역할** 웹 페이지의 구조와 콘텐츠를 정의\n- **작동 방식**\n  - 브라우저가 HTML을 파싱하여 DOM(Document Object Model) 트리 구조 생성\n  - 각 태그는 노드가 되어 계층 구조를 형성\n  - 시맨틱 태그(`<header>`, `<nav>`, `<main>` 등)로 의미있는 구조 표현\n\n## CSS (Cascading Style Sheets)\n\n- **역할**: HTML 요소의 시각적 표현(레이아웃, 색상, 폰트 등) 정의\n- **작동 방식**\n  - 브라우저가 CSS를 파싱하여 CSSOM(CSS Object Model) 생성\n  - Specificity(명시도) 규칙에 따라 스타일 우선순위 결정\n  - Cascade(종속) 원리로 여러 스타일 규칙을 병합\n  - 선택자를 통해 DOM 요소와 매칭\n\n## JavaScript\n\n- **역할**: 웹 페이지에 동적 기능과 인터랙티브 제공\n- **작동 방식**\n  - 싱글 스레드 이벤트 루프 기반으로 실행\n  - Call Stack에서 동기 코드 실행\n  - Web API를 통해 비동기 작업 처리\n  - Callback Queue와 Event Loop로 비동기 작업 완료 후 실행\n  - DOM API로 HTML 요소를 동적으로 조작\n\n- **이벤트 루프의 동작 과정**\n  ![JavaScript Event Loop](./eventLoop.png)\n  - Call Stack: 자바스크립트 함수 호출이 쌓이는 스택\n  - Web APIs: 타이머, 네트워크 요청 등을 처리하는 브라우저의 API\n  - Callback Queue: 비동기 작업이 완료되면 해당 콜백 함수가 이 큐에 추가됨\n  - Event Loop: 호출 스택이 비어 있을 때 콜백 큐에서 콜백 함수를 꺼내 호출 스택에 추가하여 실행함\n\n# 웹 애플리케이션의 렌더링 방식\n\n## SPA (Single Page Application)\n\n### SPA란?\n\n- **단일 페이지로 구성된 웹 애플리케이션**\n- 최초 한 번만 페이지를 로드하고, 이후에는 JavaScript로 동적으로 콘텐츠를 변경\n- 페이지 전환 시 전체 페이지를 새로고침하지 않고 필요한 부분만 업데이트\n\n### SPA 작동 방식\n\n```\n1. 초기 로드: HTML, CSS, JS 파일 한 번에 다운로드\n2. 라우팅: URL 변경 시 JavaScript가 감지\n3. 컴포넌트 전환: 필요한 컴포넌트만 렌더링\n4. API 호출: 서버에서 데이터만 가져옴 (JSON)\n5. DOM 업데이트: 가져온 데이터로 화면 갱신\n```\n\n### SPA의 장점\n\n- **빠른 사용자 경험**: 페이지 새로고침 없이 즉각적인 반응\n- **서버 부하 감소**: HTML 렌더링을 클라이언트에서 처리\n- **모바일 앱과 유사한 경험**: 부드러운 전환 효과\n- **개발 효율성**: 프론트엔드와 백엔드 분리\n\n### SPA의 단점\n\n- **초기 로딩 시간**: 첫 로드 시 많은 JS 파일 다운로드\n- **SEO 문제**: 검색 엔진 크롤러가 콘텐츠를 제대로 읽지 못할 수 있음\n- **JavaScript 의존성**: JS가 비활성화되면 작동 불가\n- **메모리 관리**: 장시간 사용 시 메모리 누수 가능성\n\n### 대표적인 SPA 프레임워크\n\n- **React**: Meta(Facebook) 개발, 컴포넌트 기반\n- **Vue**: 점진적 프레임워크, 학습 곡선 완만\n- **Angular**: Google 개발, 완전한 프레임워크\n- **Svelte**: 컴파일 타임에 최적화\n\n## CSR (Client Side Rendering)\n\n![CSR Flow](./csr.png)\n\n### CSR이란?\n\n- **클라이언트(브라우저)에서 JavaScript로 렌더링**\n- SPA의 기본 렌더링 방식\n\n### CSR 작동 과정\n\n1. 서버: 빈 HTML + JavaScript 번들 전송\n\n```html\n<div id=\"root\"></div>\n<script src=\"bundle.js\"></script>\n```\n\n2. 브라우저: JavaScript 다운로드 및 실행\n3. JavaScript: API 호출 → 데이터 가져오기\n4. React/Vue 등\n   데이터로 컴포넌트 렌더링\n5. 브라우저: 최종 화면 표시\n\n### CSR의 장점\n\n- **풍부한 인터랙션**: 즉각적인 사용자 반응\n- **서버 부담 적음**: HTML 생성 불필요\n- **부분 업데이트**: 필요한 부분만 다시 그림\n\n### CSR의 단점\n\n- **초기 로딩 느림**: JS 다운로드 + 실행 + API 호출\n- **SEO 불리**: 빈 HTML만 크롤링됨\n- **JavaScript 필수**: JS 없으면 화면 안 보임\n\n---\n\n## SSR (Server Side Rendering)\n\n![SSR Flow](./ssr.png)\n\n### SSR이란?\n\n- **서버에서 HTML을 완전히 렌더링하여 전송**\n- 최초 로드 시 완성된 HTML 제공\n\n### SSR 작동 과정\n\n1. 브라우저: 페이지 요청\n\n2. 서버:\n   - 데이터 가져오기 (DB, API 등)\n   - React/Vue 컴포넌트 실행\n   - 완성된 HTML 생성\n\n3. 브라우저:\n   - HTML 즉시 표시 (First Paint)\n   - JavaScript 다운로드\n   - Hydration (이벤트 리스너 연결)\n\n4. 이후: CSR처럼 동작\n\n### SSR의 장점\n\n- **빠른 초기 로딩**: 완성된 HTML 즉시 표시\n- **SEO 최적화**: 크롤러가 완전한 콘텐츠 읽기 가능\n- **저사양 기기**: JS 실행 전에도 콘텐츠 보임\n\n### SSR의 단점\n\n- **서버 부하**: 매 요청마다 렌더링 필요\n- **복잡한 구현**: 서버/클라이언트 환경 모두 고려\n- **TTFB 느림**: 서버 렌더링 시간 소요\n\n### SSR 프레임워크\n\n- **Next.js** (React 기반)\n- **Nuxt.js** (Vue 기반)\n- **SvelteKit** (Svelte 기반)\n\n---\n\n# firebase로 데이터 저장하기\n\n다음과 같은 페이지를 만들었습니다.\n\n이름/이메일/신청사유 등을 적으면 데이터가 저장되고, 신청란 하단에서 확인할 수 있는 페이지입니다.\n![페이지](./page.png)\n\n## firebase console에서 프로젝트 생성\n\n![firebase console](./firebase-console.png)\n\n1. [firebase 콘솔](https://console.firebase.google.com/)에 들어갑니다.\n2. 프로젝트 생성 후, **웹 애플리케이션(Web App)** 을 추가합니다.\n   - 앱 이름을 입력하고 “Firebase SDK 추가” 단계까지 진행합니다.\n   - 완료 후 표시되는 Firebase 설정 스크립트(firebaseConfig)를 복사해둡니다.\n\n3. **Build → Realtime Database** 메뉴로 이동하여 실시간 데이터베이스를 생성합니다.\n4. Database 규칙 설정\n   - 프로젝트 초기에 테스트 중이라면 아래와 같이 임시로 전체 공개 권한을 설정할 수 있습니다.\n\n   ```\n    {\n      \"rules\": {\n        \".read\": true,\n        \".write\": true\n      }\n    }\n   ```\n\n## 신청 페이지 작성\n\n### HTML\n\n```html\n<!DOCTYPE html>\n<html lang=\"ko\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>팀 신청폼</title>\n    <link\n      href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css\"\n      rel=\"stylesheet\"\n    />\n    <link rel=\"stylesheet\" href=\"form.css\" />\n  </head>\n  <body>\n    <div class=\"top-bar\">\n      <div class=\"container\">\n        <a href=\"../index.html\" class=\"btn btn-outline-primary btn-md\"> 🏠 메인페이지로 </a>\n      </div>\n    </div>\n\n    <div class=\"container\">\n      <div class=\"card p-4\">\n        <h2 class=\"text-center mb-4\">팀 합류 신청서</h2>\n        <form id=\"joinForm\">\n          <div class=\"mb-3\">\n            <label for=\"name\" class=\"form-label\">이름</label>\n            <input\n              type=\"text\"\n              class=\"form-control\"\n              id=\"name\"\n              placeholder=\"이름을 입력하세요\"\n              required\n            />\n          </div>\n\n          <div class=\"mb-3\">\n            <label for=\"email\" class=\"form-label\">이메일</label>\n            <input\n              type=\"email\"\n              class=\"form-control\"\n              id=\"email\"\n              placeholder=\"example@email.com\"\n              required\n            />\n          </div>\n\n          <div class=\"mb-3\">\n            <label for=\"message\" class=\"form-label\">자기소개 / 합류 이유</label>\n            <textarea\n              class=\"form-control\"\n              id=\"message\"\n              rows=\"4\"\n              placeholder=\"간단히 소개해주세요\"\n              required\n            ></textarea>\n          </div>\n\n          <button type=\"submit\" class=\"submit-btn btn btn-primary\">제출하기</button>\n        </form>\n        <div id=\"alertBox\" class=\"mt-3\"></div>\n\n        <hr class=\"my-4\" />\n\n        <h5 class=\"mb-3 text-center\">현재 신청자 <span id=\"applicantCount\">0</span>명</h5>\n        <ul id=\"applicantList\" class=\"list-group small\"></ul>\n      </div>\n    </div>\n    <script type=\"module\" src=\"firebase.js\"></script>\n  </body>\n</html>\n```\n\n### CSS\n\n```css\nbody {\n  background: linear-gradient(135deg, #f8f9fa, #e9ecef);\n  font-family: 'Noto Sans KR', sans-serif;\n}\n.container {\n  margin-top: 30px;\n}\n.card {\n  border: none;\n  border-radius: 15px;\n  box-shadow: 0 5px 20px rgba(0, 0, 0, 0.1);\n}\n.btn-primary {\n  background: #007bff;\n  border: none;\n}\n.btn-primary:hover {\n  background: #0056b3;\n}\n.submit-btn {\n  display: block;\n  margin: 20px auto 0 auto;\n  font-size: 1.2rem;\n  font-weight: 600;\n  letter-spacing: 0.5px;\n  padding: 12px 40px;\n  border-radius: 50px;\n  box-shadow: 0 4px 10px rgba(0, 123, 255, 0.25);\n  transition: all 0.2s ease;\n  width: auto;\n}\n.submit-btn:hover {\n  transform: translateY(-1px);\n  box-shadow: 0 6px 15px rgba(0, 123, 255, 0.35);\n}\n```\n\n### JS\n\n- 프로젝트 생성시 받은 `firebaseConfig`를 작성합니다.\n- firebase에 데이터를 입력하고, 데이터를 가져오는 로직이 작성되어있습니다.\n\n```js\nimport { initializeApp } from 'https://www.gstatic.com/firebasejs/12.4.0/firebase-app.js';\nimport {\n  getDatabase,\n  ref,\n  push,\n  set,\n  onValue,\n} from 'https://www.gstatic.com/firebasejs/12.4.0/firebase-database.js';\n\nconst firebaseConfig = {\n  // ... config 작성\n};\n\n// Firebase 초기화\nconst app = initializeApp(firebaseConfig);\nconst db = getDatabase(app);\n\n// 폼 이벤트 리스너\nconst form = document.getElementById('joinForm');\nconst alertBox = document.getElementById('alertBox');\n\nform.addEventListener('submit', async (e) => {\n  e.preventDefault();\n\n  const name = document.getElementById('name').value.trim();\n  const email = document.getElementById('email').value.trim();\n  const message = document.getElementById('message').value.trim();\n\n  if (!name || !email || !message) {\n    alertBox.innerHTML = '<div class=\"alert alert-danger\">모든 항목을 입력해주세요.</div>';\n    return;\n  }\n\n  try {\n    const submissionsRef = ref(db, 'team_applications');\n    const newSubmission = push(submissionsRef);\n\n    await set(newSubmission, {\n      name,\n      email,\n      message,\n      timestamp: new Date().toISOString(),\n    });\n\n    form.reset();\n    alertBox.innerHTML =\n      '<div class=\"alert alert-success\">🎉 신청이 성공적으로 제출되었습니다!</div>';\n  } catch (error) {\n    console.error(error);\n    alertBox.innerHTML =\n      '<div class=\"alert alert-danger\">오류가 발생했습니다. 다시 시도해주세요.</div>';\n  }\n});\n\n// 신청자 목록 갱신\nconst submissionsRef = ref(db, 'team_applications');\n\nonValue(submissionsRef, (snapshot) => {\n  const data = snapshot.val();\n  applicantList.innerHTML = '';\n\n  if (data) {\n    const entries = Object.values(data);\n    applicantCount.textContent = entries.length;\n\n    entries\n      .sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp))\n      .forEach((entry) => {\n        const li = document.createElement('li');\n        li.classList.add('list-group-item');\n        li.innerHTML = `\n          <strong>${entry.name}</strong> (${entry.email})<br />\n          <small class=\"text-muted\">${entry.message}</small>\n        `;\n        applicantList.appendChild(li);\n      });\n  } else {\n    applicantCount.textContent = 0;\n  }\n});\n```\n\n- `ref()`\n  - 데이터베이스에서 특정 위치(path)를 가리키는 참조(reference) 객체를 만드는 함수\n  - 이 참조를 통해 데이터를 읽거나 쓸 수 있음\n  - `snapshot`\n    - `onValue()`, `get()` 등 데이터를 읽어올 때 콜백 함수에서 받는 읽기 결과 객체\n    - 데이터베이스 경로(ref)가 가리키는 값 전체를 담고 있음\n    - 데이터를 꺼낼 수 있는 인터페이스 제공\n- `set()`\n  - 특정 참조에 데이터를 덮어쓰는(write) 함수\n  - 같은 경로가 이미 존재하면 기존 값이 모두 대체됨\n- `push()`\n  - 고유 키를 가진 새 항목을 만들고, 그 위치를 참조 객체로 반환하는 함수\n  - uniqueKey를 생성하여 중복 방지\n\n## 작동 확인\n\n폼을 작성하고 제출하기를 누르면 firebase 콘솔의 Realtime Database에서 다음과 같이 데이터가 들어간 것을 확인할 수 있습니다.\n![데이터 확인](./data.png)\n","frontmatter":{"date":"2025년 10월 28일","slug":"til-20251028","title":"웹 프론트엔드 정리","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","web","front-end"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAACiklEQVR42oWSW0gUYRTHfaq3LlpC0YaRJYpECL0o2cVsc0PFMAMhzdWMlIiECjaiHoykQCLzIUmQ2pDACDQqKqGHJM0L3oJMa3NndmZ2Znfdm7vOzM78+5rdbLXd7cCfA993zm/mO+efhD+hqlqSrHMITgxiaXqEaPivplZpYgji9y9YHUnLvFBIy867TZjPSwZlSAelT1sWU7wDbElEZemgj6SCv3wq0qzEAspadne2wHpoC+gT2aBLM0EXZ8BWkgG3MRPec1nwns2Cr2EPHOU6CM2NK34mJtD7/BGsuRvA3rsGfnIE/PhnCNNjkOhZKMwcFBcH8X0XeEMyXO03EwHDh4v9L2DN2wiu4zaEgAjuTQ/svWYsDfRC+kQ0+g7Bx9dhN2yGx3w/PhBK+DA4/AHUgVSwLRchsAxsNQdBF27DQvUueIxENbvhrc8EV6KDr88cGWFMYHiw4uwUqIKtYEzVEGQVvNUC4ccMZLcAdYGDsmBHoOMSmMIUBD6+TQCMbErmKNDHyRZP54HragVnboP92UMEXndCHnqp1QTaG2ErSCZ2Gk0EDPtQcTthq8gBXb5Xm5+tXg9q31o4j62Bz6TXavy3KsAYdJAs3yJAJT5QDS6CNZK5EagjsAh+bACsqQq+OzWQx/vDTjAVgT2ZTV5j+z8QZGP2pnJQRTvBPX1AZvgTDp8fod/XYhDSYC/c53PA1eYj5HGt7P0HGLngr1ZinniROrpdMzd7pRL+1jr4LuTAXZUGoZS4oO4wVElMAIzatP9Vt9ZAl2bBun8TqNx14PXrwelTwJRlgD2TD093WxQsHjA6yNYly1f4+57A0dwA4UYdvD2dEGcmocpy3LbYwKgnxP9g7JpfJIkLvJd0n7YAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/c3ff7bd189cd94b0904d73903d6c685a/ce24c/web-front.png","srcSet":"/static/c3ff7bd189cd94b0904d73903d6c685a/aacf3/web-front.png 200w,\n/static/c3ff7bd189cd94b0904d73903d6c685a/0e3a3/web-front.png 400w,\n/static/c3ff7bd189cd94b0904d73903d6c685a/ce24c/web-front.png 800w,\n/static/c3ff7bd189cd94b0904d73903d6c685a/ee2e1/web-front.png 1600w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/c3ff7bd189cd94b0904d73903d6c685a/abf68/web-front.webp 200w,\n/static/c3ff7bd189cd94b0904d73903d6c685a/ce081/web-front.webp 400w,\n/static/c3ff7bd189cd94b0904d73903d6c685a/c11e0/web-front.webp 800w,\n/static/c3ff7bd189cd94b0904d73903d6c685a/0843b/web-front.webp 1600w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":600}}}}},{"id":"c7329843-3453-5550-9110-b03237d5a605","excerpt":"이번 주에는 Java 프로젝트에 데이터베이스를 연결하고, MyBatis와 JPA를 사용해보는 시간을 가졌습니다. MyBatis로 SQL을 직접 작성하며 데이터베이스와 상호작용하는 방식을 먼저 경험했고, 이후 JPA로 넘어가면서 데이터베이스 연결에…","body":"\n> 이번 주에는 Java 프로젝트에 데이터베이스를 연결하고, MyBatis와 JPA를 사용해보는 시간을 가졌습니다. MyBatis로 SQL을 직접 작성하며 데이터베이스와 상호작용하는 방식을 먼저 경험했고, 이후 JPA로 넘어가면서 데이터베이스 연결에 대한 서로 다른 접근 방식을 배울 수 있었습니다. 실습을 통해 Maven으로 프로젝트 의존성을 관리하면서 라이브러리 추가가 얼마나 간편해졌는지 체감했고, JPA의 영속성 컨텍스트라는 개념을 처음 접하고 이해하는 시간을 가졌습니다.\n\n# Maven으로 JAVA 프로젝트 관리\n\n![Maven](./maven.jpg)\n\n## Maven\n\n- 메이븐(Apache Maven)은 개발자로 하여금 자바 프로젝트 관리를 쉽게 도와주는 빌드 툴입니다.\n  - 기존에는 필요한 라이브러리(jar 파일)를 수동으로 프로젝트에 추가해야 했으며, 협업 시 각 개발 환경의 라이브러리 버전을 일치시키는 작업이 어려워 버전 관리가 복잡했습니다.\n  - Maven은 `pom.xml`에 필요한 라이브러리 정보만 적어주면, Maven이 알아서 중앙 저장소에서 다운로드해서 프로젝트에 추가해줍니다.\n    - **POM**: \"Porject Object Model\"의 약어로 `pom.xml` 파일로 관리되며, 반복적으로 진행되어 왔던 프로젝트 빌드, 리포트, 문서화 작업을 지원합니다.\n\n- 프로젝트를 생성할 때 build system을 설정할 수 있는데, 이 때 maven을 선택 할 수 있습니다.\n  ![Maven 프로젝트 생성](./new-maven.png)\n\n### Maven Central Repository 워크플로\n\n![Maven Central Repository](./maven-workflow.png)\n\n1. 종속성 정의: 개발자가 pom.xml에 필요한 종속성을 지정합니다.\n2. 검색 및 다운로드: Maven은 먼저 로컬 저장소에서 종속성을 검색하고, 없으면 중앙 저장소에서 가져옵니다.\n3. 아티팩트 가져오기: 중앙 저장소에서 가져온 종속성은 로컬 저장소에 저장되어 이후 사용됩니다.\n4. 프로젝트 빌드: Maven은 다운로드된 종속성을 사용하여 프로젝트를 빌드합니다.\n\n예를 들어, MyBatis를 사용하려면 아래와 같이 `pom.xml`을 작성합니다.\n\n```xml\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>Mybatis</groupId>\n  <artifactId>MybatisBasic</artifactId>\n  <version>0.0.1-SNAPSHOT</version>\n  <name>MybatisBasic</name>\n  <description>MybatisBasic</description>\n\n  <dependencies>\n\n    <!-- https://mvnrepository.com/artifact/com.mysql/mysql-connector-j -->\n    <dependency>\n      <groupId>com.mysql</groupId>\n      <artifactId>mysql-connector-j</artifactId>\n      <version>9.3.0</version>\n    </dependency>\n\n    <!-- https://mvnrepository.com/artifact/org.mybatis/mybatis -->\n    <dependency>\n      <groupId>org.mybatis</groupId>\n      <artifactId>mybatis</artifactId>\n      <version>3.5.6</version>\n    </dependency>\n\n  </dependencies>\n\n  <build>\n    <plugins>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-compiler-plugin</artifactId>\n        <version>3.8.1</version>\n        <configuration>\n          <source>17</source>\n          <target>17</target>\n        </configuration>\n      </plugin>\n    </plugins>\n  </build>\n</project>\n```\n\n1. pom.xml에 **mysql-connector-j**와 mybatis 의존성을 추가합니다.\n2. Maven이 로컬 저장소(~/.m2/repository)에서 해당 라이브러리를 먼저 찾습니다.\n3. 로컬에 없으면 Maven 중앙 저장소에서 다운로드합니다.\n4. 다운로드가 완료되면 External Libraries에 추가되어 프로젝트에서 바로 사용할 수 있습니다.\n   ![External Libraries에](./external-lib.png)\n\n# MyBatis로 데이터베이스에 접근하기\n\nMyBatis는 SQL을 직접 작성하고 Mapper 인터페이스를 통해 호출하는 방식으로, SQL문을 거의 그대로 작성하는 방식입니다.\nMapper 인터페이스를 작성하고, xml에 매핑 정보를 작성하면, MyBatis가 인터페이스 구현체를 생성하여 sql문을 연결시켜줍니다.\n\n## Mapper 인터페이스\n\n```java\npublic interface UserMapper {\n    User selectById(Long id);\n    void insertUser(User user);\n}\n```\n\n## XML 매핑\n\n```xml\n<mapper namespace=\"com.example.UserMapper\">\n    <select id=\"selectById\" parameterType=\"long\" resultType=\"User\">\n        SELECT * FROM users WHERE id = #{id}\n    </select>\n\n    <update id=\"update\" parameterType=\"User\">\n        UPDATE users SET name = #{name} WHERE id = #{id}\n    </update>\n    ...\n</mapper>\n```\n\n# JPA로 데이터베이스에 접근하기\n\n> JPA는 객체 상태 변화를 기반으로 SQL을 자동 생성하고 실행하는 ORM(Object-Relational Mapping) 프레임워크입니다.\n>\n> MyBatis와 달리, SQL을 직접 작성하지 않아도 엔티티 객체의 상태를 변경하면 데이터베이스가 자동으로 업데이트됩니다.\n\n```java\nUser user = entityManager.find(User.class, 1L); // SELECT 쿼리 실행\nuser.setName(\"홍길동\");                          // UPDATE 쿼리 자동 생성\n```\n\n이처럼 JPA에서 user 객체를 변경했을 때, SQL이 자동으로 실행되는 이유는 영속성 컨텍스트(Persistence Context) 때문입니다.\n\nMyBatis에서는 개발자가 SQL을 직접 제어했다면, JPA는 객체의 상태 변화를 추적해서 자동으로 SQL을 생성해준다는 차이가 있었습니다.\n\n## 영속성 컨텍스트(Persistence Context)\n\n- 영속성 컨텍스트는 엔티티 객체를 영구 저장하는 환경으로, 일종의 1차 캐시 역할을 수행합니다.\n- 엔티티를 조회하면 해당 객체가 영속성 컨텍스트에 저장되고, 같은 트랜잭션 내에서 동일 데이터를 다시 조회하면 DB 쿼리를 수행하지 않고 캐시에서 객체를 반환합니다.\n\n```java\nUser user1 = entityManager.find(User.class, 1L); // DB 쿼리 실행\nUser user2 = entityManager.find(User.class, 1L); // 위에서 실행한 쿼리와 같으므로 쿼리를 실행하지 않고 context에서 가져옴\nSystem.out.println(user1 == user2); // true: 같은 객체\n```\n\n## 엔티티의 생명주기\n\n> 엔티티는 영속성 컨텍스트와의 관계에 따라 다음 4가지 상태를 가집니다.\n\n![Entity의 생명주기](./persistence-context.png)\n\n1. **비영속(New/Transient)**: 영속성 컨텍스트와 전혀 관계 없는 새로운 상태\n2. **영속(Managed)**: 영속성 컨텍스트에 관리되는 상태\n3. **준영속(Detached)**: 영속성 컨텍스트에 저장되었다가 분리된 상태\n4. **삭제(Removed)**: 삭제된 상태\n\n```java\n// 비영속\nUser user = new User();\nuser.setName(\"홍길동\");\n\n// 영속\nentityManager.persist(user);\n\n// 준영속\nentityManager.detach(user);\n\n// 삭제\nentityManager.remove(user);\n```\n\n## 더티 체킹(Dirty Checking)\n\n> Dirty라고 하는것은 보통 변경사항을 칭합니다. 변경사항을 추적/감지하는 것을 Dirty Checking이라고 합니다.\n>\n> 영속 상태의 엔티티를 수정하면, JPA가 자동으로 변경 감지를 수행하여 트랜잭션 커밋 시점에 UPDATE 쿼리를 실행합니다.\n\n```java\n@Transactional\npublic void updateUser(Long id, String newName) {\n    User user = entityManager.find(User.class, id);\n    user.setName(newName); // 트랜잭션 커밋 시 자동으로 UPDATE 실행\n}\n```\n\nJPA는 엔티티를 영속성 컨텍스트에 저장할 때 **스냅샷**도 함께 저장합니다.\n\n트랜잭션 커밋 시점에 현재 엔티티와 스냅샷을 비교해서, 변경된 부분이 있으면 UPDATE 쿼리를 자동 생성하는 것입니다.\n\n![더티 체킹 동작 과정](./dirty-checking.png)\n\nMyBatis에서는 UPDATE 쿼리를 직접 작성하고 호출해야 했는데, JPA는 객체만 수정하면 알아서 처리해주니 정말 편리했습니다.\n\n하지만 영속성 컨텍스트에 연결되어있는 객체(영속 상태), 연결되지 않은 객체(비영속/준영속 상태)를 어떻게 구별할지나, 일부 객체만 수정하고싶을 때는 어떻게 관리할 수 있는지 더 알아볼 필요가 있을 것 같습니다.\n\n## 쓰기 지연(Transactional Write-behind)\n\n> persist()를 호출해도 즉시 INSERT 쿼리를 실행하지 않고, 트랜잭션 커밋 시점에 한 번에 실행합니다.\n>\n> 이를 통해 DB와의 통신 횟수를 줄이고 성능을 개선할 수 있습니다.\n\n```java\nUser user1 = new User(\"홍길동\");\nUser user2 = new User(\"김철수\");\nUser user3 = new User(\"이영희\");\n\nentityManager.persist(user1);\nentityManager.persist(user2);\nentityManager.persist(user3);\n\n    // 커밋 시점에 INSERT 쿼리 실행 (홍길동, 김철수, 이영희)\n```\n\n- 필요 시 flush()를 호출하여 중간에 강제로 쿼리를 실행할 수도 있습니다.\n  - 지금까지 영속성 컨텍스트에 쌓여 있는 변경 사항에 대한 쿼리들이 전부 실행됩니다.\n\n## 2차 캐시 - 애플리케이션 전역 캐시\n\n- 영속성 컨텍스트는 1차 캐시로 트랜잭션 단위로 동작합니다.\n- 트랜잭션이 종료되면 캐시도 사라지므로, 다른 트랜잭션에서는 동일 데이터를 조회할 경우 다시 DB에 쿼리가 발생합니다.\n\n```java\n// 트랜잭션 1\nUser user1 = entityManager.find(User.class, 1L); // DB 쿼리\n\n// 트랜잭션 2\nUser user2 = entityManager.find(User.class, 1L); // 또 DB 쿼리\n```\n\n다른 트랜잭션에서는 캐시를 공유하지 않기 때문에, 같은 데이터를 조회해도 매번 DB에 쿼리가 날아갑니다.\n\n애플리케이션 전체에서 공유하는 캐시가 필요하다면 2차 캐시(Second Level Cache)를 사용할 수 있습니다.\n\n다음과 같은 경우 2차 캐시를 사용하는 것이 적합합니다.\n\n- 조회 성능 최적화가 필요한 경우\n- 조회가 빈번하고, 데이터 변경은 드문 경우\n- 여러 트랜잭션에서 동일 데이터를 반복 조회할 때\n\n![캐시](./cache.png)\n\n# 이번 주 TIL 요약\n\n1. **Maven을 통한 프로젝트 관리**\n   - Maven으로 의존성을 관리하면서 라이브러리 추가가 얼마나 간단해졌는지 체감했습니다.\n\n2. **MyBatis와 JPA**\n   - MyBatis를 사용하며 SQL을 직접 작성하고 데이터베이스와 상호작용하는 방식을 먼저 경험했습니다.\n     - 장점: SQL을 직접 제어 가능, 복잡한 쿼리나 성능 튜닝 시 유리\n     - 단점: CRUD마다 SQL을 작성해야 하는 번거로움\n   - 이후 JPA로 전환하면서 객체 중심 개발 방식과 영속성 컨텍스트를 경험했습니다.\n     - 장점: 객체 상태 변경만으로 SQL이 자동 생성되어 생산성이 높음\n     - 단점: 영속성에 대한 이해가 없을 경우, 객체 변경 시 원치 않는 SQL이 실행될 수 있으므로 주의 필요\n\n3. **JPA 영속성 컨텍스트와 더티 체킹**\n   - 영속성 컨텍스트는 엔티티 객체를 관리하는 1차 캐시 역할을 수행하며, 동일 트랜잭션 내에서 DB 접근 없이 객체를 재사용할 수 있음\n   - 더티 체킹을 통해 영속 상태 엔티티를 수정하면, 트랜잭션 커밋 시점에 자동으로 UPDATE 쿼리가 실행됨\n   - MyBatis에서 직접 SQL을 작성하던 것과 달리, 개발자가 비즈니스 로직에 더 집중할 수 있어 편리함\n   - 내부 동작 원리를 이해하면 예기치 못한 쿼리 발생을 예방하고, 안정적으로 활용 가능\n","frontmatter":{"date":"2025년 10월 21일","slug":"til-20251021","title":"JAVA - Maven, Mybatis, JPA","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","database","java"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAABOElEQVR42qWTTUvDQBCG+9P1IN6EouJBQaUgCFWLYqGCpSqKChGrgl/YKpU2aWqySTYmGzf7uinpIZc0SeewA8Pw7DtfJWQxISB8NvbTrISMxu7uIYJgRmCsyDu7AK0eIHTdRDwfMAzHLvjowJhfBD2qx3FRVKHsW/RSivDmGv52BVwq40WAk4reNIr9xyGeVQftl2+oxMPhg5YKnTqUWlvFcquLHWUAzWboEz+fwsnfjs+hfBHcdk2sSODxkyaBPgyXJfIyK4zKUnoWzj9tNDsWGq8/OHk30bODnMC4gYIQ/F1eges6WF8FMy3wgQqYRs6hxMBwNIK1sQWyVIY5twC7vApnswIifSEgVzXYa+ugu1V4py24ezW4chd/642CCi1Z4lBP7lG07ELMfnrTzi0bUKSMMgX4D8o/kW0u+lU+AAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/5f2dd57d3835641ebe038dbd18c6d76c/27877/java-thumb.png","srcSet":"/static/5f2dd57d3835641ebe038dbd18c6d76c/20f42/java-thumb.png 172w,\n/static/5f2dd57d3835641ebe038dbd18c6d76c/01d54/java-thumb.png 344w,\n/static/5f2dd57d3835641ebe038dbd18c6d76c/27877/java-thumb.png 688w","sizes":"(min-width: 688px) 688px, 100vw"},"sources":[{"srcSet":"/static/5f2dd57d3835641ebe038dbd18c6d76c/6a027/java-thumb.webp 172w,\n/static/5f2dd57d3835641ebe038dbd18c6d76c/f18d0/java-thumb.webp 344w,\n/static/5f2dd57d3835641ebe038dbd18c6d76c/42203/java-thumb.webp 688w","type":"image/webp","sizes":"(min-width: 688px) 688px, 100vw"}]},"width":800,"height":600}}}}},{"id":"d0a60c0e-6e3c-59ba-b14d-7bd8a16ce893","excerpt":"📌 인덱스란? 인덱스는 데이터베이스에서 데이터 검색 속도를 향상시키기 위한 자료구조입니다. 책의 뒷부분에 있는 색인처럼, 특정 데이터를 빠르게 찾을 수 있도록 도와주는 역할을 합니다.\n\n인덱스가 없으면 데이터베이스는 원하는 데이터를 찾기 위해 테이블의…","body":"\n## 📌 인덱스란?\n\n인덱스는 데이터베이스에서 데이터 검색 속도를 향상시키기 위한 자료구조입니다. 책의 뒷부분에 있는 색인처럼, 특정 데이터를 빠르게 찾을 수 있도록 도와주는 역할을 합니다.\n\n인덱스가 없으면 데이터베이스는 원하는 데이터를 찾기 위해 테이블의 모든 행을 처음부터 끝까지 검색(Full Table Scan)해야 하지만 인덱스가 있으면 정렬된 구조를 통해 훨씬 빠르게 데이터를 찾을 수 있습니다.\n\n## 🏗️ 인덱스의 내부 구조\n\n### B-Tree 트리\n\n![B-Tree](./b-tree.png)\n\n대부분의 데이터베이스는 **B-Tree(Balanced Tree)** 구조를 사용합니다.\n\nB-Tree는 균형 잡힌 트리 구조로, 다음과 같은 특징이 있습니다.\n\n> 1. 모든 노드들은 최대 m개의 자식을 가짐\n> 2. 리프 노드를 제외하고, k개의 자식을 가진 노드는 k-1개의 키를 가짐\n> 3. 모든 리프 노드를 제외하고, 최소 m//2 + 1개의 자식을 가져야 함(최소 2개 이상)\n> 4. 모든 리프 노드는 같은 레벨에 있어야 함\n\n- **루트 노드**: 트리의 최상단\n- **브랜치 노드**: 중간 단계 노드들\n- **리프 노드**: 실제 데이터의 위치(포인터)를 가진 최하단 노드\n\n트리 생성 시 최악의 경우 트리가 한쪽으로 치우쳐 O(n) 시간이 걸리는 탐색 구조를 갖게 되는데,\n\nB-Tree는 모든 리프 노드가 동일한 깊이에 있도록 균형을 유지하기 때문에,\n아래와 같이 선형으로 트리가 생성되는 것을 방지합니다.\n![Worse case of Tree algorithm - O(n)](./worse-case-tree.png)\n\n### 인덱스 생성 예시\n\n다음과 같은 users 테이블이 있다고 가정합니다.\n\n![예시 테이블 users](./users-table.png)\n\n인덱스를 생성하면, 원본 테이블과는 별개의 공간에 정렬된 트리 구조로 데이터를 저장합니다. 값으로 정렬되며, 값과 키(id 또는 행 위치)가 저장되어있어 검색에 용이합니다.\n\nusers 테이블에 대해 email 컬럼으로 인덱스를 생성하면 다음과 같은 트리구조가 생성됩니다.\n\n![예시 테이블 users의 email 인덱스 트리](./users-email-index.png)\n\n인덱스 트리를 저장함으로써 데이터 검색 시간이 O(log n)으로 줄어들어, 데이터가 많아져도 검색 속도가 크게 느려지지 않습니다.\n\n하지만 별도의 저장공간이 추가로 필요하며, INSERT, UPDATE, DELETE 작업 시 인덱스 트리를 재정렬 해야하기 때문에 CUD 작업 속도가 느려질 수 있습니다.\n\n읽기가 많은 테이블에서, 자주 조회하는 컬럼만 인덱스를 생성하는 것이 좋습니다.\n\n## 🔍 인덱스 활용\n\n### 1. WHERE 절을 사용한 검색\n\n```\nSELECT * FROM users WHERE email = 'kim@test.com';\n```\n\n다음과 같이 Select 했을 때, `users` 테이블에 대한 인덱스가 없으면 첫번째 행 부터 끝까지 탐색하여 O(n)의 시간이 걸립니다.\n\n하지만 `users.email`에 대한 인덱스가 있는 경우 정렬된 트리에서 값을 찾으므로 검색 시간을 O(log n)으로 줄일 수 있습니다.\n\n### 2. JOIN 연산\n\n```\nSELECT u.name, o.product_name, o.price\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.email = 'kim@test.com';\n```\n\nNested Loop Join 방식으로 동작한다고 할 때, 인덱스가 없다면 `users` 테이블의 모든 행 전체 탐색, `orders` 테이블의 모든 행 전체 탐색으로 O(n\\*m)의 시간이 걸립니다.\n\n`users.email`, `orders.user_id`에 인덱스가 있는 경우, 검색 시간을 O(log n + log m)로 줄일 수 있습니다.\n\n### ORDER BY 정렬\n\n이미 정렬되어 저장되어있으므로, 결과 반환만 하면 됩니다. 별도의 정렬을 실행할 필요가 없습니다.\n\n### GROUP BY 그룹화\n\n이미 정렬되어 저장되어있으며, 같은 값에 연속으로 접근할 수 있으므로, 순차적으로 읽으면서 연산 작업을 하면 됩니다.\n\n추가 그룹핑 작업이 필요하지 않습니다.\n\n## 💡 인덱스 사용이 불가능한 경우\n\n인덱스는 특정 컬럼값에 따라 정렬되어 있기 때문에 예상이 불가능한 값으로 조회하려고 할 때는 사용할 수 없습니다.\n\n### ❌ 컬럼에 함수를 사용할 때\n\n```sql\n-- ❌ 함수 연산 결과물은 인덱스로 접근 불가\nSELECT * FROM users WHERE YEAR(created_at) = 2024;\n\n-- ✅\nSELECT * FROM users WHERE created_at >= '2024-01-01' AND created_at < '2025-01-01';\n```\n\n### ❌ LIKE 앞에 % 사용할 때\n\n```sql\n-- ❌ 문자열 첫번째 요소를 알 수 없는 경우 인덱스로 접근 불가\nSELECT * FROM users WHERE name LIKE '%김%';\n\n-- ✅\nSELECT * FROM users WHERE name LIKE '김%';\n```\n\n### 카디널리티(Cardinality) 고려\n\n> 카디널리티는 특정 컬럼의 중복되지 않는 값의 개수를 의미합니다.\n>\n> **인덱스는 카디널리티가 높을수록 효과적**입니다.\n\n- **높은 카디널리티**: 이메일, 주민등록번호, 전화번호\n- **낮은 카디널리티**: 성별(남/여), 나이대(10대/20대/30대), 참/거짓\n\n## 📝 인덱스 생성 방법\n\n### 단일 컬럼 인덱스\n\n```sql\n-- 인덱스 생성\nCREATE INDEX idx_email ON users(email);\n\n-- 인덱스 삭제\nDROP INDEX idx_email ON users;\n```\n\n### 복합 인덱스 (여러 컬럼)\n\n```sql\n-- 이름과 나이를 함께 인덱싱, 정렬 우선순위는 왼쪽 컬럼 우선\nCREATE INDEX idx_name_age ON users(name, age);\n```\n\n### 자동 생성되는 인덱스\n\n- **PRIMARY KEY**: 테이블 생성 시 자동으로 인덱스 생성\n- **UNIQUE 제약조건**: 고유값을 보장하기 위해 자동으로 인덱스 생성\n\n```sql\nCREATE TABLE users (\n    id INT PRIMARY KEY,           -- 자동으로 인덱스 생성\n    email VARCHAR(100) UNIQUE     -- 자동으로 인덱스 생성\n);\n```\n\n---\n\n## 🧪 성능 테스트\n\n![mysql](./mysql.png)\n인덱스 검색 성능을 MySQL을 통해 실행해보았습니다.\n\n### 1. 테스트 데이터 준비\n\n```sql\n-- 테스트용 테이블 생성\nCREATE TABLE test_users (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(50),\n    email VARCHAR(100),\n    age INT,\n    created_at DATETIME\n);\n\n-- 대용량 데이터 삽입 (100만 건)\nDELIMITER $$\nCREATE PROCEDURE insert_test_data()\nBEGIN\n    DECLARE i INT DEFAULT 1;\n    WHILE i <= 1000000 DO\n        INSERT INTO test_users (name, email, age, created_at)\n        VALUES (\n            CONCAT('User', i),\n            CONCAT('user', i, '@test.com'),\n            FLOOR(20 + RAND() * 50),\n            NOW() - INTERVAL FLOOR(RAND() * 365) DAY\n        );\n        SET i = i + 1;\n    END WHILE;\nEND$$\nDELIMITER ;\n\n-- 프로시저 실행\nCALL insert_test_data();\n```\n\n`test_users` 테이블을 생성하고, 100만건의 데이터가 생성되었습니다. 데이터 생성에 시간이 몇 분 정도 소요됩니다.\n\n![test_users 데이터 count](./create.png)\n\n### 2. 인덱스 없이 검색 테스트\n\n```sql\n-- 실행 계획 확인\nEXPLAIN SELECT * FROM test_users WHERE email = 'user500000@test.com';\n\n-- 실행 시간 측정\nEXPLAIN ANALYZE\nSELECT * FROM test_users WHERE email = 'user500000@test.com';\n```\n\n`EXPLAIN`으로 SELECT문 실행에 대한 정보를 확인하면 아래와 같은 결과가 나옵니다.\n\n- type: ALL - 풀 테이블 스캔 (처음부터 끝까지 탐색)\n- key: NULL - 인덱스를 사용하지 않음\n- rows: 995,849 - 99만 건을 확인해야 할 것으로 추정됨\n- Extra: Using where - WHERE 조건으로 필터링\n\n`EXPLAIN ANALYZE`로 SELECT문 실행 결과를 확인하면 아래와 같은 결과가 나옵니다.\nFilter 작업의 하위에 Table scan이 포함되어있기 때문에 최종 실행 시간은 Filter를 참고하면 됩니다.\n\n```\n-> Filter: (test_users.email = 'user500000@test.com')  (cost=100554 rows=99585) (actual time=215..343 rows=1 loops=1)\n    -> Table scan on test_users  (cost=100554 rows=995849) (actual time=1.27..286 rows=1e+6 loops=1)\n```\n\n- Filter: 각 행을 검사할 때의 조건\n  - cost=100554 rows=99585: MySQL의 쿼리 검사 예측으로, 99585개의 행이 나올것으로 예측함\n  - actual time=215..343: 필터링하는 데 걸린 시간 215~343ms, 전체 실행 시간은 343ms\n  - rows=1: 실제 결과 1건\n  - loops=1: 필터링 1회 수행\n\n- Table scan on test_users: `test_users` 테이블의 전체 행을 처음부터 끝까지 읽는 작업이 발생함\n  - cost=100554 rows=995849: 옵티마이저가 약 99만 행을 읽을 것으로 예측함\n  - actual time=1.27..286: SELECT에 걸린 시간 1.27ms ~ 286ms\n  - rows=1e+6: 실제로 읽은 행 수 - 약 100만 건\n  - loops=1: 테이블 전체를 1회 스캔\n\n### 3. 인덱스 생성 후 테스트\n\n```sql\n-- 인덱스 생성\nCREATE INDEX idx_email ON test_users(email);\n\n-- 실행 계획 확인\nEXPLAIN SELECT * FROM test_users WHERE email = 'user500000@test.com';\n\n-- 실행 시간 측정\nEXPLAIN ANALYZE\nSELECT * FROM test_users WHERE email = 'user500000@test.com';\n```\n\n인덱스를 생성하면 100만건의 데이터가 email 값을 기준으로 B-Tree로 저장됩니다. 인덱스 생성에 3s 정도 소요되었습니다.\n![인덱스 생성](./create-index.png)\n\n`EXPLAIN`으로 SELECT문 실행에 대한 정보를 확인하면 아래와 같은 결과를 확인할 수 있습니다.\n\n- type: ref - 인덱스를 이용해 조건에 맞는 행만 탐색\n- key: idx_email - email 컬럼으로 생성된 인덱스 이름\n- rows: 1 - 1건을 확인해야 할 것으로 추정됨\n- Extra: Using index - 테이블 데이터를 직접 읽지 않고 인덱스만으로 조회 가능\n\n`EXPLAIN ANALYZE`로 SELECT문 실행 결과를 확인하면 아래와 같은 결과가 나옵니다.\n\n```\n-> Index lookup on test_users using idx_email (email='user500000@test.com')  (cost=0.35 rows=1) (actual time=4.38..4.39 rows=1 loops=1)\n```\n\n- Index lookup on test_users: `idx_email` 인덱스를 사용하여 조건에 맞는 행을 바로 찾음\n  - cost=0.35 rows=1: 옵티마이저가 약 1건의 결과를 예측함\n  - actual time=4.38..4.39: 실제 조회에 걸린 시간 — 4.38ms ~ 4.39ms\n  - rows=1: 실제 결과 1건\n\n### 4. 쿼리 실행 시간 비교\n\n> 인덱스 사용으로 실행 시간이 343ms -> 4ms로 86배 빨라졌습니다.\n\n---\n\n## 🎯 요약\n\n다음과 같은 기준으로 인덱스를 고려하면 좋습니다.\n\n- WHERE 절에 자주 사용되는 컬럼\n- JOIN에 사용되는 컬럼\n- ORDER BY에 자주 사용되는 컬럼\n- 카디널리티가 높은 컬럼\n\n### 인덱스 사용 시 장점\n\n- 검색 속도가 대폭 향상\n- ORDER BY, GROUP BY 작업의 성능 개선\n- MIN, MAX 같은 집계 함수가 빨라짐\n\n### 인덱스 사용 시 단점\n\n- 인덱스를 저장하기 위한 추가 디스크 공간이 필요 (테이블 크기의 약 10%)\n- INSERT, UPDATE, DELETE 작업 시 인덱스도 함께 수정해야 하므로 성능이 저하됨\n- 너무 많은 인덱스는 오히려 성능을 떨어뜨릴 수 있으므로 조회 최적화에 사용\n","frontmatter":{"date":"2025년 10월 14일","slug":"til-20251014","title":"데이터베이스 인덱스(Index) 정리 - MySQL 인덱스 실습","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","database","sql"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAABuUlEQVR42q2Tyy9DQRTG+6f5I1iKjY3EWmJjYSEhESuJhIVXamMhlEgboR5NhFKl1Vb04dH29nV7X+5r5n7mDqKvoJjkZM59zG++75wZD/55eLr52XGc/wX+BNwV8E4QUZS0L6GeblRtR7MsMggmn2HY5PfADzXpssQjU5Hhjz+8ffsLMF9XEctXeX6QfEKuKvOcNtjnQIsQEOrwhSZptuJQm4erpSorWAndQFRVCJKKwLvKjgopA7rhDptSPhM2t1raSwm4q6g8jzyWeU3bgHPBa8QLNW5pOnDZBDMyPmixBZj5EPTEKkhyHi/sHWpRPNZ1LB5fN1t2H3pnfZjaDWPhKIaeyXUshuK8i+sX9yidjMHK+vCS8EI5nwaxNNS2+qDdLHGAbtmsTA1ASTcxw1QNeQ9Y7GNi5wzDa0GEcwJG1g4hFSLQr2Yg+vshh0ahnI6jHhiAGp6EVWHqlBwrr/0JtAnFs6giURS57bKiYyNyj8HlPb6JJZzDfPCDaEVYpTC06Cx0ZtkqXcJIb8Ku3cIhRntT3A5/HI8Cuw2pkgjZMDueNXx39dw6fkbrYvpmyYXwnLTktGnLV+bWfXILtj+yAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/ebb63e01e087a1bb0cf51c8031eec64b/ce24c/mysql.png","srcSet":"/static/ebb63e01e087a1bb0cf51c8031eec64b/aacf3/mysql.png 200w,\n/static/ebb63e01e087a1bb0cf51c8031eec64b/0e3a3/mysql.png 400w,\n/static/ebb63e01e087a1bb0cf51c8031eec64b/ce24c/mysql.png 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/ebb63e01e087a1bb0cf51c8031eec64b/abf68/mysql.webp 200w,\n/static/ebb63e01e087a1bb0cf51c8031eec64b/ce081/mysql.webp 400w,\n/static/ebb63e01e087a1bb0cf51c8031eec64b/c11e0/mysql.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":600}}}}},{"id":"28de0fd8-75e3-5afd-8009-4f1352b4fa84","excerpt":"MySQL을 사용하면서 자주 찾아보게 되는 명령어들과 핵심 개념들을 정리해봤다. 권한 관리\n데이터베이스 생성 및 조회\n테이블 생성 및 관리\n테이블 구조 변경 (ALTER)\n데이터 삽입 (INSERT)\n데이터 조회 (SELECT)…","body":"\n# TIL 챌린지 주제 선정\n\n> MySQL을 사용하면서 자주 찾아보게 되는 명령어들과 핵심 개념들을 정리해봤다.\n\n![MySQL](./mysql.png)\n\n# 데이터베이스 기본 조작\n\n## 권한 관리\n\n```sql\n-- 사용자 생성\nCREATE USER 'newuser'@'localhost' IDENTIFIED BY 'password';\nCREATE USER 'newuser'@'%' IDENTIFIED BY 'password';  -- 모든 호스트 허용\n\n-- 권한 부여\nGRANT ALL PRIVILEGES ON mydb.* TO 'newuser'@'localhost';\nGRANT SELECT, INSERT, UPDATE ON mydb.users TO 'newuser'@'localhost';\nGRANT SELECT ON mydb.* TO 'readonly'@'localhost';\n\n-- 권한 확인\nSHOW GRANTS FOR 'newuser'@'localhost';\n\n-- 권한 적용\nFLUSH PRIVILEGES;\n\n-- 권한 제거\nREVOKE ALL PRIVILEGES ON mydb.* FROM 'newuser'@'localhost';\n\n-- 사용자 삭제\nDROP USER 'newuser'@'localhost';\n\n-- 비밀번호 변경\nALTER USER 'newuser'@'localhost' IDENTIFIED BY 'newpassword';\n```\n\n## 데이터베이스 생성 및 조회\n\n```sql\n-- 데이터베이스 목록 확인\nSHOW DATABASES;\n\n-- 데이터베이스 생성\nCREATE DATABASE mydb;\nCREATE DATABASE IF NOT EXISTS mydb CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n\n-- 데이터베이스 선택\nUSE mydb;\n\n-- 데이터베이스 삭제\nDROP DATABASE mydb;\n```\n\n## 테이블 생성 및 관리\n\n```sql\n-- 테이블 생성\nCREATE TABLE users (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    username VARCHAR(50) NOT NULL UNIQUE,\n    email VARCHAR(100) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    INDEX idx_email (email)\n);\n\n-- 테이블 목록 확인\nSHOW TABLES;\n\n-- 테이블 구조 확인\nDESCRIBE users;\nSHOW CREATE TABLE users;\n\n-- 테이블 삭제\nDROP TABLE users;\nTRUNCATE TABLE users;  -- 데이터만 삭제, 구조는 유지\n```\n\n## 테이블 구조 변경 (ALTER)\n\n```sql\n-- 컬럼 추가\nALTER TABLE users ADD COLUMN age INT;\nALTER TABLE users ADD COLUMN phone VARCHAR(20) AFTER email;\n\n-- 컬럼 수정\nALTER TABLE users MODIFY COLUMN username VARCHAR(100);\nALTER TABLE users CHANGE old_name new_name VARCHAR(50);\n\n-- 컬럼 삭제\nALTER TABLE users DROP COLUMN age;\n\n-- 인덱스 추가/삭제\nALTER TABLE users ADD INDEX idx_username (username);\nALTER TABLE users DROP INDEX idx_username;\n\n-- PRIMARY KEY 추가\nALTER TABLE users ADD PRIMARY KEY (id);\n\n-- FOREIGN KEY 추가\nALTER TABLE orders ADD CONSTRAINT fk_user\n    FOREIGN KEY (user_id) REFERENCES users(id);\n```\n\n---\n\n# CRUD 기본 쿼리\n\n## 데이터 삽입 (INSERT)\n\n```sql\n-- 단일 행 삽입\nINSERT INTO users (username, email) VALUES ('john', 'john@example.com');\n\n-- 여러 행 한번에 삽입\nINSERT INTO users (username, email) VALUES\n    ('jane', 'jane@example.com'),\n    ('bob', 'bob@example.com');\n\n-- 중복 시 무시\nINSERT IGNORE INTO users (username, email) VALUES ('john', 'john@example.com');\n\n-- 중복 시 업데이트\nINSERT INTO users (username, email) VALUES ('john', 'john@example.com')\nON DUPLICATE KEY UPDATE email = VALUES(email);\n```\n\n## 데이터 조회 (SELECT)\n\n```sql\n-- 전체 조회\nSELECT * FROM users;\n\n-- 특정 컬럼만 조회\nSELECT username, email FROM users;\n\n-- 조건부 조회 (WHERE)\nSELECT * FROM users WHERE id = 1;\nSELECT * FROM users WHERE username LIKE 'j%';\nSELECT * FROM users WHERE created_at > '2024-01-01';\n\n-- 정렬 (ORDER BY)\nSELECT * FROM users ORDER BY created_at DESC;\nSELECT * FROM users ORDER BY username ASC, created_at DESC;\n\n-- 상위 n개 컬럼 조회 (LIMIT n)\nSELECT * FROM users LIMIT 10;\n\n-- 중복 제거\nSELECT DISTINCT username FROM users;\n```\n\n## 데이터 수정 (UPDATE)\n\n```sql\nUPDATE users SET email = 'newemail@example.com' WHERE id = 1;\nUPDATE users SET username = 'newname', email = 'new@example.com' WHERE id = 1;\n\nUPDATE users SET email = CONCAT(username, '@newdomain.com')\nWHERE email LIKE '%@olddomain.com';\n```\n\n### 데이터 삭제 (DELETE)\n\n```sql\nDELETE FROM users WHERE id = 1;\nDELETE FROM users WHERE created_at < '2023-01-01';\n```\n\n---\n\n# 조인 (JOIN)\n\n다음과 같이 `users`, `orders` 두 테이블이 있을 때\n![users](./user.png)\n![orders](./order.png)\n\n## INNER JOIN\n\n양쪽 테이블에 모두 존재하는 데이터만 가져온다.\n\n```sql\nSELECT u.username, o.order_id, o.amount\nFROM users u\nINNER JOIN orders o ON u.id = o.user_id;\n```\n\n![inner join](./innerjoin.png)\n\n## LEFT JOIN\n\n왼쪽 테이블의 모든 데이터를 포함하고, 오른쪽 테이블에 매칭되는 데이터가 있으면 가져온다.\n\n```sql\n-- 주문이 없는 사용자도 포함\nSELECT u.username, o.order_id\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id;\n```\n\n![left join](./leftjoin.png)\n\n---\n\n# 집계 함수 (Aggregation)\n\n## 기본 집계 함수\n\n```sql\n-- 개수 세기\nSELECT COUNT(*) FROM users;\nSELECT COUNT(DISTINCT email) FROM users;\n\n-- 합계, 평균, 최대, 최소\nSELECT SUM(amount) FROM orders;\nSELECT AVG(amount) FROM orders;\nSELECT MAX(amount) FROM orders;\nSELECT MIN(amount) FROM orders;\n```\n\n## GROUP BY\n\n```sql\nSELECT user_id, COUNT(*) as order_count, SUM(amount) as total_amount\nFROM orders\nGROUP BY user_id;\n```\n\n## HAVING (그룹화 후 조건)\n\n```sql\nSELECT user_id, COUNT(*) as order_count\nFROM orders\nGROUP BY user_id\nHAVING COUNT(*) > 5;\n```\n\n---\n\n# 그 외\n\n## WITH 절 (CTE)\n\n- WITH 절의 **CTE(Common Table Expression)** 는 임시 결과 집합을 만든다.\n\n### 기본 사용법\n\n```sql\nWITH\nuser_stats AS (\n    SELECT user_id, COUNT(*) as order_count\n    FROM orders\n    GROUP BY user_id\n),\nrevenue_stats AS (\n    SELECT user_id, SUM(amount) as total_revenue\n    FROM orders\n    GROUP BY user_id\n)\n\n-- 테이블처럼 SELECT에 사용할 수 있다.\nSELECT\n    u.username,\n    us.order_count,\n    rs.total_revenue\nFROM users u\nLEFT JOIN user_stats us ON u.id = us.user_id\nLEFT JOIN revenue_stats rs ON u.id = rs.user_id;\n```\n\n## CROSS JOIN\n\n- **CROSS JOIN**은 두 테이블의 모든 가능한 조합 - 카티션 프로덕트를 만든다.\n\n### 기본 사용법\n\n```sql\n-- 모든 색상과 모든 크기의 조합\nSELECT c.color, s.size\nFROM colors c\nCROSS JOIN sizes s;\n\n-- 카티션 프로덕트와 같다.\nSELECT c.color, s.size\nFROM colors c, sizes s;\n```\n\n![colors, sizes 테이블](./colors-sizes.png)\n\n![cross join](./crossjoin.png)\n\n# mysqldump 백업 및 복원\n\n```cmd\n-- 단일 데이터베이스\nmysqldump -u root -p mydb > backup.sql\n\n-- 여러 데이터베이스\nmysqldump -u root -p --databases db1 db2 > backup.sql\n\n-- 모든 데이터베이스\nmysqldump -u root -p --all-databases > backup.sql\n\n-- 특정 테이블만\nmysqldump -u root -p mydb users orders > backup.sql\n\n-- sql 파일 적용\nmysql -u root -p mydb < backup.sql\n```\n","frontmatter":{"date":"2025년 9월 30일","slug":"til-20250930","title":"MySQL 정리","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","database","sql"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAABuUlEQVR42q2Tyy9DQRTG+6f5I1iKjY3EWmJjYSEhESuJhIVXamMhlEgboR5NhFKl1Vb04dH29nV7X+5r5n7mDqKvoJjkZM59zG++75wZD/55eLr52XGc/wX+BNwV8E4QUZS0L6GeblRtR7MsMggmn2HY5PfADzXpssQjU5Hhjz+8ffsLMF9XEctXeX6QfEKuKvOcNtjnQIsQEOrwhSZptuJQm4erpSorWAndQFRVCJKKwLvKjgopA7rhDptSPhM2t1raSwm4q6g8jzyWeU3bgHPBa8QLNW5pOnDZBDMyPmixBZj5EPTEKkhyHi/sHWpRPNZ1LB5fN1t2H3pnfZjaDWPhKIaeyXUshuK8i+sX9yidjMHK+vCS8EI5nwaxNNS2+qDdLHGAbtmsTA1ASTcxw1QNeQ9Y7GNi5wzDa0GEcwJG1g4hFSLQr2Yg+vshh0ahnI6jHhiAGp6EVWHqlBwrr/0JtAnFs6giURS57bKiYyNyj8HlPb6JJZzDfPCDaEVYpTC06Cx0ZtkqXcJIb8Ku3cIhRntT3A5/HI8Cuw2pkgjZMDueNXx39dw6fkbrYvpmyYXwnLTktGnLV+bWfXILtj+yAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/ebb63e01e087a1bb0cf51c8031eec64b/ce24c/mysql.png","srcSet":"/static/ebb63e01e087a1bb0cf51c8031eec64b/aacf3/mysql.png 200w,\n/static/ebb63e01e087a1bb0cf51c8031eec64b/0e3a3/mysql.png 400w,\n/static/ebb63e01e087a1bb0cf51c8031eec64b/ce24c/mysql.png 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/ebb63e01e087a1bb0cf51c8031eec64b/abf68/mysql.webp 200w,\n/static/ebb63e01e087a1bb0cf51c8031eec64b/ce081/mysql.webp 400w,\n/static/ebb63e01e087a1bb0cf51c8031eec64b/c11e0/mysql.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":600}}}}},{"id":"8dd27941-3448-5d55-bafa-06d4459b6f45","excerpt":"스터디 팀원으로 참여해 본 적은 많지만 주도적으로 운영해 본 적은 없던 것 같다. 이번에 멀티캠퍼스에서 스터디 이벤트를 열었는데, 대면 스터디가 대부분이었다. 비대면 학습을 진행하다보니 같은 반 수강생들과 교류할 기회가 부족해서 아쉬운 마음에…","body":"\n![스터디1](./001.png)\n\n# TIL 챌린지 주제 선정\n\n![스터디 제안](./message.png)\n\n> 스터디 팀원으로 참여해 본 적은 많지만 주도적으로 운영해 본 적은 없던 것 같다.\n>\n> 이번에 멀티캠퍼스에서 스터디 이벤트를 열었는데, 대면 스터디가 대부분이었다. 비대면 학습을 진행하다보니 같은 반 수강생들과 교류할 기회가 부족해서 아쉬운 마음에 비대면 스터디를 열어보았다.\n>\n> 사실 운영해야 하는 것에 부담을 느끼고 있었는데, 고민 끝에 친분이 좀 생긴 비대면반 모임에 제안을 해보았다. 전부 참여하고 싶다고 해 주셨다! (정말 기뻤습니다🥹)\n>\n> 경기도민으로써 체력을 아낄 수 있도록 비대면으로 잘 진행 해 보려고 한다.\n\n![스터디2](./002.png)\n\n---\n\n# 스터디 운영 기획\n\n## 주제 선정\n\n스터디 주제는 CS(Computer Science) 기본 지식으로 결정했다.\n\n### 스터디 운영 방식\n\n![진행방식](./진행방식.png)\n\n운영 방식에 대해서 고민이 많았는데, 문서를 편집하느라 무의미한 시간을 보내지 않았으면 좋겠다고 생각했다.\n\n그래서 주제를 정하고 중요 키워드 위주로, 부족한 부분만 함께 정리해보는 방식을 제안했다.\n\n- 매주 공통 주제 선정\n- 관련 키워드 조사, 각자 내용 학습: [tech-interview-for-developer](https://github.com/gyoogle/tech-interview-for-developer) 참고\n- 스터디 전날까지 발생한 질문사항 공유\n- 키워드와 질문사항에 대해 스터디 시간에 답변하고, 빠르게 정리\n\n> **질문 확인 및 답변, 정리 -> 다음 주 주제 선정 -> 과제: 키워드 조사 및 학습, 질문 사항 공유**\n\n위와 같은 방식으로 진행하기로 했다.\n\n주제 선정 방식은 매주 다음 주제를 선정하도록 했지만, 틀이 될만한 주제들을 미리 정해두었다.\n\n- 다른 스터디 레포의 목차, CS 도서 목차 등을 참고해서 선정했다.\n  ![스터디 주제](./주제.png)\n\n정리 방식은 두 가지를 제안했는데, Github Repository를 활용하는 것과, 노션의 공동 편집 기능을 사용하는 것이다.\n각각의 장점은 아래와 같다.\n\n**1. GitHub Repository 활용**\n\n- Git/GitHub 사용을 연습할 기회\n- Issue/Pull Request와 같은 기능을 사용해 볼 수 있음\n\n**2. Notion 공동편집**\n\n- 모두에게 익숙한 툴이기 때문에 정리 시간이 적게 소요됨\n- 실시간으로 함께 편집 가능\n\n> 이번 기회에 Git/GitHub를 익히고싶다는 의견이 많아서 Github Repository를 활용하기로 결정했다. 스터디 레파지토리이기 때문에 실수해도 걱정 없다.\n\n## CS 스터디 레파지토리 운영 방식들\n\nGithub에 `CS 스터디`로 검색하여 다른 스터디 레파지토리의 운영 방식을 참고해 보았다.\n\n![cs 스터디 검색 결과](./github.png)\n\n- `README.md`에 스터디 규칙을 작성한다.\n- Issue에 주제를 올리고, Comment로 각자 질문을 달아 둔다.\n- 주제별 디렉토리를 만들고 Markdown 형식으로 정리한다. `[경로]/README.md`에 작성하면 Github 웹 뷰에서 보기 편하다.\n- 같은 주제에서 각자 이름으로 파일을 만들어 내용을 정리하거나, 한 주제를 여러 파트로 나눠서 각자 정리한다.\n\n---\n\n## 기대 효과 및 앞으로의 계획\n\n이번 CS 스터디는 아래와 같은 목표를 가지고 있다.\n\n- 체계적인 CS 지식 습득: 단순 암기가 아닌 이해 중심의 학습\n- 협업 도구 숙련도 향상: Git/GitHub 활용 능력 개발\n- 질문하는 습관 형성: 모르는 것을 부끄러워하지 않고 적극적으로 질문하는 문화 조성\n- 지식 공유와 토론: 각자의 관점에서 바라본 내용을 공유하며 더 깊이 있게 이해\n\n![이번 주 issue 등록](./issue.png)\n\n이번 주 스터디는 목요일에 예정되어 있다. 스터디 Repository에 첫 주 Issue 등록을 완료하고, 키워드 및 질문 작성 가이드를 남겨 두었다.\n\n첫 스터디 시간 전 까지 간략한 스터디 소개와 규칙을 작성하고, 첫 시간에는 `fork`하여 `pull request`하는 법 등에 대해 정리하여 공유하려고 한다.\n\n또, 아직 내용 정리에 대해서는 돌아가며 한 사람이 할 지, 매 주 여러 사람이 할지, 각자 파트를 정리할 지... 갈피를 잡지 못했는데 첫 시간을 진행해 본 뒤 편집 방식을 결정하려고 한다.\n\n나아가서 CS 주제 이외에도, 새로 스터디 하고 싶은 내용이 생긴다면 다른 주제로, 다른 팀원을 모집하여 진행해보고 싶기도 하다.\n","frontmatter":{"date":"2025년 9월 23일","slug":"til-20250923","title":"GitHub Repository를 이용해서 비대면 스터디 운영하기","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","github","study"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAADfElEQVR42lWT+08aWRiG+TP38tMmTbY2KnVVarvVekfdXmzcNk1t2prsaja72datq66XKtRqVBhguAiIFhAQVEAHUGCYefYw1r2cyTvfd3Jmnrzn+84xVbw9bNne4PGHOUvH0KslyMdBOYRMCHLRy/nxLmoxj7I7he7rQN0ZQQ30U/PfE/mgkWu+LkwVuYuTwwAUMvDxFbprGn3hMSyMwvIT9JAdfcYKP5tRnbMosbfo7haqAqjtWNECPdR2BgxpgW5MZbkbJReBoPjRNg6rz8AmtCig75+Ab/YSPmmmtPE7BzszsNsDESu1usPgoIB9VtAqgGLLp4ltqJURCblEhGhAonQUI58IE/VuoiR34SxBWcni+fgr739qxvWHBUICGOgzwKpfRH9PfcvCYcbH1Zibn+fx2BgOpwOb3caDh4/48MFurFVrGkn3FL88vUfDN1+TWusS0D40IfYHRQl6BdDb9x9gjaXFv/j+zm12Al5WVpbo6+vBtrpsrKpqhZRvmpw8w/JkP4rjjgHcX77N4oQFxWOtA+sOvf8AS6UCufwJlcoFxYLCYSrO+XnRWKuJdzYwydrbHzmUJ2BvkIK7F9/Sc54NDbHw+uZll+tATXxcOZCpxd2QlIlH/ITDQVLJGB7HJhdKjkpJIb23ztqGnfsPbqEGh0Rnu0htPmfltwlG7zZ8dpj2Eo3FcG2sEpa3CHs2CQVkXC4Hn0J+ipJTFN1HWeTnF+dIssQXDV8x+qiRsvh/8XUrS1O9DFu+xVR1dwigB108avUCTVfRNFXMaiJW0YXQxWZFjAUl8sdHjE+O03D3Oo3dZqZe9hKa/Y6ce4g/X7YKoKtVXArJqGCpkCd9mCCZiBKL7hl5StQwIeaaOFZnp1nezc3Q1NVES38L5s4brL3pQPNa8M2Yyax31IFthkPjWAiHJVGnYvH/Kojm1Ec09okblusCdhOzcNfQdg3fXDtZxyAR+zDxte5LYHZ/lXQ8QD4ToXSWoiQOeF3F/IGIB2TTYY5iThbevaCpsxnzQAtNA+00d1xjyz5l3DQlG+Q46cSEtx1NFh1zt1Hz3PpX8lVuoepqR5UaKXjvI20vMP7KysOxdro7v8S5PS+8n3Be3KdSPsCUk0bIS8NCl/HUNWLoKj9z/0C+Hl3DJBxPycS3ROfXSUov2Ji+hyxvUMjvk4p7yST9/A1OoffZHxBhuQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/fe074fa7807b608d1550059151ce1add/ce24c/001.png","srcSet":"/static/fe074fa7807b608d1550059151ce1add/aacf3/001.png 200w,\n/static/fe074fa7807b608d1550059151ce1add/0e3a3/001.png 400w,\n/static/fe074fa7807b608d1550059151ce1add/ce24c/001.png 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/fe074fa7807b608d1550059151ce1add/abf68/001.webp 200w,\n/static/fe074fa7807b608d1550059151ce1add/ce081/001.webp 400w,\n/static/fe074fa7807b608d1550059151ce1add/c11e0/001.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":600}}}}},{"id":"c8455c7c-5e31-5b66-8aaf-d7b7fc656b18","excerpt":"지금까지 웹개발을 하면서, 주로 JavaScript, PHP, Go 같은 언어를 사용해왔다. 이런 언어들은 슬라이스(slice)나 딕셔너리(dictionary)처럼 데이터를 다루기 편리한 문법을 기본으로 제공한다. 반면 Java는 배열을 선언하면…","body":"\n![til 챌린지 Java의 Collections와 Stream](./til-start.png)\n\n# TIL 챌린지 주제 선정\n\n> 지금까지 웹개발을 하면서, 주로 JavaScript, PHP, Go 같은 언어를 사용해왔다.\n> 이런 언어들은 슬라이스(slice)나 딕셔너리(dictionary)처럼 데이터를 다루기 편리한 문법을 기본으로 제공한다.\n>\n> 반면 Java는 배열을 선언하면 고정 길이를 가지기 때문에, 슬라이스나 딕셔너리 처럼 사용하기에는 불편함이 있다.\n> 리스트, 맵, 집합 등 다양한 자료구조를 다루려면 `java.util`의 컬렉션 패키지를 사용하는 것이 보편적인 듯 하다.\n>\n> 이번주 TIL 챌린지에서는 Java에서 데이터를 다루는 기본 도구인 컬렉션(Collection)과, 데이터를 선언적이고 함수형 스타일로 처리할 수 있는 스트림(Stream)에 대해 살펴보았다.\n\n![java](./java-thumb.png)\n\n# Java의 Collections\n\n> Java 플랫폼에는 Collections 프레임워크가 포함되어 있다.\n>\n> 컬렉션은 객체 그룹을 나타내는 객체로, 기본적인 자료구조에 대한 인터페이스와 구현체를 제공한다.\n>\n> primitive 타입의 배열이 아닌 경우, Object의 배열, 집합, 큐 등을 사용하게 된다면 Collections를 사용하게 되므로 알아보자.\n\n## Collections의 인터페이스\n\n![Collections의 인터페이스](./collections-interface.png)\n\n### 1. Collection 인터페이스\n\n`Collection` 인터페이스는 모든 컬렉션의 루트 인터페이스로, 기본적인 컬렉션 연산을 정의한다.\n\n**주요 메서드**\n\n- `add(E e)`: 요소 추가\n- `remove(Object o)`: 요소 제거\n- `contains(Object o)`: 요소 포함 여부 확인\n- `size()`: 컬렉션 크기 반환\n- `isEmpty()`: 빈 컬렉션 여부 확인\n- `iterator()`: Iterator 반환\n\n```java\nCollection<String> collection = new ArrayList<>();\ncollection.add(\"Java\");\ncollection.add(\"Python\");\nSystem.out.println(collection.size()); // 2\nSystem.out.println(collection.contains(\"Java\")); // true\n```\n\n### 2. List 인터페이스\n\n`List`는 순서가 있는 컬렉션으로, 중복 요소를 허용하며 인덱스로 요소에 접근할 수 있다.\n\n**특징**\n\n- 순서 보장 (insertion order)\n- 중복 허용\n- 인덱스 기반 접근\n\n**주요 구현체**\n\n- `ArrayList`: 동적 배열, 빠른 조회\n- `LinkedList`: 이중 연결 리스트, 삽입과 삭제가 빠름\n- `Vector`: 동기화된 ArrayList (legacy 클래스로 사용하지 않는 것을 추천)\n\n```java\nList<Integer> numbers = new ArrayList<>();\nnumbers.add(10);\nnumbers.add(20);\nnumbers.add(10); // 중복 허용\n\n// 인덱스 기반 접근\nSystem.out.println(numbers.get(0)); // 10\nnumbers.set(1, 25); // 인덱스 1의 값을 25로 변경\n\n// 특정 위치에 삽입\nnumbers.add(1, 15); // {10, 15, 25, 10}\n```\n\n### 3. Set 인터페이스\n\n`Set`은 중복 요소를 허용하지 않는 컬렉션이다.\n\n**특징:**\n\n- 중복 불허\n- 수학적 집합의 개념\n\n**주요 구현체**\n\n- `HashSet`: 해시 테이블 기반, O(1) 성능\n- `LinkedHashSet`: 삽입 순서를 보장하는 HashSet\n- `TreeSet`: 정렬된 집합, Red-Black Tree 구조\n\n```java\nSet<String> languages = new HashSet<>();\nlanguages.add(\"Java\");\nlanguages.add(\"Python\");\nlanguages.add(\"Java\"); // {\"Java\", \"Python\"}\n\nSystem.out.println(languages.size()); // 2\n\n// TreeSet 사용 시 자동 정렬\nSet<Integer> sortedNumbers = new TreeSet<>();\nsortedNumbers.add(30);\nsortedNumbers.add(10);\nsortedNumbers.add(20);\nSystem.out.println(sortedNumbers); // [10, 20, 30]\n```\n\n### 4. Queue 인터페이스\n\n`Queue`는 FIFO(First-In-First-Out) 방식으로 요소를 처리하는 컬렉션이다.\n\n**주요 메서드**\n\n- `offer(E e)`: 큐에 요소 추가\n- `poll()`: 큐에서 요소 제거하고 반환\n- `peek()`: 큐의 첫 번째 요소 조회 (제거하지 않음)\n\n**주요 구현체**\n\n- `LinkedList`: Queue 인터페이스도 구현\n- `PriorityQueue`: 우선순위 큐\n- `ArrayDeque`: 양방향 큐\n\n```java\nQueue<String> queue = new LinkedList<>();\nqueue.offer(\"First\");\nqueue.offer(\"Second\");\nqueue.offer(\"Third\");\n\nSystem.out.println(queue.poll()); // \"First\"\nSystem.out.println(queue.peek()); // \"Second\" (제거되지 않음)\n```\n\n### 5. Deque 인터페이스\n\n`Deque`(Double-ended Queue)는 양쪽 끝에서 삽입과 삭제가 가능한 큐이다.\n\n**특징**\n\n- 양방향 큐 (Double-ended Queue)\n- 스택과 큐의 기능을 모두 제공\n- LIFO와 FIFO 모두 지원\n\n**주요 구현체**\n\n- `ArrayDeque`: 동적 배열 기반, 성능이 뛰어남\n- `LinkedList`: 이중 연결 리스트, 메모리 오버헤드 존재\n- `ConcurrentLinkedDeque`: 동시성 지원하는 링크 기반 덱\n\n```java\nDeque<Integer> deque = new ArrayDeque<>();\n\n// 양쪽 끝에 요소 추가\ndeque.addFirst(1);\ndeque.addLast(2);\ndeque.addFirst(0); // [0, 1, 2]\n\n// 스택처럼 사용\ndeque.push(3); // [3, 0, 1, 2]\nSystem.out.println(deque.pop()); // 3, [0, 1, 2]\n\n// 큐처럼 사용\ndeque.offer(4); // [0, 1, 2, 4]\nSystem.out.println(deque.poll()); // 0, [1, 2, 4]\n\n// 양쪽 끝 요소 조회\nSystem.out.println(deque.peekFirst()); // 1\nSystem.out.println(deque.peekLast()); // 4\n\n// 양쪽 끝에서 제거\nSystem.out.println(deque.removeFirst()); // 1\nSystem.out.println(deque.removeLast()); // 4\n```\n\n### 6. Map 인터페이스\n\n`Map`은 키-값 쌍으로 데이터를 저장하는 인터페이스로, Collection을 상속하지 않지만 컬렉션 프레임워크의 일부이다.\n\n**특징:**\n\n- 키-값 매핑\n- 키는 중복 불허, 값은 중복 허용\n\n**주요 구현체**\n\n- `HashMap`: 해시 테이블 기반\n- `LinkedHashMap`: 삽입 순서 보장\n- `TreeMap`: 키로 정렬된 맵\n\n```java\nMap<String, Integer> scores = new HashMap<>();\nscores.put(\"Alice\", 95);\nscores.put(\"Bob\", 87);\nscores.put(\"Charlie\", 92);\n\nSystem.out.println(scores.get(\"Alice\")); // 95\nSystem.out.println(scores.containsKey(\"Bob\")); // true\n\n// 모든 키-값 쌍 순회\nfor (Map.Entry<String, Integer> entry : scores.entrySet()) {\n    System.out.println(entry.getKey() + \": \" + entry.getValue());\n}\n```\n\n## Collections 클래스 계층 구조 한눈에 보기\n\n![Collections 클래스 계층 구조 한눈에 보기](./collections.png)\n\n---\n\n# Stream\n\n> Stream API는 Java 8에서 도입된 함수형 프로그래밍 패러다임을 지원하는 기능으로, 컬렉션 데이터를 선언적 방식으로 처리할 수 있게 해준다.\n>\n> 스트림을 사용하면 컬렉션이나 배열을 반복문(`for`, `while`) 없이 처리 할 수 있는데,\n> JavaScript에서 자주 사용하는 라이브러리인 lodash나 Rxjs와 비슷한 패러다임으로 느꼈다.\n\n![Stream](./stream.jpg)\n\n- 위와 같이 Stream은 컬렉션이나 배열 등의 여러 데이터 묶음을 처리한다. 순회, 필터링, 집계와 같은 처리에 사용하며, 병렬 처리를 지원하기도 한다.\n\n## Stream의 특징\n\n### 선언적(Declarative)\n\n- 직접 반복문이나 조건문을 쓰지 않고, 람다나 메서드를 전달하여 처리 - 함수형 프로그래밍\n\n```java\nList<String> names = Arrays.asList(\"Alice\", \"Bob\", \"Charlie\");\nnames.stream()\n     .filter(n -> n.length() <= 4)  // 필터 조건 정의\n     .map(String::toUpperCase)      // 변환 방식 정의\n     .forEach(System.out::println);\n\n```\n\n### 파이프라이닝(Pipelining)\n\n- 여러 중간 연산을 연결하여 하나의 연속적인 처리 과정을 구성\n- Stream 연산을 마치 파이프처럼 이어 붙여 복합 연산을 간결하게 작성\n\n```java\nList<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5);\nint sum = numbers.stream()\n                 .filter(n -> n % 2 == 0)   // 짝수만 필터\n                 .map(n -> n * n)           // 제곱으로 변환\n                 .reduce(0, Integer::sum);  // 합계 계산\nSystem.out.println(sum); // 20\n```\n\n- filter → map → reduce가 연속적으로 연결되어 하나의 처리 흐름이 됨\n\n### 내부 반복(Internal Iteration)\n\n- Stream이 요소를 반복 처리하는 로직을 내부에서 수행하여 반복문을 작성할 필요가 없음\n- 기존 컬렉션 반복(Iterator, for-each)은 외부 반복(External Iteration)\n- 내부의 모든 요소에 대해 전달한 동작을 실행한다고 생각하자.\n\n![스트림의 내부 반복](./internal-iteration.png)\n\n```java\nList<String> list = Arrays.asList(\"a\", \"b\", \"c\");\nlist.stream().forEach(System.out::println); // 내부 반복, 전체 요소 출력\n```\n\n```bash\n출력 결과\n---\na\nb\nc\n```\n\n### 지연 실행(Lazy Evaluation)\n\n- 중간 연산은 바로 실행되지 않고, 최종 연산이 호출될 때 실행됨\n- 불필요한 연산을 줄여 성능 최적화 가능\n\n```java\nList<String> names = Arrays.asList(\"Alice\", \"Bob\", \"Charlie\");\n\nStream<String> stream = names.stream()\n                             .filter(n -> {\n                                 System.out.println(\"filter 호출: \" + n);\n                                 return n.length() <= 4;\n                             });\nSystem.out.println(\"아직 filter 실행 안됨\");\n\nList<String> result = stream.collect(Collectors.toList()); // 여기서 filter가 실제로 실행된다.\n```\n\n## Stream의 사용 방법\n\n![](./stream-step.png)\n\n- Stream의 사용 방식은 스트림 생성, 중간 연산, 최종 연산 세 단계로 분류 할 수 있다.\n\n### 1. 스트림 생성\n\n- 연산 파이프라인을 붙일 스트림을 생성해야한다. 스트림 생성에 사용된 컬렉션 객체 또는 배열은 스트림 연산이 실행되어도 변경되지 않는다.\n- 컬렉션, 배열로부터 생성할 수 있고, 원본이 없는 경우 of로 스트림을 생성할 수 있다.\n- 무한스트림은 끝이 없는 Stream으로, 요소를 계속 생성할 수 있으며 주로 계산, 난수 생성, 시퀀스 생성 등에서 사용한다.\n\n```java\n// 컬렉션에서 생성\nList<String> list = Arrays.asList(\"a\", \"b\", \"c\");\nStream<String> stream1 = list.stream();\n\n// 배열에서 생성\nString[] array = {\"x\", \"y\", \"z\"};\nStream<String> stream2 = Arrays.stream(array);\n\n// 직접 생성\nStream<String> stream3 = Stream.of(\"hello\", \"world\");\n\n// 무한 스트림 생성\nStream<Integer> infiniteStream = Stream.iterate(0, n -> n + 1);\nStream<Double> randomStream = Stream.generate(Math::random);\n```\n\n### 2. 중간 연산\n\n- 중간 연산은 Stream을 반환하므로, 계속해서 여러 연산을 연결해서 파이프라인을 만들 수 있다.\n- 중간 연산은 **지연 실행(lazy)**되므로 최종 연산이 호출될 때까지 실제로 실행되지 않는다.\n- 중간 연산의 종류 몇 가지\n  - `filter`: 필터링, 조건에 맞는 요소만 남김\n  - `map`: 매핑, 요소를 다른 값으로 변환\n  - `sorted`: 정렬\n  - `distinct`: 중복 제거\n  - `limit`: 최대 n개 요소만 남김\n  - `skip`: 처음 n개 요소 제외\n\n```java\nList<String> names = Arrays.asList(\"Alice\", \"Bob\", \"Charlie\");\n\nStream<String> stream = names.stream()\n                             .filter(n -> n.length() <= 4)  // 4글자 이하인 것만 필터링\n                             .map(String::toUpperCase);     // 대문자로 변환\n\nStream<Integer> sortedStream = Stream.of(3,1,4,2).sorted(); // 1,2,3,4\nStream<Integer> distinctStream = Stream.of(1,2,2,3,1).distinct(); // 1,2,3\n\nStream<Integer> limited = Stream.iterate(1, n -> n + 1).limit(5); // 1,2,3,4,5\nStream<Integer> skipped = Stream.iterate(1, n -> n + 1).skip(3).limit(3); // 4,5,6\n```\n\n### 3. 최종 연산\n\n- 최종 연산이 실행 될 때, 모든 중간 연산이 실행된다.\n- 결과를 반환한다.\n- 최종 연산의 종류 몇 가지\n  - `collect`: 스트림의 결과를 컬렉션, 맵, 문자열 등으로 모아서 변환\n  - `forEach`: 스트림의 각 요소를 순회하며 주어진 동작 실행\n  - `reduce`: 스트림의 요소를 하나의 값으로 누적\n  - `count`: 스트림 요소의 개수 반환\n\n```java\nList<String> result = stream.collect(Collectors.toList()); // stream 연산 결과를 result 리스트에 할당\n\nStream.of(1,2,3).forEach(System.out::println); // 1 2 3 출력\n\nint sum = Stream.of(1,2,3,4).reduce(0, Integer::sum); // 10\nOptional<Integer> max = Stream.of(1,2,3,4).reduce(Integer::max); // 4\n\nlong evenCount = Stream.of(1,2,3,4,5)\n                       .filter(n -> n % 2 == 0)\n                       .count(); // 2\n```\n\n---\n\n# 참고\n\n## 공식 문서\n\n- [Oracle Java Collections Tutorial](https://docs.oracle.com/javase/tutorial/collections/index.html)\n- [Stream API Documentation](https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html)\n- [Processing Data with Java SE 8 Streams - Oracle](https://www.oracle.com/technical-resources/articles/java/ma14-java-se-8-streams.html)\n- [Java SE Stream API - Dev.java](https://dev.java/learn/api/streams/)\n","frontmatter":{"date":"2025년 9월 16일","slug":"til-20250916","title":"Java의 Collections와 Stream","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","java"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAABOElEQVR42qWTTUvDQBCG+9P1IN6EouJBQaUgCFWLYqGCpSqKChGrgl/YKpU2aWqySTYmGzf7uinpIZc0SeewA8Pw7DtfJWQxISB8NvbTrISMxu7uIYJgRmCsyDu7AK0eIHTdRDwfMAzHLvjowJhfBD2qx3FRVKHsW/RSivDmGv52BVwq40WAk4reNIr9xyGeVQftl2+oxMPhg5YKnTqUWlvFcquLHWUAzWboEz+fwsnfjs+hfBHcdk2sSODxkyaBPgyXJfIyK4zKUnoWzj9tNDsWGq8/OHk30bODnMC4gYIQ/F1eges6WF8FMy3wgQqYRs6hxMBwNIK1sQWyVIY5twC7vApnswIifSEgVzXYa+ugu1V4py24ezW4chd/642CCi1Z4lBP7lG07ELMfnrTzi0bUKSMMgX4D8o/kW0u+lU+AAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/5f2dd57d3835641ebe038dbd18c6d76c/27877/java-thumb.png","srcSet":"/static/5f2dd57d3835641ebe038dbd18c6d76c/20f42/java-thumb.png 172w,\n/static/5f2dd57d3835641ebe038dbd18c6d76c/01d54/java-thumb.png 344w,\n/static/5f2dd57d3835641ebe038dbd18c6d76c/27877/java-thumb.png 688w","sizes":"(min-width: 688px) 688px, 100vw"},"sources":[{"srcSet":"/static/5f2dd57d3835641ebe038dbd18c6d76c/6a027/java-thumb.webp 172w,\n/static/5f2dd57d3835641ebe038dbd18c6d76c/f18d0/java-thumb.webp 344w,\n/static/5f2dd57d3835641ebe038dbd18c6d76c/42203/java-thumb.webp 688w","type":"image/webp","sizes":"(min-width: 688px) 688px, 100vw"}]},"width":800,"height":600}}}}},{"id":"8aae0dc5-e03d-5b34-ad5f-4983782d046c","excerpt":"첫번째 TIL 글로 IntelliJ에서 패키지/클래스 생성법과, JAR 파일 만들고 실행하는 방법을 정리해보려고 한다. Java IDE로 Eclipse도 많이 쓰지만, IntelliJ를 설치하여 사용중이었기 때문에 수업에서 사용하지 않아도 그대로…","body":"\n# TIL 챌린지를 시작하며\n\n첫번째 TIL 글로 IntelliJ에서 패키지/클래스 생성법과, JAR 파일 만들고 실행하는 방법을 정리해보려고 한다.\n\n> Java IDE로 **Eclipse**도 많이 쓰지만, **IntelliJ**를 설치하여 사용중이었기 때문에 수업에서 사용하지 않아도 그대로 사용하고있다.\n> 수업 시간에 JAR 만들기를 Eclipse로 진행하고 있어서 급히 다른 블로그 글을 보고 처리했는데,\n> 다른 IntelliJ를 사용하는 분들이 질문하셨던 것에 링크만 공유하고 자세히 답을 못 드렸어서 이번 TIL에 남겨보았다.\n\n---\n\n## IntelliJ Community 소개\n\n![IntelliJ](./til-20250909.png)\n\n**IntelliJ Community**는 무료 버전이지만, Java/Kotlin/Groovy, Maven/Gradle, JUnit 등 순수 자바 개발에 필요한 기능은 충분히 사용할 수 있다.\n\n다만 **Spring/Java EE, Web 프레임워크, 공식 Database Tools**는 **IntelliJ Ultimate**에서 사용할 수 있다.\n\n---\n\n### 설치 방법 (macOS 기준)\n\n1. **[IntelliJ IDEA Community 다운로드 (JetBrains)](https://www.jetbrains.com/ko-kr/idea/download)**\n   - 스크롤을 내리면 Community 에디션 다운로드 버튼이 보인다. OS에 맞는 파일을 다운로드한다.\n     ![IntelliJ IDEA Community 다운로드](./다운로드.png)\n2. IntelliJ 실행 후, `File > Project Structure > SDKs > JDK` 설정\n   ![JDK설정](./JDK설정.png)\n   - JDK를 별도로 설치 하지 않았다면, Download JDK에서 다운로드 할 수 있다. 경로 연결은 알아서 설정된다.\n   - 이미 설치되어 있다면, `Add JDK from disk`를 선택하고 Java Home path 경로를 설정해준다.\n\n---\n\n## Java 코드 작성하기\n\n### 1. 프로젝트 생성\n\n- New Project → Java 선택 → JDK 선택 후, `Create`\n  ![New Project](./new-project.png)\n\n  ![Project](./프로젝트.png)\n  - 다음과 같이 프로젝트가 생성되고, **기본 프로젝트 루트 경로인 `src`** 안에 **Main 클래스**가 생성된다. 이제 자바 코드를 작성 할 수 있다.\n\n### 2. 패키지 생성\n\n- 프로젝트 내부에서 패키지를 생성하고자 하는 경로 우클릭 → New → Package\n  - 기본 프로젝트 루트 경로인 `src` 안에 생성해야한다.\n    ![New Package](./new-package.png)\n- `com.example.app`와 같이 패키지 명 설정\n\n#### 패키지란?\n\n> Java 클래스들을 묶는 이름공간(namespace)으로,\n> 실제로는 폴더 구조와 1:1 대응된다.\n>\n> 같은 이름의 클래스라도 패키지가 다르면 구분 가능하다.\n\n### 3. 클래스 생성\n\n- 프로젝트 내부에서 클래스를 생성하고자 하는 경로 우클릭 → New → Java Class\n  - 기본 프로젝트 루트 경로인 `src` 안에 생성해도 된다.\n    ![New Class](./new-class.png)\n- 코드 작성\n\n  ```java\n  package com.example.app;\n\n  public class Main {\n      public static void main(String[] args) {\n          System.out.println(\"Hello world!\");\n      }\n  }\n  ```\n\n  - 실행 ▶️ 버튼을 클릭하면 `main`메소드가 실행횐다.\n\n---\n\n## JAR 파일 만들고 실행하기\n\n### JAR(Java ARchive)이란?\n\n> Java 클래스 파일(.class), 리소스 파일(.xml, .properties, 이미지 등)을 하나로 묶은 압축 파일\n\n- 여러 개의 .class와 리소스를 한 번에 배포하기 편하게 묶음\n\n- 라이브러리(JAR 파일)를 다른 프로젝트에서 import해서 재사용\n\n- Main Class가 지정된 경우, 실행 가능한 프로그램으로 사용\n\n### IntelliJ Artifacts로 JAR 만들기\n\n1. **File → Project Structure → Artifacts**\n   ![Artifacts](./jar-artifacts.png)\n   - **`+` → JAR → Empty**\n     - Main Class가 없는 경우(main 메소드 실행 코드가 없을 경우) 선택한다.\n     - Output Layout 탭에서 어떤 .class와 리소스를 넣을지 설정해야한다.\n   - **`+` → JAR → From modules with dependencies…**\n     - **Main Class**에 Main Class 지정\n       ![Main Class 지정](./jar-module.png)\n       - main 메소드가 있는 클래스를 지정해준다. jar 실행 시, 이 클래스의 main 메소드가 실행된다.\n         ![설정보기](./자세한설정.png)\n         - Output Directory 설정한 경로에 jar 파일이 생성 될 것이다.\n\n2. **Build → Build Artifacts… → Build**\n3. `out/artifacts/.../앱이름.jar` 확인\n   ![jar 생성 확인](./jar확인.png)\n\n이렇게 생성된 jar 파일을 아래와 같이 실행해 볼 수 있다.\n\n```bash\njava -jar 앱이름.jar\n```\n\n위의 [Java 코드 작성하기 - 3. 클래스 생성]에서 작성한 `Hello world!`를 출력하는 코드를 jar을 생성하여 실행해보았다.\n![helloWorld](./helloWorld.png)\nmain 메소드에 작성한 내용이 실행되어 Hello world!가 잘 출력되고있다.\n","frontmatter":{"date":"2025년 9월 9일","slug":"til-20250909","title":"IntelliJ(Community)로 Java 코드 작성하기","category":"til-challenge","tags":["bootcamp","multi-campus","ureca-backend","java"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAACXklEQVR42p2TXWhSYRjHj26YR7JdzRWB0k1eyG6sMColq0mu2sdpk5yi1RhZa7WNInVS5hGqkQl9EVIrylaOjQWzghEEQSiJotBNF1FXXuVNF9MI/Hf2Ts88w0K6+PO+z/k/7+88z/tBvTUEsGi6jdF2ByiKgogSkbEhiVZyN3c9whbPEhQXf4CaN17HK/MDnNUOEdN80IzJG5NgAyzUajW/WKfTwel01gBFFVFo7XmKjb7fWO/+CWr6QAgvD0/Btd1FzEOdnSguFZFOp6FSqXiA1+tFLpfjY9FydZUKW5jn2HAFkHqLoKZMd/C4OwrntjN8ciKRQCAQIHOZTEZGt9uNTCZTFyjti0EWAOgJDniPa/c+M4OBHSN8ciqVQjAYJHOapnlgNputC5T0z0LGcsDlCkNcu6H+efTtHCdmc1MzaS0cDpOYYRhoNBpEIhEkk0nhgVSBllnQVWCw+xmC1jdg9B5ijo2OoVwuI5/PQ6vVgmVZlEolFAoFWK1WkiMWiwWnLAD6mBgm7O9g3r0ClMvlaGtrg0KhgEQiId+USiWJ/3ZtBMALXLvjjg/Yv8e3ujc1i2rjtV5d4PCxOE6fTEJv8FfaaSItrdW/LrYAOGhbxPGhDHR72fpVNPBSBMABx3tYXJ+h3XeNmHq9AXa7HRaLhchms8FoNDYO7D3xEV3DX9DecZOYl9wexOOvEYvNEC0sxOH3XxVelaoqWyGxzK0CzYOf0HHuG7aabgn++t8tG0/lsGvkKzRcpZvMd9F65CFaep6AZqJYd/QFLwkzXV+9UUjPfwd9ucw9vV/4A5K5/u0UD6FzAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/623154fc417e74be99de4e14ebfba4a1/28c56/til-20250909.png","srcSet":"/static/623154fc417e74be99de4e14ebfba4a1/840e3/til-20250909.png 150w,\n/static/623154fc417e74be99de4e14ebfba4a1/0fb21/til-20250909.png 300w,\n/static/623154fc417e74be99de4e14ebfba4a1/28c56/til-20250909.png 600w","sizes":"(min-width: 600px) 600px, 100vw"},"sources":[{"srcSet":"/static/623154fc417e74be99de4e14ebfba4a1/6aa7a/til-20250909.webp 150w,\n/static/623154fc417e74be99de4e14ebfba4a1/bc227/til-20250909.webp 300w,\n/static/623154fc417e74be99de4e14ebfba4a1/6128c/til-20250909.webp 600w","type":"image/webp","sizes":"(min-width: 600px) 600px, 100vw"}]},"width":800,"height":600}}}}},{"id":"2f2d7e84-2eb3-5903-bd29-952f2529c388","excerpt":"블로그를 배포하고, 글을 몇 개 작성했다. 그리고 페이지 별 title 메타 태그 같은것도 설정해주었다. 하지만 아이디를 검색해보면, 내 블로그가 나오지 않고 전혀 다른 컨텐츠들이 노출된다. (여기 검색 된 인스타와 유튜브 계정은 다른 사람의 계정인…","body":"\n![aaxx98을 검색했을 때...](./search-aaxx98.png)\n\n블로그를 배포하고, 글을 몇 개 작성했다. 그리고 페이지 별 title 메타 태그 같은것도 설정해주었다.\n\n하지만 아이디를 검색해보면, 내 블로그가 나오지 않고 전혀 다른 컨텐츠들이 노출된다. (여기 검색 된 인스타와 유튜브 계정은 다른 사람의 계정인 것 같다.)\n\n검색 결과에 내 블로그를 노출시키려면 검색엔진에 요청해야한다.\n\n나의 Gatsby 블로그가 검색 엔진에 검색 될 수 있도록 여러가지 설정을 해 주었다.\n\nGoogle Search Console과 플러그인으로 간단하게 설정 할 수 있다.\n\n# 검색 노출을 위한 Google Search Console 연동\n\n## [Google Search Console](https://search.google.com/search-console)\n\n![search-console-attr](./search-console-attr.png)\n\n- Google Search Console에 접속하여 URL 접두어에 블로그 url을 입력한다.\n- 이제 입력한 url의 웹페이지에 대한 소유권 확인이 필요하다. 소유권 확인 창에서, 다른 확인 방법 > HTML 태그를 열어서 코드를 복사한다.\n  - google-site-verification 메타 태그를 받아서 `<head>` 태그 내부에 삽입해주면, Google 크롤러가 해당 태그를 읽어 사이트 소유권을 확인해준다.\n  - `<head>` 요소를 `Seo 컴포넌트`에서 관리하고 있어서 내부에 메타 태그를 추가해주었다.\n  - 적용한 후에 `<head>`내부에서 메타 태그를 찾아보자.\n    ![google-site-verification 메타 태그 확인](./site-verification.png)\n  - 등록한 url 페이지에 배포해야한다. 변경사항을 배포하고, 배포가 완료 되었으면 소유권 확인을 완료하자.\n\n## 사이트맵(sitemap) 생성\n\n- 필요 플러그인: `gatsby-plugin-sitemap`\n\n- 설치\n\n  ```console\n  npm install gatsby-plugin-sitemap\n  ```\n\n- `gatsby-config.js` 설정\n  ```TypeScript\n  plugins: [\n    'gatsby-plugin-sitemap',\n  ]\n  ```\n\n설정 후, 빌드하면 `public/sitemap-index.xml`, `public/sitemap-0.xml`이 생성된다.\n\n- `/sitemap-index.xml`로 접근 할 수 있다.\n  ![sitemap-index](./sitemap-index.png)\n\n이 파일을 Google Search Console에 제출하면, 구글이 블로그 전체 페이지 구조를 한 번에 파악할 수 있다.\n\n- Google Search Console > 색인생성 > Sitemaps에서 사이트맵 링크를 추가해주자.\n  ![사이트맵 설정](./search-console-sitemap.png)\n\n## robots.txt 구성\n\n- 필요 플러그인: `gatsby-plugin-robots-txt`\n\n- 설치\n\n  ```console\n  npm install gatsby-plugin-robots-txt\n  ```\n\n- `gatsby-config.js` 설정\n\n  ```TypeScript\n  plugins: [\n    {\n      resolve: \"gatsby-plugin-robots-txt\",\n      options: {\n        policy: [{ userAgent: \"*\", allow: \"/\" }],\n      },\n    }\n  ]\n  ```\n\n`robots.txt`가 있으면 크롤러가 사이트 접근 정책과 사이트맵 위치를 쉽게 알 수 있게 된다.\n\n설정 후, 빌드하면 `public/robots.txt`가 생성된다.\n\n- `/robots.txt`로 접근 할 수 있다.\n  ![robots.txt](./robots-txt.png)\n\n`{ userAgent: \"*\", allow: \"/\" }`에서, userAgent는 검색 엔진 크롤러의 종류이고, allow는 크롤링을 허용할 경로를 의미한다.\n\n모든 크롤러, 모든 경로로 설정한 것이므로, 모든 검색엔진 크롤러는 이 사이트의 모든 경로를 자유롭게 크롤링해도 된다는 뜻이다.\n\n---\n\n# 뭔가 잘 안됨😅\n\n2025/09/03 13시 46분에 Google Search Console을 먼저 연동하고, 15시 쯤 사이트맵 플러그인을 적용하여 사이트맵 설정을 했습니다.\n\n그런데... 사이트맵을 읽을 수 없음 상태로 아직 색인되지 않고있습니다.\n\n`sitemap-index.xml`이 정상적으로 생성되고 있는데, 최근 크롤링 시간이 14시로 아직 검색엔진에 크롤링이 안된 것 같아서 좀 더 기다려보겠습니다.\n\n![Google Search Console URL 검사 결과](./search-console-url.png)\n","frontmatter":{"date":"2025년 9월 3일","slug":"gatsby-blog-3","title":"Gatsby 블로그 구축하기 (3) - 구글 검색에 블로그 노출하기","category":"blog","tags":["gatsby","seo","blog"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB8klEQVR42pVTS2sTURTO/3XpSly4cuNKQXFRwYUGFESNWotZVAsmEGKl1JQqtGlmMiRxknlmksl9fp47M0nzaLQeOJw75/Gdc+a7t4Ql0UZ1bv8mGttzSuuOlGkwUZQpSUbRmVTLDETRUc07m/gadAaoC99JV+L5V4b9lgAXl9PMRRWJp/45Tr2zPK711RMKavb0gCFMFBoXQPvcB3/3ErNmA8rbBfpP0O5H+NAEHjW/o3b4Gig/A3esorPeXLlc4zixOXYawJuHVeDtC8Q7d8GPbxHgA7z6VMXnFnCzeoT9+zcg7twGO/tZjK82Vx7GGpVvHAe/NNI4AdurgP04hgy+AG4ZLXuCexWFx7UBBp1D8N33kCN3+4RbWS0autTw4xGnxmrFfwUpmibWkDS2kAqSVEkJJURulbGC8tSCJkW5mjSzVD8HL5mPGWNZ4F9iWDZ1lmVjOpkgiiIwqg2CANNpmgMaoDSd0TUR11g9H6Ner8N1XfR6PSRJAsdxEMfxJaCktYwKAjXKC7tNDbDMfoXKfUW98S/+4bIuPy+z5nxVvWYXsSV2Vlg2pDi9PtoXFjp2FxYp4zybuD/4DbvrUKyT5ZjYyPM33vQKoGE6isdwhyP4QZid83UUovEYIZEwHHkIwhCeHyAeTzauzrXu4f/IH2v5h/zHg2NsAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/f8571b45d7b598fbe3fef991b50a9b26/4c3f5/google.png","srcSet":"/static/f8571b45d7b598fbe3fef991b50a9b26/67385/google.png 127w,\n/static/f8571b45d7b598fbe3fef991b50a9b26/600bb/google.png 254w,\n/static/f8571b45d7b598fbe3fef991b50a9b26/4c3f5/google.png 507w","sizes":"(min-width: 507px) 507px, 100vw"},"sources":[{"srcSet":"/static/f8571b45d7b598fbe3fef991b50a9b26/f117c/google.webp 127w,\n/static/f8571b45d7b598fbe3fef991b50a9b26/7b72e/google.webp 254w,\n/static/f8571b45d7b598fbe3fef991b50a9b26/2284a/google.webp 507w","type":"image/webp","sizes":"(min-width: 507px) 507px, 100vw"}]},"width":800,"height":599.6055226824458}}}}},{"id":"ac725582-b5c4-5079-a939-e3defa8dc1aa","excerpt":"Utterances는 GitHub Issues 기반의 댓글 위젯으로, 별도의 서버를 구축할 필요 없이 깔끔하게 댓글 시스템을 붙일 수 있다. 댓글이 달리면 지정한 GitHub 저장소의 Issues에 등록되고, Issues에서 댓글을 관리할 수 있다.…","body":"\n# Utterances 소개\n\nUtterances는 GitHub Issues 기반의 댓글 위젯으로, 별도의 서버를 구축할 필요 없이 깔끔하게 댓글 시스템을 붙일 수 있다.\n댓글이 달리면 지정한 GitHub 저장소의 Issues에 등록되고, Issues에서 댓글을 관리할 수 있다.\n\nGithub 계정으로 로그인하고 댓글을 작성하면 아래와 같이 Issue에 등록된다.\n\n![github issue](./utterances-capture.png)\n\n## 설정 방법\n\n아래와 같은 형식인데 utterances 사이트에서 설정 값을 선택하면 스크립트를 생성해준다.\n\n[Utterances 공식 사이트](https://utteranc.es/) - configuration 항목에서 옵션을 선택한다.\n\n- Repository: username/repo명, 공개 레포지토리만 사용 가능하다. 블로그 레포가 비공개인 경우 댓글 Issue를 등록할 공개 레포지토리를 만들어서 사용하면 된다.\n\n- Blog Post ↔️ Issue Mapping: 포스트별로 구분할 기준, pathname을 선택했다.\n\n- Theme: 디자인 테마 선택\n\n- Repository에 권한 설정 - [utterances app](https://github.com/apps/utterances)에서 Issue 등록을 위해 utterances에서 사용할 레파지토리 권한을 설정해준다.\n\n일반 HTML 파일이나 정적 페이지에서는 Enable Utterances에 생성된 스크립트를 복사하여 붙여넣으면 되지만, Gatsby에서 사용하기 위해서는 React 컴포넌트 형식으로 변환해주어야한다.\n\n![enable utterances](./enable-utterances.png)\n\n## React 컴포넌트로 변환\n\nGatsby는 SSR을 수행하므로 빌드 시점에 DOM이 없어 `<script`>가 실행되지 않는다.\n\nReact는 가상 DOM을 관리하므로 외부 스크립트가 직접 DOM을 건드리면 충돌이 발생한다.\n\nuseEffect를 사용해 브라우저 환경에서만 스크립트를 삽입해 정상적으로 동작하게 해야한다.\n\n- Comment 컴포넌트 생성\n\n  ```TypeScript\n  import React, { useEffect, useRef, useState } from \"react\";\n\n  const Comment = () => {\n  const commentsEl = useRef<HTMLDivElement | null>(null);\n  const [status, setStatus] = useState<\"pending\" | \"success\" | \"failed\">(\n      \"pending\"\n  );\n\n  useEffect(() => {\n      const existingScript = commentsEl.current?.querySelector(\"script\");\n      if (existingScript) {\n      existingScript.remove();\n      }\n\n      const scriptEl = document.createElement(\"script\");\n      scriptEl.onload = () => setStatus(\"success\");\n      scriptEl.onerror = () => setStatus(\"failed\");\n      scriptEl.async = true;\n      scriptEl.src = \"https://utteranc.es/client.js\";\n      scriptEl.setAttribute(\"repo\", \"aaxx98/aaxx98.github.io\");\n      scriptEl.setAttribute(\"issue-term\", \"pathname\");\n      scriptEl.setAttribute(\"theme\", \"github-light\");\n      scriptEl.setAttribute(\"crossorigin\", \"anonymous\");\n\n      if (commentsEl.current) {\n      commentsEl.current.appendChild(scriptEl);\n      }\n\n      // cleanup function\n      return () => {\n      if (commentsEl.current) {\n          const script = commentsEl.current.querySelector(\"script\");\n          if (script) {\n          script.remove();\n          }\n      }\n      };\n  }, []);\n\n  return (\n      <div className=\"w-full max-w-[850px] mx-auto px-8 mt-8\">\n      {status === \"failed\" && (\n          <div className=\"text-red-500 text-center p-4\">\n          댓글을 불러오는 중 오류가 발생했습니다. 새로고침 후 다시 시도해주세요.\n          </div>\n      )}\n      {status === \"pending\" && (\n          <div className=\"text-gray-500 text-center p-4\">\n          댓글을 불러오는 중...\n          </div>\n      )}\n      <div\n          ref={commentsEl}\n          className=\"utterances-container\"\n          style={{\n          minHeight: \"200px\",\n          width: \"100%\",\n          display: \"block\",\n          }}\n      />\n      </div>\n  );\n  };\n\n  export default Comment;\n  ```\n\n위에서 생성한 Comment를 원하는 위치에서 사용하면 된다.\n\n처음 작성하고 댓글이 표시되지 않았었는데, repo 경로에 오타가 있었다.\n\n잘 나타나지 않는 경우 repo를 제대로 지정했는지, 권한 설정을 했는지 확인해보자.\n","frontmatter":{"date":"2025년 9월 1일","slug":"gatsby-blog-2","title":"Gatsby 블로그 구축하기 (2) - Utterances 연동해서 댓글란 만들기","category":"blog","tags":["gatsby","utterances","github","blog"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAAC1ElEQVR42pVT7UtTURzel772tT+igqCvQmFJfQjUEiRjflhG5kstrMSQZpjORAsRk0oiKXUVkpSaWVlUlC8V6mwvba65e3d397673W13b/fp3EMTgwz7we8+95zz4zm/l+eocrkcFIvFw7DajGDcdmRzGcg5GdlslpzIkGVs2VQsy6K2phonyi6geP9VlOxrgra6ES7WQQNkwhYMhjC/8AWfZ+ewtLyC7yYzQSNMZgtsNjtZE7Sv0ljVyUo1agvq0XbaiZ5zUXTpRGg071BRWotoLExJZ+fmMTT8CJNT07h99x7GJ15gZOQxJianMPp0DA+HDRh7Nk4rUqnL69GsnsXNBh86NA5cHvChfQaoKBjAg6FBSphOZ6C0Rskg7/nsFZL8GS259FAztOULaD/D43odi446Bjca/NCWfYK+rZsGJZMSEokkdTGeQFKSKCouSal1MkpYXFQE8/QB3NGNQl8TQGedC30Xo6g68hy9vX00yOsLYI1xY9Fohslig23VCfMPO74trcBF9vODpYQa9U4Y32xDf0sPzh9fxpUqCxorTSg+qCWNNtMgKZUi/RQRiUQp0n8hipgYhxCNIRiOkLVAK1AtLltRUrgX/ZcMaKuZxq2mGZwtPYUWXTMlU3oUjghwOF1gWI6i4k4XAw/vI2rgSLar+LpoRDAUhiohpaEuOQxt4Q7oju5Ga9kuHNuzHYOD99cJlUDOw4PlPJRccTfH4+caA18gCI4Q59uoUj52hwPdXd3o1LdDf60VBsMTpFLp38IGBCEG1u0B7/UjRMpTLnCTC1wMR1HJfH3Kmyl+ozxCYYEOhXFzFHNybvOXonyUKWUyGZpR3jdKIU7kwXt98PuD8AdChFDeoEn8KZutvM94OgFfPACv6AdPnI3wiEqxzTP8V9mKcYIXr60f8cryAS/N7zFpegs32ftvwrylMkSHSRFiKg5RItpLxiCRvb8R/gKf0yzDbAZojQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/a3dbdb53527872506109d74af949b679/ce24c/utterances-thumbnail.png","srcSet":"/static/a3dbdb53527872506109d74af949b679/aacf3/utterances-thumbnail.png 200w,\n/static/a3dbdb53527872506109d74af949b679/0e3a3/utterances-thumbnail.png 400w,\n/static/a3dbdb53527872506109d74af949b679/ce24c/utterances-thumbnail.png 800w,\n/static/a3dbdb53527872506109d74af949b679/ee2e1/utterances-thumbnail.png 1600w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/a3dbdb53527872506109d74af949b679/abf68/utterances-thumbnail.webp 200w,\n/static/a3dbdb53527872506109d74af949b679/ce081/utterances-thumbnail.webp 400w,\n/static/a3dbdb53527872506109d74af949b679/c11e0/utterances-thumbnail.webp 800w,\n/static/a3dbdb53527872506109d74af949b679/0843b/utterances-thumbnail.webp 1600w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":600}}}}},{"id":"ef3b8d12-9ab4-5ad2-b3e6-e51e4114cd89","excerpt":"Gatsby 소개와 선택한 이유 Gatsby는 React 기반의 정적 사이트 생성기(Static Site Generator, SSG)다.\n\nNext.js처럼 서버 사이드 렌더링을 지원하는 프레임워크도 있지만, Gatsby는 서버가 없고 정적 파일을…","body":"\n![Gatsby js](./gatsby-blog.png)\n\n# Gatsby 블로그\n\n## Gatsby 소개와 선택한 이유\n\nGatsby는 React 기반의 정적 사이트 생성기(Static Site Generator, SSG)다.\n\nNext.js처럼 서버 사이드 렌더링을 지원하는 프레임워크도 있지만, Gatsby는 서버가 없고 정적 파일을 빌드해서 배포하는 데 특화되어 있다.\n\n정적 사이트 생성기의 특징은 모든 페이지가 HTML, CSS, JS 파일로 미리 렌더링된다는 점이다. 덕분에 GitHub Pages 같은 정적 호스팅 서비스만 있으면 별도 운영비가 들지 않고 빠르게 배포할 수 있다.\n\n## Gatsby init부터 deploy까지\n\nGatsby 공식문서 튜토리얼대로 따라하면 되기 때문에 세팅이 간단했다.\n\nTypeScript를 선호하기 때문에 `gatsby new`로 프로젝트 생성 시에 TypeScript 사용을 선택했다.\n\n블로그 디자인은 간단하게 className으로 조정하며 만드려고 해서 tailwind css를 사용했다.\n\n레포지토리를 생성하고, Github Pages 설정 후, `npm run deploy` 명령어 한 줄만 실행하면, 빌드된 정적 페이지가 자동으로 gh-pages 브랜치에 생성되어 배포되는 것을 확인할 수 있었다.\n\nGithub Actions 스크립트로 자동배포 설정을 해두어 main에 push하면 자동 배포되도록 설정했다.\n\n### 참고\n\n- [Gatsby TypeScript 가이드](https://www.gatsbyjs.com/docs/how-to/custom-configuration/typescript/)\n- [Gatsby getting started](https://www.gatsbyjs.com/docs/tutorial/getting-started/)\n- [Install Tailwind CSS with Gatsby](https://tailwindcss.com/docs/installation/framework-guides/gatsby)\n- [Github Pages로 배포](https://www.gatsbyjs.com/docs/how-to/previews-deploys-hosting/how-gatsby-works-with-github-pages/)\n","frontmatter":{"date":"2025년 8월 21일","slug":"gatsby-blog-1","title":"Gatsby 블로그 구축하기 (1) - Gatsby 프로젝트 생성","category":"blog","tags":["gatsby","react","blog","tailwind-css"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB/ElEQVR42mMI0WklGzFQpDlMry1cvz1Mvz1Ur41kzX4qjd4KdUAUoN4cBtev2woyTheLhjAkOxjyfKZPq90yqWJjst0Ef9XGUN02oJ5g7RZvxfpAzWYwF4SAeoAkUEOAelOoTiuEzXBi942Dmy8f2nL54KZL2R5TgXJBWi2RRh0VkfMTbftCtFsDNZqDtVqA5gYBzdJrKw6aDTQaKAJkMxzdce3///+f3n/98PbrntXnvRXroow71805unbW0f7S9UCdWR5Tkmz7ga6Lt+pNdZy4Ye6xHK/pkcadIJu3LD757fOPzx++Hd91/fDWK77KDRUR89fMOAIMgg1zj6+bfXTV9EPr5xzdu/b8ullHF/Xs2bTgBFBk65JTMxu3M0yv27py6sFHt1/tWH5m1bRDQM1xlj07V54F2nxk21WgIFA10IL5HbtmNm47f+TOtiWnti87Awyj0/tuMezfcAlo4YGNl47tvAbxM9CpiTZ9nbmrY8y7w/TbGpKWpDlNSrDuy/acBlQAJMvD52W5T9208AQDkAN0wJTqTbEWPX7Q0G4D6veQrQ3SbAGGjZd8rb8ayESgLBD5qzW6y1QDPZ/hOpkByPECxrNiPTCQ4XEYCk45oPjQbQUlIT1IbIEYoERl0A4M+UCNJgagwyIMOsINOkhKYcC0ADSFYcAyBgAwr3ojnjL9xwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/94688911b8f5aaa3a414502512ddaf6f/ce24c/gatsby-blog.png","srcSet":"/static/94688911b8f5aaa3a414502512ddaf6f/aacf3/gatsby-blog.png 200w,\n/static/94688911b8f5aaa3a414502512ddaf6f/0e3a3/gatsby-blog.png 400w,\n/static/94688911b8f5aaa3a414502512ddaf6f/ce24c/gatsby-blog.png 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/94688911b8f5aaa3a414502512ddaf6f/abf68/gatsby-blog.webp 200w,\n/static/94688911b8f5aaa3a414502512ddaf6f/ce081/gatsby-blog.webp 400w,\n/static/94688911b8f5aaa3a414502512ddaf6f/c11e0/gatsby-blog.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":600}}}}}]}}}